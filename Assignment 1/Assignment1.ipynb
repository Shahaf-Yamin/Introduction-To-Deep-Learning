{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Assignment1.ipynb","provenance":[{"file_id":"1iO5ynsj7gtK5Hnk4ufKtP58keGsplYNI","timestamp":1636560349988}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3bhfIKbQzGq2"},"source":["# Assignment 1. Music Century Classification\n","\n","**Assignment Responsible**: Natalie Lang.\n","\n","In this assignment, we will build models to predict which\n","**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n","based on the Million Song Dataset. The data is available to download from the UCI \n","Machine Learning Repository. Here are some links about the data:\n","\n","- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n","- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n","\n","Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"]},{"cell_type":"markdown","metadata":{"id":"47oq1vy5PUIV"},"source":["## Question 1. Data (21%)\n","\n","Start by setting up a Google Colab notebook in which to do your work.\n","Since you are working with a partner, you might find this link helpful:\n","\n","- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n","\n","The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together. \n","\n","To process and read the data, we use the popular `pandas` package for data analysis."]},{"cell_type":"code","metadata":{"id":"1aFWpuNSzGq9"},"source":["import pandas\n","import numpy as np\n","import matplotlib.pyplot as plt\n","np.random.seed(1) # To reproduce our experiment"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7UWL6mFzGq-"},"source":["Now that your notebook is set up, we can load the data into the notebook. The code below provides\n","two ways of loading the data: directly from the internet, or through mounting Google Drive.\n","The first method is easier but slower, and the second method is a bit involved at first, but\n","can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n","figuring how to do that now.\n","\n","Here are some resources to help you get started:\n","\n","- http.://colab.research.google.com/notebooks/io.ipynb"]},{"cell_type":"code","metadata":{"id":"EY6PrfV4zGq_"},"source":["load_from_drive = False\n","\n","if not load_from_drive:\n","  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n","else:\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  csv_path = '/content/gdrive/My Drive/YearPredictionMSD.txt.zip' # TODO - UPDATE ME WITH THE TRUE PATH!\n","\n","t_label = [\"year\"]\n","x_labels = [\"var%d\" % i for i in range(1, 91)]\n","df = pandas.read_csv(csv_path, names=t_label + x_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgB83beNzGq_"},"source":["Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n","DataFrame `df` as a table:"]},{"cell_type":"code","metadata":{"id":"H5bBEnj3zGq_","colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"status":"ok","timestamp":1636559846228,"user_tz":-120,"elapsed":568,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"17c828b4-207a-43b5-c3e0-5ed2b1379ef5"},"source":["df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>year</th>\n","      <th>var1</th>\n","      <th>var2</th>\n","      <th>var3</th>\n","      <th>var4</th>\n","      <th>var5</th>\n","      <th>var6</th>\n","      <th>var7</th>\n","      <th>var8</th>\n","      <th>var9</th>\n","      <th>var10</th>\n","      <th>var11</th>\n","      <th>var12</th>\n","      <th>var13</th>\n","      <th>var14</th>\n","      <th>var15</th>\n","      <th>var16</th>\n","      <th>var17</th>\n","      <th>var18</th>\n","      <th>var19</th>\n","      <th>var20</th>\n","      <th>var21</th>\n","      <th>var22</th>\n","      <th>var23</th>\n","      <th>var24</th>\n","      <th>var25</th>\n","      <th>var26</th>\n","      <th>var27</th>\n","      <th>var28</th>\n","      <th>var29</th>\n","      <th>var30</th>\n","      <th>var31</th>\n","      <th>var32</th>\n","      <th>var33</th>\n","      <th>var34</th>\n","      <th>var35</th>\n","      <th>var36</th>\n","      <th>var37</th>\n","      <th>var38</th>\n","      <th>var39</th>\n","      <th>...</th>\n","      <th>var51</th>\n","      <th>var52</th>\n","      <th>var53</th>\n","      <th>var54</th>\n","      <th>var55</th>\n","      <th>var56</th>\n","      <th>var57</th>\n","      <th>var58</th>\n","      <th>var59</th>\n","      <th>var60</th>\n","      <th>var61</th>\n","      <th>var62</th>\n","      <th>var63</th>\n","      <th>var64</th>\n","      <th>var65</th>\n","      <th>var66</th>\n","      <th>var67</th>\n","      <th>var68</th>\n","      <th>var69</th>\n","      <th>var70</th>\n","      <th>var71</th>\n","      <th>var72</th>\n","      <th>var73</th>\n","      <th>var74</th>\n","      <th>var75</th>\n","      <th>var76</th>\n","      <th>var77</th>\n","      <th>var78</th>\n","      <th>var79</th>\n","      <th>var80</th>\n","      <th>var81</th>\n","      <th>var82</th>\n","      <th>var83</th>\n","      <th>var84</th>\n","      <th>var85</th>\n","      <th>var86</th>\n","      <th>var87</th>\n","      <th>var88</th>\n","      <th>var89</th>\n","      <th>var90</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2001</td>\n","      <td>49.94357</td>\n","      <td>21.47114</td>\n","      <td>73.07750</td>\n","      <td>8.74861</td>\n","      <td>-17.40628</td>\n","      <td>-13.09905</td>\n","      <td>-25.01202</td>\n","      <td>-12.23257</td>\n","      <td>7.83089</td>\n","      <td>-2.46783</td>\n","      <td>3.32136</td>\n","      <td>-2.31521</td>\n","      <td>10.20556</td>\n","      <td>611.10913</td>\n","      <td>951.08960</td>\n","      <td>698.11428</td>\n","      <td>408.98485</td>\n","      <td>383.70912</td>\n","      <td>326.51512</td>\n","      <td>238.11327</td>\n","      <td>251.42414</td>\n","      <td>187.17351</td>\n","      <td>100.42652</td>\n","      <td>179.19498</td>\n","      <td>-8.41558</td>\n","      <td>-317.87038</td>\n","      <td>95.86266</td>\n","      <td>48.10259</td>\n","      <td>-95.66303</td>\n","      <td>-18.06215</td>\n","      <td>1.96984</td>\n","      <td>34.42438</td>\n","      <td>11.72670</td>\n","      <td>1.36790</td>\n","      <td>7.79444</td>\n","      <td>-0.36994</td>\n","      <td>-133.67852</td>\n","      <td>-83.26165</td>\n","      <td>-37.29765</td>\n","      <td>...</td>\n","      <td>-25.38187</td>\n","      <td>-3.90772</td>\n","      <td>13.29258</td>\n","      <td>41.55060</td>\n","      <td>-7.26272</td>\n","      <td>-21.00863</td>\n","      <td>105.50848</td>\n","      <td>64.29856</td>\n","      <td>26.08481</td>\n","      <td>-44.59110</td>\n","      <td>-8.30657</td>\n","      <td>7.93706</td>\n","      <td>-10.73660</td>\n","      <td>-95.44766</td>\n","      <td>-82.03307</td>\n","      <td>-35.59194</td>\n","      <td>4.69525</td>\n","      <td>70.95626</td>\n","      <td>28.09139</td>\n","      <td>6.02015</td>\n","      <td>-37.13767</td>\n","      <td>-41.12450</td>\n","      <td>-8.40816</td>\n","      <td>7.19877</td>\n","      <td>-8.60176</td>\n","      <td>-5.90857</td>\n","      <td>-12.32437</td>\n","      <td>14.68734</td>\n","      <td>-54.32125</td>\n","      <td>40.14786</td>\n","      <td>13.01620</td>\n","      <td>-54.40548</td>\n","      <td>58.99367</td>\n","      <td>15.37344</td>\n","      <td>1.11144</td>\n","      <td>-23.08793</td>\n","      <td>68.40795</td>\n","      <td>-1.82223</td>\n","      <td>-27.46348</td>\n","      <td>2.26327</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2001</td>\n","      <td>48.73215</td>\n","      <td>18.42930</td>\n","      <td>70.32679</td>\n","      <td>12.94636</td>\n","      <td>-10.32437</td>\n","      <td>-24.83777</td>\n","      <td>8.76630</td>\n","      <td>-0.92019</td>\n","      <td>18.76548</td>\n","      <td>4.59210</td>\n","      <td>2.21920</td>\n","      <td>0.34006</td>\n","      <td>44.38997</td>\n","      <td>2056.93836</td>\n","      <td>605.40696</td>\n","      <td>457.41175</td>\n","      <td>777.15347</td>\n","      <td>415.64880</td>\n","      <td>746.47775</td>\n","      <td>366.45320</td>\n","      <td>317.82946</td>\n","      <td>273.07917</td>\n","      <td>141.75921</td>\n","      <td>317.35269</td>\n","      <td>19.48271</td>\n","      <td>-65.25496</td>\n","      <td>162.75145</td>\n","      <td>135.00765</td>\n","      <td>-96.28436</td>\n","      <td>-86.87955</td>\n","      <td>17.38087</td>\n","      <td>45.90742</td>\n","      <td>32.49908</td>\n","      <td>-32.85429</td>\n","      <td>45.10830</td>\n","      <td>26.84939</td>\n","      <td>-302.57328</td>\n","      <td>-41.71932</td>\n","      <td>-138.85034</td>\n","      <td>...</td>\n","      <td>28.55107</td>\n","      <td>1.52298</td>\n","      <td>70.99515</td>\n","      <td>-43.63073</td>\n","      <td>-42.55014</td>\n","      <td>129.82848</td>\n","      <td>79.95420</td>\n","      <td>-87.14554</td>\n","      <td>-45.75446</td>\n","      <td>-65.82100</td>\n","      <td>-43.90031</td>\n","      <td>-19.45705</td>\n","      <td>12.59163</td>\n","      <td>-407.64130</td>\n","      <td>42.91189</td>\n","      <td>12.15850</td>\n","      <td>-88.37882</td>\n","      <td>42.25246</td>\n","      <td>46.49209</td>\n","      <td>-30.17747</td>\n","      <td>45.98495</td>\n","      <td>130.47892</td>\n","      <td>13.88281</td>\n","      <td>-4.00055</td>\n","      <td>17.85965</td>\n","      <td>-18.32138</td>\n","      <td>-87.99109</td>\n","      <td>14.37524</td>\n","      <td>-22.70119</td>\n","      <td>-58.81266</td>\n","      <td>5.66812</td>\n","      <td>-19.68073</td>\n","      <td>33.04964</td>\n","      <td>42.87836</td>\n","      <td>-9.90378</td>\n","      <td>-32.22788</td>\n","      <td>70.49388</td>\n","      <td>12.04941</td>\n","      <td>58.43453</td>\n","      <td>26.92061</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2001</td>\n","      <td>50.95714</td>\n","      <td>31.85602</td>\n","      <td>55.81851</td>\n","      <td>13.41693</td>\n","      <td>-6.57898</td>\n","      <td>-18.54940</td>\n","      <td>-3.27872</td>\n","      <td>-2.35035</td>\n","      <td>16.07017</td>\n","      <td>1.39518</td>\n","      <td>2.73553</td>\n","      <td>0.82804</td>\n","      <td>7.46586</td>\n","      <td>699.54544</td>\n","      <td>1016.00954</td>\n","      <td>594.06748</td>\n","      <td>355.73663</td>\n","      <td>507.39931</td>\n","      <td>387.69910</td>\n","      <td>287.15347</td>\n","      <td>112.37152</td>\n","      <td>161.68928</td>\n","      <td>144.14353</td>\n","      <td>199.29693</td>\n","      <td>-4.24359</td>\n","      <td>-297.00587</td>\n","      <td>-148.36392</td>\n","      <td>-7.94726</td>\n","      <td>-18.71630</td>\n","      <td>12.77542</td>\n","      <td>-25.37725</td>\n","      <td>9.71410</td>\n","      <td>0.13843</td>\n","      <td>26.79723</td>\n","      <td>6.30760</td>\n","      <td>28.70107</td>\n","      <td>-74.89005</td>\n","      <td>-289.19553</td>\n","      <td>-166.26089</td>\n","      <td>...</td>\n","      <td>18.50939</td>\n","      <td>16.97216</td>\n","      <td>24.26629</td>\n","      <td>-10.50788</td>\n","      <td>-8.68412</td>\n","      <td>54.75759</td>\n","      <td>194.74034</td>\n","      <td>7.95966</td>\n","      <td>-18.22685</td>\n","      <td>0.06463</td>\n","      <td>-2.63069</td>\n","      <td>26.02561</td>\n","      <td>1.75729</td>\n","      <td>-262.36917</td>\n","      <td>-233.60089</td>\n","      <td>-2.50502</td>\n","      <td>-12.14279</td>\n","      <td>81.37617</td>\n","      <td>2.07554</td>\n","      <td>-1.82381</td>\n","      <td>183.65292</td>\n","      <td>22.64797</td>\n","      <td>-39.98887</td>\n","      <td>43.37381</td>\n","      <td>-31.56737</td>\n","      <td>-4.88840</td>\n","      <td>-36.53213</td>\n","      <td>-23.94662</td>\n","      <td>-84.19275</td>\n","      <td>66.00518</td>\n","      <td>3.03800</td>\n","      <td>26.05866</td>\n","      <td>-50.92779</td>\n","      <td>10.93792</td>\n","      <td>-0.07568</td>\n","      <td>43.20130</td>\n","      <td>-115.00698</td>\n","      <td>-0.05859</td>\n","      <td>39.67068</td>\n","      <td>-0.66345</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2001</td>\n","      <td>48.24750</td>\n","      <td>-1.89837</td>\n","      <td>36.29772</td>\n","      <td>2.58776</td>\n","      <td>0.97170</td>\n","      <td>-26.21683</td>\n","      <td>5.05097</td>\n","      <td>-10.34124</td>\n","      <td>3.55005</td>\n","      <td>-6.36304</td>\n","      <td>6.63016</td>\n","      <td>-3.35142</td>\n","      <td>37.64085</td>\n","      <td>2174.08189</td>\n","      <td>697.43346</td>\n","      <td>459.24587</td>\n","      <td>742.78961</td>\n","      <td>229.30783</td>\n","      <td>387.89697</td>\n","      <td>249.06662</td>\n","      <td>245.89870</td>\n","      <td>176.20527</td>\n","      <td>98.82222</td>\n","      <td>150.97286</td>\n","      <td>78.49057</td>\n","      <td>-62.00282</td>\n","      <td>43.49659</td>\n","      <td>-96.42719</td>\n","      <td>-108.96608</td>\n","      <td>14.22854</td>\n","      <td>14.54178</td>\n","      <td>-23.55608</td>\n","      <td>-39.36953</td>\n","      <td>-43.59209</td>\n","      <td>20.83714</td>\n","      <td>35.63919</td>\n","      <td>-181.34947</td>\n","      <td>-93.66614</td>\n","      <td>-90.55616</td>\n","      <td>...</td>\n","      <td>4.56917</td>\n","      <td>-37.32280</td>\n","      <td>4.15159</td>\n","      <td>12.24315</td>\n","      <td>35.02697</td>\n","      <td>-178.89573</td>\n","      <td>82.46573</td>\n","      <td>-20.49425</td>\n","      <td>101.78577</td>\n","      <td>-19.77808</td>\n","      <td>-21.52657</td>\n","      <td>3.36303</td>\n","      <td>-11.63176</td>\n","      <td>51.55411</td>\n","      <td>-50.57576</td>\n","      <td>-28.14755</td>\n","      <td>-83.15795</td>\n","      <td>-7.35260</td>\n","      <td>-22.11505</td>\n","      <td>1.18279</td>\n","      <td>-122.70467</td>\n","      <td>150.57360</td>\n","      <td>24.37468</td>\n","      <td>41.19821</td>\n","      <td>-37.04318</td>\n","      <td>-28.72986</td>\n","      <td>162.19614</td>\n","      <td>22.18309</td>\n","      <td>-8.63509</td>\n","      <td>85.23416</td>\n","      <td>34.57337</td>\n","      <td>-171.70734</td>\n","      <td>-16.96705</td>\n","      <td>-46.67617</td>\n","      <td>-12.51516</td>\n","      <td>82.58061</td>\n","      <td>-72.08993</td>\n","      <td>9.90558</td>\n","      <td>199.62971</td>\n","      <td>18.85382</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2001</td>\n","      <td>50.97020</td>\n","      <td>42.20998</td>\n","      <td>67.09964</td>\n","      <td>8.46791</td>\n","      <td>-15.85279</td>\n","      <td>-16.81409</td>\n","      <td>-12.48207</td>\n","      <td>-9.37636</td>\n","      <td>12.63699</td>\n","      <td>0.93609</td>\n","      <td>1.60923</td>\n","      <td>2.19223</td>\n","      <td>47.32082</td>\n","      <td>894.28471</td>\n","      <td>809.86615</td>\n","      <td>318.78559</td>\n","      <td>435.04497</td>\n","      <td>341.61467</td>\n","      <td>334.30734</td>\n","      <td>322.99589</td>\n","      <td>190.61921</td>\n","      <td>235.84715</td>\n","      <td>96.89517</td>\n","      <td>210.58870</td>\n","      <td>5.60463</td>\n","      <td>-199.63958</td>\n","      <td>204.85812</td>\n","      <td>-77.17695</td>\n","      <td>-65.79741</td>\n","      <td>-6.95097</td>\n","      <td>-12.15262</td>\n","      <td>-3.85410</td>\n","      <td>20.68990</td>\n","      <td>-20.30480</td>\n","      <td>37.15045</td>\n","      <td>11.20673</td>\n","      <td>-124.09519</td>\n","      <td>-295.98542</td>\n","      <td>-33.31169</td>\n","      <td>...</td>\n","      <td>45.25506</td>\n","      <td>10.42226</td>\n","      <td>27.88782</td>\n","      <td>-17.12676</td>\n","      <td>-31.54772</td>\n","      <td>-76.86293</td>\n","      <td>41.17343</td>\n","      <td>-138.32535</td>\n","      <td>-53.96905</td>\n","      <td>-21.30266</td>\n","      <td>-24.87362</td>\n","      <td>-2.46595</td>\n","      <td>-4.05003</td>\n","      <td>-56.51161</td>\n","      <td>-34.56445</td>\n","      <td>-5.07092</td>\n","      <td>-47.75605</td>\n","      <td>64.81513</td>\n","      <td>-97.42948</td>\n","      <td>-12.59418</td>\n","      <td>55.23699</td>\n","      <td>28.85657</td>\n","      <td>54.53513</td>\n","      <td>-31.97077</td>\n","      <td>20.03279</td>\n","      <td>-8.07892</td>\n","      <td>-55.12617</td>\n","      <td>26.58961</td>\n","      <td>-10.27183</td>\n","      <td>-30.64232</td>\n","      <td>9.92661</td>\n","      <td>-55.95724</td>\n","      <td>64.92712</td>\n","      <td>-17.72522</td>\n","      <td>-1.49237</td>\n","      <td>-7.50035</td>\n","      <td>51.76631</td>\n","      <td>7.88713</td>\n","      <td>55.66926</td>\n","      <td>28.74903</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>515340</th>\n","      <td>2006</td>\n","      <td>51.28467</td>\n","      <td>45.88068</td>\n","      <td>22.19582</td>\n","      <td>-5.53319</td>\n","      <td>-3.61835</td>\n","      <td>-16.36914</td>\n","      <td>2.12652</td>\n","      <td>5.18160</td>\n","      <td>-8.66890</td>\n","      <td>2.67217</td>\n","      <td>0.45234</td>\n","      <td>2.51380</td>\n","      <td>18.79583</td>\n","      <td>592.17931</td>\n","      <td>619.01842</td>\n","      <td>681.30323</td>\n","      <td>415.21939</td>\n","      <td>639.90327</td>\n","      <td>287.20710</td>\n","      <td>375.31963</td>\n","      <td>212.76265</td>\n","      <td>246.26651</td>\n","      <td>143.48234</td>\n","      <td>217.45556</td>\n","      <td>9.90577</td>\n","      <td>-62.51153</td>\n","      <td>-76.96635</td>\n","      <td>-60.62065</td>\n","      <td>67.81811</td>\n","      <td>-9.20742</td>\n","      <td>-30.73303</td>\n","      <td>21.58525</td>\n","      <td>-31.21664</td>\n","      <td>-36.39659</td>\n","      <td>28.18814</td>\n","      <td>39.46981</td>\n","      <td>-77.13200</td>\n","      <td>-43.39948</td>\n","      <td>-57.69462</td>\n","      <td>...</td>\n","      <td>-74.40960</td>\n","      <td>78.78128</td>\n","      <td>-14.74786</td>\n","      <td>18.02148</td>\n","      <td>-19.61304</td>\n","      <td>-50.34714</td>\n","      <td>87.06521</td>\n","      <td>43.77874</td>\n","      <td>-5.00339</td>\n","      <td>101.08108</td>\n","      <td>-13.34314</td>\n","      <td>-59.17573</td>\n","      <td>-46.22182</td>\n","      <td>-27.10155</td>\n","      <td>-7.07840</td>\n","      <td>23.04732</td>\n","      <td>29.32027</td>\n","      <td>2.10740</td>\n","      <td>-5.77951</td>\n","      <td>2.68326</td>\n","      <td>-13.78081</td>\n","      <td>6.33542</td>\n","      <td>-37.38191</td>\n","      <td>-14.90918</td>\n","      <td>26.87263</td>\n","      <td>7.07232</td>\n","      <td>-127.04955</td>\n","      <td>86.78200</td>\n","      <td>-68.14511</td>\n","      <td>67.44416</td>\n","      <td>4.81440</td>\n","      <td>-3.75991</td>\n","      <td>-30.92584</td>\n","      <td>26.33968</td>\n","      <td>-5.03390</td>\n","      <td>21.86037</td>\n","      <td>-142.29410</td>\n","      <td>3.42901</td>\n","      <td>-41.14721</td>\n","      <td>-15.46052</td>\n","    </tr>\n","    <tr>\n","      <th>515341</th>\n","      <td>2006</td>\n","      <td>49.87870</td>\n","      <td>37.93125</td>\n","      <td>18.65987</td>\n","      <td>-3.63581</td>\n","      <td>-27.75665</td>\n","      <td>-18.52988</td>\n","      <td>7.76108</td>\n","      <td>3.56109</td>\n","      <td>-2.50351</td>\n","      <td>2.20175</td>\n","      <td>-0.58487</td>\n","      <td>-9.78657</td>\n","      <td>35.81410</td>\n","      <td>1047.28364</td>\n","      <td>1451.87226</td>\n","      <td>633.17982</td>\n","      <td>448.46796</td>\n","      <td>826.14418</td>\n","      <td>277.55902</td>\n","      <td>202.20787</td>\n","      <td>241.85866</td>\n","      <td>199.31274</td>\n","      <td>180.60934</td>\n","      <td>168.49980</td>\n","      <td>89.28058</td>\n","      <td>237.30605</td>\n","      <td>-72.22211</td>\n","      <td>-10.02772</td>\n","      <td>-41.24980</td>\n","      <td>-7.59473</td>\n","      <td>-5.23307</td>\n","      <td>24.88978</td>\n","      <td>39.42813</td>\n","      <td>-40.17760</td>\n","      <td>26.51372</td>\n","      <td>79.84191</td>\n","      <td>-15.49724</td>\n","      <td>46.37942</td>\n","      <td>-209.97900</td>\n","      <td>...</td>\n","      <td>-61.06002</td>\n","      <td>50.86072</td>\n","      <td>-3.54799</td>\n","      <td>36.50303</td>\n","      <td>20.94570</td>\n","      <td>-79.43478</td>\n","      <td>-15.49133</td>\n","      <td>17.79165</td>\n","      <td>95.84510</td>\n","      <td>-37.68620</td>\n","      <td>8.51302</td>\n","      <td>13.72492</td>\n","      <td>-71.83419</td>\n","      <td>-191.37407</td>\n","      <td>-34.71662</td>\n","      <td>28.34789</td>\n","      <td>45.25187</td>\n","      <td>17.07862</td>\n","      <td>31.46894</td>\n","      <td>-13.44802</td>\n","      <td>38.68815</td>\n","      <td>109.03046</td>\n","      <td>-42.45525</td>\n","      <td>18.67531</td>\n","      <td>-50.86612</td>\n","      <td>11.26242</td>\n","      <td>59.30165</td>\n","      <td>178.15846</td>\n","      <td>-29.04997</td>\n","      <td>70.22336</td>\n","      <td>32.38589</td>\n","      <td>-32.75535</td>\n","      <td>-61.05473</td>\n","      <td>56.65182</td>\n","      <td>15.29965</td>\n","      <td>95.88193</td>\n","      <td>-10.63242</td>\n","      <td>12.96552</td>\n","      <td>92.11633</td>\n","      <td>10.88815</td>\n","    </tr>\n","    <tr>\n","      <th>515342</th>\n","      <td>2006</td>\n","      <td>45.12852</td>\n","      <td>12.65758</td>\n","      <td>-38.72018</td>\n","      <td>8.80882</td>\n","      <td>-29.29985</td>\n","      <td>-2.28706</td>\n","      <td>-18.40424</td>\n","      <td>-22.28726</td>\n","      <td>-4.52429</td>\n","      <td>-11.46411</td>\n","      <td>3.28514</td>\n","      <td>1.99943</td>\n","      <td>27.77109</td>\n","      <td>1693.72442</td>\n","      <td>3825.48305</td>\n","      <td>2714.53243</td>\n","      <td>1036.34216</td>\n","      <td>1171.81248</td>\n","      <td>468.44308</td>\n","      <td>1042.15436</td>\n","      <td>278.94429</td>\n","      <td>497.83085</td>\n","      <td>423.82729</td>\n","      <td>239.91028</td>\n","      <td>-61.01287</td>\n","      <td>-1383.48696</td>\n","      <td>-1828.43740</td>\n","      <td>-131.54731</td>\n","      <td>138.81510</td>\n","      <td>51.36991</td>\n","      <td>-45.25035</td>\n","      <td>138.31791</td>\n","      <td>-107.60348</td>\n","      <td>-17.01878</td>\n","      <td>-36.53276</td>\n","      <td>226.67213</td>\n","      <td>716.76768</td>\n","      <td>-267.06525</td>\n","      <td>-362.27860</td>\n","      <td>...</td>\n","      <td>191.56779</td>\n","      <td>72.49396</td>\n","      <td>-38.96949</td>\n","      <td>61.22195</td>\n","      <td>24.49062</td>\n","      <td>182.62433</td>\n","      <td>510.41684</td>\n","      <td>-379.38804</td>\n","      <td>226.54992</td>\n","      <td>-201.28237</td>\n","      <td>6.89971</td>\n","      <td>86.07237</td>\n","      <td>-42.85773</td>\n","      <td>-215.01900</td>\n","      <td>88.60866</td>\n","      <td>14.51385</td>\n","      <td>-28.33832</td>\n","      <td>255.17385</td>\n","      <td>14.17125</td>\n","      <td>25.06417</td>\n","      <td>218.85618</td>\n","      <td>-222.53173</td>\n","      <td>35.58546</td>\n","      <td>30.88622</td>\n","      <td>-24.91594</td>\n","      <td>-2.65009</td>\n","      <td>-69.53483</td>\n","      <td>333.67598</td>\n","      <td>-28.24399</td>\n","      <td>202.51566</td>\n","      <td>-18.73598</td>\n","      <td>-71.15954</td>\n","      <td>-123.98443</td>\n","      <td>121.26989</td>\n","      <td>10.89629</td>\n","      <td>34.62409</td>\n","      <td>-248.61020</td>\n","      <td>-6.07171</td>\n","      <td>53.96319</td>\n","      <td>-8.09364</td>\n","    </tr>\n","    <tr>\n","      <th>515343</th>\n","      <td>2006</td>\n","      <td>44.16614</td>\n","      <td>32.38368</td>\n","      <td>-3.34971</td>\n","      <td>-2.49165</td>\n","      <td>-19.59278</td>\n","      <td>-18.67098</td>\n","      <td>8.78428</td>\n","      <td>4.02039</td>\n","      <td>-12.01230</td>\n","      <td>-0.74075</td>\n","      <td>-1.26523</td>\n","      <td>-4.41983</td>\n","      <td>140.44937</td>\n","      <td>2850.23336</td>\n","      <td>1875.28895</td>\n","      <td>1362.98053</td>\n","      <td>784.39737</td>\n","      <td>908.09838</td>\n","      <td>367.12005</td>\n","      <td>692.58547</td>\n","      <td>286.72625</td>\n","      <td>395.46735</td>\n","      <td>221.19089</td>\n","      <td>211.62098</td>\n","      <td>141.17304</td>\n","      <td>647.52054</td>\n","      <td>-451.67671</td>\n","      <td>-170.33993</td>\n","      <td>-106.30851</td>\n","      <td>129.80285</td>\n","      <td>-118.54997</td>\n","      <td>116.14019</td>\n","      <td>-18.36186</td>\n","      <td>-29.42843</td>\n","      <td>13.59803</td>\n","      <td>296.86552</td>\n","      <td>-332.24640</td>\n","      <td>219.84847</td>\n","      <td>-180.27193</td>\n","      <td>...</td>\n","      <td>14.33401</td>\n","      <td>-10.61959</td>\n","      <td>-37.44137</td>\n","      <td>32.72492</td>\n","      <td>-16.62357</td>\n","      <td>-343.07974</td>\n","      <td>148.00075</td>\n","      <td>-64.73672</td>\n","      <td>59.16029</td>\n","      <td>-129.60142</td>\n","      <td>24.47146</td>\n","      <td>-90.78617</td>\n","      <td>-34.58624</td>\n","      <td>-285.37506</td>\n","      <td>-8.78066</td>\n","      <td>63.91160</td>\n","      <td>58.86067</td>\n","      <td>43.28537</td>\n","      <td>22.69472</td>\n","      <td>-0.93940</td>\n","      <td>417.76862</td>\n","      <td>78.26912</td>\n","      <td>-173.95232</td>\n","      <td>-35.42845</td>\n","      <td>10.59859</td>\n","      <td>-16.51518</td>\n","      <td>157.75671</td>\n","      <td>294.31838</td>\n","      <td>-37.30155</td>\n","      <td>80.00327</td>\n","      <td>67.16763</td>\n","      <td>282.77624</td>\n","      <td>-4.63677</td>\n","      <td>144.00125</td>\n","      <td>21.62652</td>\n","      <td>-29.72432</td>\n","      <td>71.47198</td>\n","      <td>20.32240</td>\n","      <td>14.83107</td>\n","      <td>39.74909</td>\n","    </tr>\n","    <tr>\n","      <th>515344</th>\n","      <td>2005</td>\n","      <td>51.85726</td>\n","      <td>59.11655</td>\n","      <td>26.39436</td>\n","      <td>-5.46030</td>\n","      <td>-20.69012</td>\n","      <td>-19.95528</td>\n","      <td>-6.72771</td>\n","      <td>2.29590</td>\n","      <td>10.31018</td>\n","      <td>6.26597</td>\n","      <td>-1.78800</td>\n","      <td>-6.19786</td>\n","      <td>20.16600</td>\n","      <td>598.45275</td>\n","      <td>1140.69539</td>\n","      <td>721.49244</td>\n","      <td>272.84841</td>\n","      <td>564.06690</td>\n","      <td>199.41547</td>\n","      <td>189.04637</td>\n","      <td>217.32042</td>\n","      <td>137.13390</td>\n","      <td>150.34608</td>\n","      <td>98.21589</td>\n","      <td>48.12644</td>\n","      <td>-601.59295</td>\n","      <td>10.58466</td>\n","      <td>-83.35368</td>\n","      <td>96.86756</td>\n","      <td>69.40708</td>\n","      <td>8.06033</td>\n","      <td>-26.01693</td>\n","      <td>-2.93173</td>\n","      <td>26.18398</td>\n","      <td>-12.24660</td>\n","      <td>-14.52391</td>\n","      <td>-121.61676</td>\n","      <td>119.15632</td>\n","      <td>-229.55722</td>\n","      <td>...</td>\n","      <td>1.78072</td>\n","      <td>64.75548</td>\n","      <td>24.55866</td>\n","      <td>-1.12509</td>\n","      <td>-13.58287</td>\n","      <td>-99.66038</td>\n","      <td>-124.73875</td>\n","      <td>67.02630</td>\n","      <td>33.05618</td>\n","      <td>60.25818</td>\n","      <td>28.00288</td>\n","      <td>10.62425</td>\n","      <td>-8.86772</td>\n","      <td>78.13543</td>\n","      <td>-181.10013</td>\n","      <td>74.69489</td>\n","      <td>57.45083</td>\n","      <td>114.08816</td>\n","      <td>-9.91322</td>\n","      <td>7.53612</td>\n","      <td>97.06395</td>\n","      <td>233.17754</td>\n","      <td>-100.68441</td>\n","      <td>27.67012</td>\n","      <td>-37.33008</td>\n","      <td>-0.34676</td>\n","      <td>-207.78766</td>\n","      <td>116.75005</td>\n","      <td>-91.82912</td>\n","      <td>8.35020</td>\n","      <td>-11.50511</td>\n","      <td>-69.18291</td>\n","      <td>60.58456</td>\n","      <td>28.64599</td>\n","      <td>-4.39620</td>\n","      <td>-64.56491</td>\n","      <td>-45.61012</td>\n","      <td>-5.51512</td>\n","      <td>32.35602</td>\n","      <td>12.17352</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>515345 rows × 91 columns</p>\n","</div>"],"text/plain":["        year      var1      var2  ...     var88      var89     var90\n","0       2001  49.94357  21.47114  ...  -1.82223  -27.46348   2.26327\n","1       2001  48.73215  18.42930  ...  12.04941   58.43453  26.92061\n","2       2001  50.95714  31.85602  ...  -0.05859   39.67068  -0.66345\n","3       2001  48.24750  -1.89837  ...   9.90558  199.62971  18.85382\n","4       2001  50.97020  42.20998  ...   7.88713   55.66926  28.74903\n","...      ...       ...       ...  ...       ...        ...       ...\n","515340  2006  51.28467  45.88068  ...   3.42901  -41.14721 -15.46052\n","515341  2006  49.87870  37.93125  ...  12.96552   92.11633  10.88815\n","515342  2006  45.12852  12.65758  ...  -6.07171   53.96319  -8.09364\n","515343  2006  44.16614  32.38368  ...  20.32240   14.83107  39.74909\n","515344  2005  51.85726  59.11655  ...  -5.51512   32.35602  12.17352\n","\n","[515345 rows x 91 columns]"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"KaLuAMH_zGrA"},"source":["To set up our data for classification, we'll use the \"year\" field to represent\n","whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n","the year was released after 2000, and 0 otherwise."]},{"cell_type":"code","metadata":{"id":"tZdGlNgdzGrA"},"source":["df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xugy7FZ8eoAd","colab":{"base_uri":"https://localhost:8080/","height":732},"executionInfo":{"status":"ok","timestamp":1636559846636,"user_tz":-120,"elapsed":412,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"2a1fde93-a9fc-4a22-f840-e6ff85c13520"},"source":["df.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>year</th>\n","      <th>var1</th>\n","      <th>var2</th>\n","      <th>var3</th>\n","      <th>var4</th>\n","      <th>var5</th>\n","      <th>var6</th>\n","      <th>var7</th>\n","      <th>var8</th>\n","      <th>var9</th>\n","      <th>var10</th>\n","      <th>var11</th>\n","      <th>var12</th>\n","      <th>var13</th>\n","      <th>var14</th>\n","      <th>var15</th>\n","      <th>var16</th>\n","      <th>var17</th>\n","      <th>var18</th>\n","      <th>var19</th>\n","      <th>var20</th>\n","      <th>var21</th>\n","      <th>var22</th>\n","      <th>var23</th>\n","      <th>var24</th>\n","      <th>var25</th>\n","      <th>var26</th>\n","      <th>var27</th>\n","      <th>var28</th>\n","      <th>var29</th>\n","      <th>var30</th>\n","      <th>var31</th>\n","      <th>var32</th>\n","      <th>var33</th>\n","      <th>var34</th>\n","      <th>var35</th>\n","      <th>var36</th>\n","      <th>var37</th>\n","      <th>var38</th>\n","      <th>var39</th>\n","      <th>...</th>\n","      <th>var51</th>\n","      <th>var52</th>\n","      <th>var53</th>\n","      <th>var54</th>\n","      <th>var55</th>\n","      <th>var56</th>\n","      <th>var57</th>\n","      <th>var58</th>\n","      <th>var59</th>\n","      <th>var60</th>\n","      <th>var61</th>\n","      <th>var62</th>\n","      <th>var63</th>\n","      <th>var64</th>\n","      <th>var65</th>\n","      <th>var66</th>\n","      <th>var67</th>\n","      <th>var68</th>\n","      <th>var69</th>\n","      <th>var70</th>\n","      <th>var71</th>\n","      <th>var72</th>\n","      <th>var73</th>\n","      <th>var74</th>\n","      <th>var75</th>\n","      <th>var76</th>\n","      <th>var77</th>\n","      <th>var78</th>\n","      <th>var79</th>\n","      <th>var80</th>\n","      <th>var81</th>\n","      <th>var82</th>\n","      <th>var83</th>\n","      <th>var84</th>\n","      <th>var85</th>\n","      <th>var86</th>\n","      <th>var87</th>\n","      <th>var88</th>\n","      <th>var89</th>\n","      <th>var90</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>49.94357</td>\n","      <td>21.47114</td>\n","      <td>73.07750</td>\n","      <td>8.74861</td>\n","      <td>-17.40628</td>\n","      <td>-13.09905</td>\n","      <td>-25.01202</td>\n","      <td>-12.23257</td>\n","      <td>7.83089</td>\n","      <td>-2.46783</td>\n","      <td>3.32136</td>\n","      <td>-2.31521</td>\n","      <td>10.20556</td>\n","      <td>611.10913</td>\n","      <td>951.08960</td>\n","      <td>698.11428</td>\n","      <td>408.98485</td>\n","      <td>383.70912</td>\n","      <td>326.51512</td>\n","      <td>238.11327</td>\n","      <td>251.42414</td>\n","      <td>187.17351</td>\n","      <td>100.42652</td>\n","      <td>179.19498</td>\n","      <td>-8.41558</td>\n","      <td>-317.87038</td>\n","      <td>95.86266</td>\n","      <td>48.10259</td>\n","      <td>-95.66303</td>\n","      <td>-18.06215</td>\n","      <td>1.96984</td>\n","      <td>34.42438</td>\n","      <td>11.72670</td>\n","      <td>1.36790</td>\n","      <td>7.79444</td>\n","      <td>-0.36994</td>\n","      <td>-133.67852</td>\n","      <td>-83.26165</td>\n","      <td>-37.29765</td>\n","      <td>...</td>\n","      <td>-25.38187</td>\n","      <td>-3.90772</td>\n","      <td>13.29258</td>\n","      <td>41.55060</td>\n","      <td>-7.26272</td>\n","      <td>-21.00863</td>\n","      <td>105.50848</td>\n","      <td>64.29856</td>\n","      <td>26.08481</td>\n","      <td>-44.59110</td>\n","      <td>-8.30657</td>\n","      <td>7.93706</td>\n","      <td>-10.73660</td>\n","      <td>-95.44766</td>\n","      <td>-82.03307</td>\n","      <td>-35.59194</td>\n","      <td>4.69525</td>\n","      <td>70.95626</td>\n","      <td>28.09139</td>\n","      <td>6.02015</td>\n","      <td>-37.13767</td>\n","      <td>-41.12450</td>\n","      <td>-8.40816</td>\n","      <td>7.19877</td>\n","      <td>-8.60176</td>\n","      <td>-5.90857</td>\n","      <td>-12.32437</td>\n","      <td>14.68734</td>\n","      <td>-54.32125</td>\n","      <td>40.14786</td>\n","      <td>13.01620</td>\n","      <td>-54.40548</td>\n","      <td>58.99367</td>\n","      <td>15.37344</td>\n","      <td>1.11144</td>\n","      <td>-23.08793</td>\n","      <td>68.40795</td>\n","      <td>-1.82223</td>\n","      <td>-27.46348</td>\n","      <td>2.26327</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>48.73215</td>\n","      <td>18.42930</td>\n","      <td>70.32679</td>\n","      <td>12.94636</td>\n","      <td>-10.32437</td>\n","      <td>-24.83777</td>\n","      <td>8.76630</td>\n","      <td>-0.92019</td>\n","      <td>18.76548</td>\n","      <td>4.59210</td>\n","      <td>2.21920</td>\n","      <td>0.34006</td>\n","      <td>44.38997</td>\n","      <td>2056.93836</td>\n","      <td>605.40696</td>\n","      <td>457.41175</td>\n","      <td>777.15347</td>\n","      <td>415.64880</td>\n","      <td>746.47775</td>\n","      <td>366.45320</td>\n","      <td>317.82946</td>\n","      <td>273.07917</td>\n","      <td>141.75921</td>\n","      <td>317.35269</td>\n","      <td>19.48271</td>\n","      <td>-65.25496</td>\n","      <td>162.75145</td>\n","      <td>135.00765</td>\n","      <td>-96.28436</td>\n","      <td>-86.87955</td>\n","      <td>17.38087</td>\n","      <td>45.90742</td>\n","      <td>32.49908</td>\n","      <td>-32.85429</td>\n","      <td>45.10830</td>\n","      <td>26.84939</td>\n","      <td>-302.57328</td>\n","      <td>-41.71932</td>\n","      <td>-138.85034</td>\n","      <td>...</td>\n","      <td>28.55107</td>\n","      <td>1.52298</td>\n","      <td>70.99515</td>\n","      <td>-43.63073</td>\n","      <td>-42.55014</td>\n","      <td>129.82848</td>\n","      <td>79.95420</td>\n","      <td>-87.14554</td>\n","      <td>-45.75446</td>\n","      <td>-65.82100</td>\n","      <td>-43.90031</td>\n","      <td>-19.45705</td>\n","      <td>12.59163</td>\n","      <td>-407.64130</td>\n","      <td>42.91189</td>\n","      <td>12.15850</td>\n","      <td>-88.37882</td>\n","      <td>42.25246</td>\n","      <td>46.49209</td>\n","      <td>-30.17747</td>\n","      <td>45.98495</td>\n","      <td>130.47892</td>\n","      <td>13.88281</td>\n","      <td>-4.00055</td>\n","      <td>17.85965</td>\n","      <td>-18.32138</td>\n","      <td>-87.99109</td>\n","      <td>14.37524</td>\n","      <td>-22.70119</td>\n","      <td>-58.81266</td>\n","      <td>5.66812</td>\n","      <td>-19.68073</td>\n","      <td>33.04964</td>\n","      <td>42.87836</td>\n","      <td>-9.90378</td>\n","      <td>-32.22788</td>\n","      <td>70.49388</td>\n","      <td>12.04941</td>\n","      <td>58.43453</td>\n","      <td>26.92061</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>50.95714</td>\n","      <td>31.85602</td>\n","      <td>55.81851</td>\n","      <td>13.41693</td>\n","      <td>-6.57898</td>\n","      <td>-18.54940</td>\n","      <td>-3.27872</td>\n","      <td>-2.35035</td>\n","      <td>16.07017</td>\n","      <td>1.39518</td>\n","      <td>2.73553</td>\n","      <td>0.82804</td>\n","      <td>7.46586</td>\n","      <td>699.54544</td>\n","      <td>1016.00954</td>\n","      <td>594.06748</td>\n","      <td>355.73663</td>\n","      <td>507.39931</td>\n","      <td>387.69910</td>\n","      <td>287.15347</td>\n","      <td>112.37152</td>\n","      <td>161.68928</td>\n","      <td>144.14353</td>\n","      <td>199.29693</td>\n","      <td>-4.24359</td>\n","      <td>-297.00587</td>\n","      <td>-148.36392</td>\n","      <td>-7.94726</td>\n","      <td>-18.71630</td>\n","      <td>12.77542</td>\n","      <td>-25.37725</td>\n","      <td>9.71410</td>\n","      <td>0.13843</td>\n","      <td>26.79723</td>\n","      <td>6.30760</td>\n","      <td>28.70107</td>\n","      <td>-74.89005</td>\n","      <td>-289.19553</td>\n","      <td>-166.26089</td>\n","      <td>...</td>\n","      <td>18.50939</td>\n","      <td>16.97216</td>\n","      <td>24.26629</td>\n","      <td>-10.50788</td>\n","      <td>-8.68412</td>\n","      <td>54.75759</td>\n","      <td>194.74034</td>\n","      <td>7.95966</td>\n","      <td>-18.22685</td>\n","      <td>0.06463</td>\n","      <td>-2.63069</td>\n","      <td>26.02561</td>\n","      <td>1.75729</td>\n","      <td>-262.36917</td>\n","      <td>-233.60089</td>\n","      <td>-2.50502</td>\n","      <td>-12.14279</td>\n","      <td>81.37617</td>\n","      <td>2.07554</td>\n","      <td>-1.82381</td>\n","      <td>183.65292</td>\n","      <td>22.64797</td>\n","      <td>-39.98887</td>\n","      <td>43.37381</td>\n","      <td>-31.56737</td>\n","      <td>-4.88840</td>\n","      <td>-36.53213</td>\n","      <td>-23.94662</td>\n","      <td>-84.19275</td>\n","      <td>66.00518</td>\n","      <td>3.03800</td>\n","      <td>26.05866</td>\n","      <td>-50.92779</td>\n","      <td>10.93792</td>\n","      <td>-0.07568</td>\n","      <td>43.20130</td>\n","      <td>-115.00698</td>\n","      <td>-0.05859</td>\n","      <td>39.67068</td>\n","      <td>-0.66345</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>48.24750</td>\n","      <td>-1.89837</td>\n","      <td>36.29772</td>\n","      <td>2.58776</td>\n","      <td>0.97170</td>\n","      <td>-26.21683</td>\n","      <td>5.05097</td>\n","      <td>-10.34124</td>\n","      <td>3.55005</td>\n","      <td>-6.36304</td>\n","      <td>6.63016</td>\n","      <td>-3.35142</td>\n","      <td>37.64085</td>\n","      <td>2174.08189</td>\n","      <td>697.43346</td>\n","      <td>459.24587</td>\n","      <td>742.78961</td>\n","      <td>229.30783</td>\n","      <td>387.89697</td>\n","      <td>249.06662</td>\n","      <td>245.89870</td>\n","      <td>176.20527</td>\n","      <td>98.82222</td>\n","      <td>150.97286</td>\n","      <td>78.49057</td>\n","      <td>-62.00282</td>\n","      <td>43.49659</td>\n","      <td>-96.42719</td>\n","      <td>-108.96608</td>\n","      <td>14.22854</td>\n","      <td>14.54178</td>\n","      <td>-23.55608</td>\n","      <td>-39.36953</td>\n","      <td>-43.59209</td>\n","      <td>20.83714</td>\n","      <td>35.63919</td>\n","      <td>-181.34947</td>\n","      <td>-93.66614</td>\n","      <td>-90.55616</td>\n","      <td>...</td>\n","      <td>4.56917</td>\n","      <td>-37.32280</td>\n","      <td>4.15159</td>\n","      <td>12.24315</td>\n","      <td>35.02697</td>\n","      <td>-178.89573</td>\n","      <td>82.46573</td>\n","      <td>-20.49425</td>\n","      <td>101.78577</td>\n","      <td>-19.77808</td>\n","      <td>-21.52657</td>\n","      <td>3.36303</td>\n","      <td>-11.63176</td>\n","      <td>51.55411</td>\n","      <td>-50.57576</td>\n","      <td>-28.14755</td>\n","      <td>-83.15795</td>\n","      <td>-7.35260</td>\n","      <td>-22.11505</td>\n","      <td>1.18279</td>\n","      <td>-122.70467</td>\n","      <td>150.57360</td>\n","      <td>24.37468</td>\n","      <td>41.19821</td>\n","      <td>-37.04318</td>\n","      <td>-28.72986</td>\n","      <td>162.19614</td>\n","      <td>22.18309</td>\n","      <td>-8.63509</td>\n","      <td>85.23416</td>\n","      <td>34.57337</td>\n","      <td>-171.70734</td>\n","      <td>-16.96705</td>\n","      <td>-46.67617</td>\n","      <td>-12.51516</td>\n","      <td>82.58061</td>\n","      <td>-72.08993</td>\n","      <td>9.90558</td>\n","      <td>199.62971</td>\n","      <td>18.85382</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>50.97020</td>\n","      <td>42.20998</td>\n","      <td>67.09964</td>\n","      <td>8.46791</td>\n","      <td>-15.85279</td>\n","      <td>-16.81409</td>\n","      <td>-12.48207</td>\n","      <td>-9.37636</td>\n","      <td>12.63699</td>\n","      <td>0.93609</td>\n","      <td>1.60923</td>\n","      <td>2.19223</td>\n","      <td>47.32082</td>\n","      <td>894.28471</td>\n","      <td>809.86615</td>\n","      <td>318.78559</td>\n","      <td>435.04497</td>\n","      <td>341.61467</td>\n","      <td>334.30734</td>\n","      <td>322.99589</td>\n","      <td>190.61921</td>\n","      <td>235.84715</td>\n","      <td>96.89517</td>\n","      <td>210.58870</td>\n","      <td>5.60463</td>\n","      <td>-199.63958</td>\n","      <td>204.85812</td>\n","      <td>-77.17695</td>\n","      <td>-65.79741</td>\n","      <td>-6.95097</td>\n","      <td>-12.15262</td>\n","      <td>-3.85410</td>\n","      <td>20.68990</td>\n","      <td>-20.30480</td>\n","      <td>37.15045</td>\n","      <td>11.20673</td>\n","      <td>-124.09519</td>\n","      <td>-295.98542</td>\n","      <td>-33.31169</td>\n","      <td>...</td>\n","      <td>45.25506</td>\n","      <td>10.42226</td>\n","      <td>27.88782</td>\n","      <td>-17.12676</td>\n","      <td>-31.54772</td>\n","      <td>-76.86293</td>\n","      <td>41.17343</td>\n","      <td>-138.32535</td>\n","      <td>-53.96905</td>\n","      <td>-21.30266</td>\n","      <td>-24.87362</td>\n","      <td>-2.46595</td>\n","      <td>-4.05003</td>\n","      <td>-56.51161</td>\n","      <td>-34.56445</td>\n","      <td>-5.07092</td>\n","      <td>-47.75605</td>\n","      <td>64.81513</td>\n","      <td>-97.42948</td>\n","      <td>-12.59418</td>\n","      <td>55.23699</td>\n","      <td>28.85657</td>\n","      <td>54.53513</td>\n","      <td>-31.97077</td>\n","      <td>20.03279</td>\n","      <td>-8.07892</td>\n","      <td>-55.12617</td>\n","      <td>26.58961</td>\n","      <td>-10.27183</td>\n","      <td>-30.64232</td>\n","      <td>9.92661</td>\n","      <td>-55.95724</td>\n","      <td>64.92712</td>\n","      <td>-17.72522</td>\n","      <td>-1.49237</td>\n","      <td>-7.50035</td>\n","      <td>51.76631</td>\n","      <td>7.88713</td>\n","      <td>55.66926</td>\n","      <td>28.74903</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>50.54767</td>\n","      <td>0.31568</td>\n","      <td>92.35066</td>\n","      <td>22.38696</td>\n","      <td>-25.51870</td>\n","      <td>-19.04928</td>\n","      <td>20.67345</td>\n","      <td>-5.19943</td>\n","      <td>3.63566</td>\n","      <td>-4.69088</td>\n","      <td>2.49578</td>\n","      <td>-3.02468</td>\n","      <td>7.69273</td>\n","      <td>1004.95615</td>\n","      <td>785.06709</td>\n","      <td>591.99232</td>\n","      <td>495.75332</td>\n","      <td>291.38165</td>\n","      <td>434.08355</td>\n","      <td>291.55265</td>\n","      <td>303.58860</td>\n","      <td>216.12189</td>\n","      <td>126.10703</td>\n","      <td>147.28090</td>\n","      <td>6.17712</td>\n","      <td>-133.67424</td>\n","      <td>8.69259</td>\n","      <td>-13.76138</td>\n","      <td>-52.72072</td>\n","      <td>-24.00961</td>\n","      <td>-10.82297</td>\n","      <td>11.42039</td>\n","      <td>32.10442</td>\n","      <td>-26.26130</td>\n","      <td>-4.98856</td>\n","      <td>20.84154</td>\n","      <td>-129.69145</td>\n","      <td>-157.91059</td>\n","      <td>-43.29252</td>\n","      <td>...</td>\n","      <td>45.07251</td>\n","      <td>-6.20682</td>\n","      <td>17.53456</td>\n","      <td>43.66313</td>\n","      <td>-4.80024</td>\n","      <td>-33.62226</td>\n","      <td>-15.01731</td>\n","      <td>-33.74416</td>\n","      <td>2.12072</td>\n","      <td>-46.83225</td>\n","      <td>19.99596</td>\n","      <td>26.31683</td>\n","      <td>0.51657</td>\n","      <td>-285.36622</td>\n","      <td>-52.26245</td>\n","      <td>-13.00341</td>\n","      <td>-9.41185</td>\n","      <td>69.69108</td>\n","      <td>33.54907</td>\n","      <td>-11.39665</td>\n","      <td>93.61103</td>\n","      <td>163.39892</td>\n","      <td>5.00362</td>\n","      <td>15.23807</td>\n","      <td>-13.48394</td>\n","      <td>-6.37431</td>\n","      <td>-37.31287</td>\n","      <td>92.47370</td>\n","      <td>-90.00149</td>\n","      <td>47.25143</td>\n","      <td>6.59753</td>\n","      <td>-50.69577</td>\n","      <td>26.02574</td>\n","      <td>18.94430</td>\n","      <td>-0.33730</td>\n","      <td>6.09352</td>\n","      <td>35.18381</td>\n","      <td>5.00283</td>\n","      <td>-11.02257</td>\n","      <td>0.02263</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>50.57546</td>\n","      <td>33.17843</td>\n","      <td>50.53517</td>\n","      <td>11.55217</td>\n","      <td>-27.24764</td>\n","      <td>-8.78206</td>\n","      <td>-12.04282</td>\n","      <td>-9.53930</td>\n","      <td>28.61811</td>\n","      <td>8.25435</td>\n","      <td>-0.43743</td>\n","      <td>5.66265</td>\n","      <td>11.07787</td>\n","      <td>1080.98902</td>\n","      <td>1230.78393</td>\n","      <td>1301.63542</td>\n","      <td>952.84686</td>\n","      <td>783.02498</td>\n","      <td>560.79536</td>\n","      <td>696.19620</td>\n","      <td>253.36266</td>\n","      <td>316.92697</td>\n","      <td>151.75689</td>\n","      <td>144.07059</td>\n","      <td>-3.02894</td>\n","      <td>-111.65251</td>\n","      <td>-56.64580</td>\n","      <td>464.86598</td>\n","      <td>150.52166</td>\n","      <td>84.69609</td>\n","      <td>-91.71196</td>\n","      <td>89.31272</td>\n","      <td>16.49867</td>\n","      <td>-4.47074</td>\n","      <td>-2.02539</td>\n","      <td>13.27637</td>\n","      <td>-153.73456</td>\n","      <td>199.01552</td>\n","      <td>-278.79072</td>\n","      <td>...</td>\n","      <td>0.05909</td>\n","      <td>-92.07551</td>\n","      <td>7.80480</td>\n","      <td>-46.15966</td>\n","      <td>-39.03309</td>\n","      <td>32.52065</td>\n","      <td>164.15989</td>\n","      <td>-247.22025</td>\n","      <td>-100.28773</td>\n","      <td>-55.58712</td>\n","      <td>8.38343</td>\n","      <td>-4.57294</td>\n","      <td>-20.08525</td>\n","      <td>-357.00069</td>\n","      <td>-232.78978</td>\n","      <td>-112.81679</td>\n","      <td>-66.16128</td>\n","      <td>43.25003</td>\n","      <td>18.48417</td>\n","      <td>-2.50274</td>\n","      <td>3.25927</td>\n","      <td>94.57509</td>\n","      <td>-24.31254</td>\n","      <td>62.97582</td>\n","      <td>-19.41809</td>\n","      <td>10.35282</td>\n","      <td>-91.89392</td>\n","      <td>10.51922</td>\n","      <td>-74.98521</td>\n","      <td>12.29948</td>\n","      <td>11.63681</td>\n","      <td>25.44182</td>\n","      <td>134.62382</td>\n","      <td>21.51982</td>\n","      <td>8.17570</td>\n","      <td>35.46251</td>\n","      <td>11.57736</td>\n","      <td>4.50056</td>\n","      <td>-4.62739</td>\n","      <td>1.40192</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>48.26892</td>\n","      <td>8.97526</td>\n","      <td>75.23158</td>\n","      <td>24.04945</td>\n","      <td>-16.02105</td>\n","      <td>-14.09491</td>\n","      <td>8.11871</td>\n","      <td>-1.87566</td>\n","      <td>7.46701</td>\n","      <td>1.18189</td>\n","      <td>1.46625</td>\n","      <td>-6.34226</td>\n","      <td>8.40470</td>\n","      <td>1619.74629</td>\n","      <td>1762.43083</td>\n","      <td>714.83843</td>\n","      <td>1115.90792</td>\n","      <td>525.85703</td>\n","      <td>576.66300</td>\n","      <td>350.41961</td>\n","      <td>315.44672</td>\n","      <td>267.94363</td>\n","      <td>198.28158</td>\n","      <td>201.97524</td>\n","      <td>17.14701</td>\n","      <td>-517.47978</td>\n","      <td>-11.33396</td>\n","      <td>59.52820</td>\n","      <td>-25.10400</td>\n","      <td>23.48782</td>\n","      <td>12.63888</td>\n","      <td>40.85580</td>\n","      <td>-3.72243</td>\n","      <td>-16.97392</td>\n","      <td>0.04284</td>\n","      <td>59.45956</td>\n","      <td>-191.92216</td>\n","      <td>445.36223</td>\n","      <td>-120.62667</td>\n","      <td>...</td>\n","      <td>63.71442</td>\n","      <td>3.70732</td>\n","      <td>-9.36662</td>\n","      <td>-25.75461</td>\n","      <td>35.16677</td>\n","      <td>-33.17382</td>\n","      <td>13.80469</td>\n","      <td>-107.14403</td>\n","      <td>140.04250</td>\n","      <td>-124.16153</td>\n","      <td>-18.33032</td>\n","      <td>-9.99397</td>\n","      <td>8.96011</td>\n","      <td>-411.27991</td>\n","      <td>-99.75061</td>\n","      <td>-75.51735</td>\n","      <td>-88.57128</td>\n","      <td>22.90222</td>\n","      <td>4.14618</td>\n","      <td>-16.83238</td>\n","      <td>142.14168</td>\n","      <td>326.91932</td>\n","      <td>-3.49405</td>\n","      <td>7.58991</td>\n","      <td>-32.60725</td>\n","      <td>-4.44469</td>\n","      <td>-56.48952</td>\n","      <td>-31.19491</td>\n","      <td>-32.75384</td>\n","      <td>-52.97111</td>\n","      <td>18.03989</td>\n","      <td>-58.46192</td>\n","      <td>-65.56438</td>\n","      <td>46.99856</td>\n","      <td>-4.09602</td>\n","      <td>56.37650</td>\n","      <td>-18.29975</td>\n","      <td>-0.30633</td>\n","      <td>3.98364</td>\n","      <td>-3.72556</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>49.75468</td>\n","      <td>33.99581</td>\n","      <td>56.73846</td>\n","      <td>2.89581</td>\n","      <td>-2.92429</td>\n","      <td>-26.44413</td>\n","      <td>1.71392</td>\n","      <td>-0.55644</td>\n","      <td>22.08594</td>\n","      <td>7.43847</td>\n","      <td>-0.03578</td>\n","      <td>1.66534</td>\n","      <td>46.02060</td>\n","      <td>789.58109</td>\n","      <td>607.78514</td>\n","      <td>248.54603</td>\n","      <td>288.44004</td>\n","      <td>330.16459</td>\n","      <td>356.34405</td>\n","      <td>194.21792</td>\n","      <td>194.68297</td>\n","      <td>158.43568</td>\n","      <td>104.05610</td>\n","      <td>183.10695</td>\n","      <td>-39.54223</td>\n","      <td>-225.39832</td>\n","      <td>78.57149</td>\n","      <td>20.62209</td>\n","      <td>-9.44189</td>\n","      <td>15.94754</td>\n","      <td>51.24174</td>\n","      <td>-4.36054</td>\n","      <td>17.60516</td>\n","      <td>9.03638</td>\n","      <td>16.69044</td>\n","      <td>42.70944</td>\n","      <td>-77.62818</td>\n","      <td>46.71905</td>\n","      <td>-109.71028</td>\n","      <td>...</td>\n","      <td>36.17475</td>\n","      <td>3.44547</td>\n","      <td>46.40983</td>\n","      <td>-21.52684</td>\n","      <td>-10.50872</td>\n","      <td>-8.62787</td>\n","      <td>55.23069</td>\n","      <td>-41.74452</td>\n","      <td>-20.24522</td>\n","      <td>21.78558</td>\n","      <td>-8.69348</td>\n","      <td>-1.85679</td>\n","      <td>3.60117</td>\n","      <td>-53.94252</td>\n","      <td>-54.66970</td>\n","      <td>-15.14957</td>\n","      <td>-46.61675</td>\n","      <td>48.45009</td>\n","      <td>2.77737</td>\n","      <td>-7.80854</td>\n","      <td>38.29390</td>\n","      <td>120.35575</td>\n","      <td>23.84978</td>\n","      <td>14.87696</td>\n","      <td>-41.76961</td>\n","      <td>-21.05519</td>\n","      <td>-45.79645</td>\n","      <td>9.16222</td>\n","      <td>-21.48052</td>\n","      <td>-9.70822</td>\n","      <td>18.70812</td>\n","      <td>5.20391</td>\n","      <td>-27.75192</td>\n","      <td>17.22100</td>\n","      <td>-0.85210</td>\n","      <td>-15.67150</td>\n","      <td>-26.36257</td>\n","      <td>5.48708</td>\n","      <td>-9.13495</td>\n","      <td>6.08680</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>45.17809</td>\n","      <td>46.34234</td>\n","      <td>-40.65357</td>\n","      <td>-2.47909</td>\n","      <td>1.21253</td>\n","      <td>-0.65302</td>\n","      <td>-6.95536</td>\n","      <td>-12.20040</td>\n","      <td>17.02512</td>\n","      <td>2.00002</td>\n","      <td>-1.87785</td>\n","      <td>9.85499</td>\n","      <td>25.59837</td>\n","      <td>1905.18577</td>\n","      <td>3676.09074</td>\n","      <td>1976.85531</td>\n","      <td>913.11216</td>\n","      <td>1957.52415</td>\n","      <td>955.98525</td>\n","      <td>942.72667</td>\n","      <td>439.85991</td>\n","      <td>591.66138</td>\n","      <td>493.40770</td>\n","      <td>496.38516</td>\n","      <td>33.94285</td>\n","      <td>-255.90134</td>\n","      <td>-762.28079</td>\n","      <td>-66.10935</td>\n","      <td>-128.02217</td>\n","      <td>198.12908</td>\n","      <td>-34.44957</td>\n","      <td>176.00397</td>\n","      <td>-140.80069</td>\n","      <td>-22.56380</td>\n","      <td>12.77945</td>\n","      <td>193.30164</td>\n","      <td>314.20949</td>\n","      <td>576.29519</td>\n","      <td>-429.58643</td>\n","      <td>...</td>\n","      <td>-17.48938</td>\n","      <td>75.58779</td>\n","      <td>93.29243</td>\n","      <td>85.83507</td>\n","      <td>47.13972</td>\n","      <td>312.85482</td>\n","      <td>135.50478</td>\n","      <td>-32.47886</td>\n","      <td>49.67063</td>\n","      <td>-214.73180</td>\n","      <td>-77.83503</td>\n","      <td>-47.26902</td>\n","      <td>7.58366</td>\n","      <td>-352.56581</td>\n","      <td>-36.15655</td>\n","      <td>-53.39933</td>\n","      <td>-98.60417</td>\n","      <td>-82.37799</td>\n","      <td>45.81588</td>\n","      <td>-16.91676</td>\n","      <td>18.35888</td>\n","      <td>-315.68965</td>\n","      <td>-3.14554</td>\n","      <td>125.45269</td>\n","      <td>-130.18808</td>\n","      <td>-3.06337</td>\n","      <td>42.26602</td>\n","      <td>-9.04929</td>\n","      <td>26.41570</td>\n","      <td>23.36165</td>\n","      <td>-4.36742</td>\n","      <td>-87.55285</td>\n","      <td>-70.79677</td>\n","      <td>76.57355</td>\n","      <td>-7.71727</td>\n","      <td>3.26926</td>\n","      <td>-298.49845</td>\n","      <td>11.49326</td>\n","      <td>-89.21804</td>\n","      <td>-15.09719</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>39.13076</td>\n","      <td>-23.01763</td>\n","      <td>-36.20583</td>\n","      <td>1.67519</td>\n","      <td>-4.27101</td>\n","      <td>13.01158</td>\n","      <td>8.05718</td>\n","      <td>-8.41088</td>\n","      <td>6.27370</td>\n","      <td>-7.81564</td>\n","      <td>-12.29472</td>\n","      <td>-12.26186</td>\n","      <td>45.05025</td>\n","      <td>2022.36720</td>\n","      <td>3860.79421</td>\n","      <td>3186.91395</td>\n","      <td>849.65196</td>\n","      <td>2835.16969</td>\n","      <td>847.01876</td>\n","      <td>1001.27685</td>\n","      <td>347.33791</td>\n","      <td>500.59470</td>\n","      <td>536.16777</td>\n","      <td>318.73821</td>\n","      <td>50.13600</td>\n","      <td>451.14001</td>\n","      <td>-589.91962</td>\n","      <td>-95.42003</td>\n","      <td>-99.51192</td>\n","      <td>-28.43500</td>\n","      <td>22.34786</td>\n","      <td>132.68163</td>\n","      <td>-111.84550</td>\n","      <td>106.94141</td>\n","      <td>-9.94676</td>\n","      <td>269.57001</td>\n","      <td>416.04429</td>\n","      <td>3.08594</td>\n","      <td>-312.96128</td>\n","      <td>...</td>\n","      <td>364.86126</td>\n","      <td>4.89096</td>\n","      <td>-110.07105</td>\n","      <td>19.43169</td>\n","      <td>-13.69554</td>\n","      <td>464.30985</td>\n","      <td>203.55456</td>\n","      <td>-450.84974</td>\n","      <td>9.54696</td>\n","      <td>-433.98556</td>\n","      <td>40.33083</td>\n","      <td>60.29120</td>\n","      <td>66.28236</td>\n","      <td>-398.01020</td>\n","      <td>582.88308</td>\n","      <td>-93.75781</td>\n","      <td>-31.61288</td>\n","      <td>-311.99559</td>\n","      <td>106.91372</td>\n","      <td>-8.56906</td>\n","      <td>-42.26640</td>\n","      <td>269.83186</td>\n","      <td>10.39815</td>\n","      <td>-14.56199</td>\n","      <td>36.15556</td>\n","      <td>51.77120</td>\n","      <td>23.70949</td>\n","      <td>-15.79585</td>\n","      <td>216.13271</td>\n","      <td>100.04021</td>\n","      <td>32.86051</td>\n","      <td>-26.08461</td>\n","      <td>-186.82429</td>\n","      <td>113.58176</td>\n","      <td>9.28727</td>\n","      <td>44.60282</td>\n","      <td>158.00425</td>\n","      <td>-2.59543</td>\n","      <td>109.19723</td>\n","      <td>23.36143</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","      <td>37.66498</td>\n","      <td>-34.05910</td>\n","      <td>-17.36060</td>\n","      <td>-26.77781</td>\n","      <td>-39.95119</td>\n","      <td>-20.75000</td>\n","      <td>-0.10231</td>\n","      <td>-0.89972</td>\n","      <td>-1.30205</td>\n","      <td>-0.93041</td>\n","      <td>-3.30157</td>\n","      <td>-2.37522</td>\n","      <td>35.42988</td>\n","      <td>1166.91594</td>\n","      <td>1373.01959</td>\n","      <td>409.04286</td>\n","      <td>609.84543</td>\n","      <td>271.33121</td>\n","      <td>392.43089</td>\n","      <td>180.98525</td>\n","      <td>215.37174</td>\n","      <td>137.27208</td>\n","      <td>69.71585</td>\n","      <td>187.80590</td>\n","      <td>-12.67805</td>\n","      <td>-347.02236</td>\n","      <td>271.33219</td>\n","      <td>40.08570</td>\n","      <td>42.13962</td>\n","      <td>10.70729</td>\n","      <td>36.75105</td>\n","      <td>6.28685</td>\n","      <td>3.24624</td>\n","      <td>-3.85517</td>\n","      <td>10.67336</td>\n","      <td>-9.47190</td>\n","      <td>-62.45325</td>\n","      <td>212.55872</td>\n","      <td>-33.45944</td>\n","      <td>...</td>\n","      <td>14.26269</td>\n","      <td>12.13975</td>\n","      <td>17.85889</td>\n","      <td>16.82677</td>\n","      <td>-73.67298</td>\n","      <td>30.33815</td>\n","      <td>-101.70001</td>\n","      <td>-46.13208</td>\n","      <td>14.26704</td>\n","      <td>4.31515</td>\n","      <td>-25.67264</td>\n","      <td>8.70269</td>\n","      <td>11.69122</td>\n","      <td>-59.19425</td>\n","      <td>-43.59348</td>\n","      <td>-81.06458</td>\n","      <td>-8.27943</td>\n","      <td>-16.76813</td>\n","      <td>0.83299</td>\n","      <td>47.83962</td>\n","      <td>-50.41094</td>\n","      <td>-146.83861</td>\n","      <td>37.05135</td>\n","      <td>9.58587</td>\n","      <td>11.61329</td>\n","      <td>-11.13671</td>\n","      <td>86.66011</td>\n","      <td>-21.11909</td>\n","      <td>-4.36777</td>\n","      <td>44.53390</td>\n","      <td>11.18909</td>\n","      <td>45.20614</td>\n","      <td>53.83925</td>\n","      <td>2.59467</td>\n","      <td>-4.00958</td>\n","      <td>-47.74886</td>\n","      <td>-170.92864</td>\n","      <td>-5.19009</td>\n","      <td>8.83617</td>\n","      <td>-7.16056</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>26.51957</td>\n","      <td>-148.15762</td>\n","      <td>-13.30095</td>\n","      <td>-7.25851</td>\n","      <td>17.22029</td>\n","      <td>-21.99439</td>\n","      <td>5.51947</td>\n","      <td>3.48418</td>\n","      <td>2.61738</td>\n","      <td>2.51194</td>\n","      <td>-0.53980</td>\n","      <td>4.94747</td>\n","      <td>55.79019</td>\n","      <td>5492.46406</td>\n","      <td>1704.58482</td>\n","      <td>953.13106</td>\n","      <td>959.38535</td>\n","      <td>788.92858</td>\n","      <td>365.45762</td>\n","      <td>270.37477</td>\n","      <td>524.61443</td>\n","      <td>194.81728</td>\n","      <td>139.44778</td>\n","      <td>375.02904</td>\n","      <td>-434.18365</td>\n","      <td>-627.01684</td>\n","      <td>642.48227</td>\n","      <td>-440.32465</td>\n","      <td>-319.81435</td>\n","      <td>10.13236</td>\n","      <td>38.64956</td>\n","      <td>-30.14092</td>\n","      <td>75.22352</td>\n","      <td>-48.15860</td>\n","      <td>80.88233</td>\n","      <td>183.14023</td>\n","      <td>-178.08073</td>\n","      <td>-594.06429</td>\n","      <td>185.11781</td>\n","      <td>...</td>\n","      <td>137.58951</td>\n","      <td>38.38606</td>\n","      <td>-3.31309</td>\n","      <td>5.48498</td>\n","      <td>-128.84776</td>\n","      <td>-303.86613</td>\n","      <td>188.82297</td>\n","      <td>-108.38722</td>\n","      <td>-69.68996</td>\n","      <td>84.70685</td>\n","      <td>-54.19731</td>\n","      <td>-112.09276</td>\n","      <td>85.47803</td>\n","      <td>38.20818</td>\n","      <td>-277.12066</td>\n","      <td>132.27890</td>\n","      <td>91.74732</td>\n","      <td>-11.84666</td>\n","      <td>-28.23002</td>\n","      <td>-21.30369</td>\n","      <td>205.87882</td>\n","      <td>-22.13147</td>\n","      <td>-52.71243</td>\n","      <td>-122.98466</td>\n","      <td>-0.77068</td>\n","      <td>-56.79478</td>\n","      <td>91.28164</td>\n","      <td>-2.31187</td>\n","      <td>49.34151</td>\n","      <td>-243.95844</td>\n","      <td>23.80442</td>\n","      <td>251.76360</td>\n","      <td>18.81642</td>\n","      <td>157.09656</td>\n","      <td>-27.79449</td>\n","      <td>-137.72740</td>\n","      <td>115.28414</td>\n","      <td>23.00230</td>\n","      <td>-164.02536</td>\n","      <td>51.54138</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>37.68491</td>\n","      <td>-26.84185</td>\n","      <td>-27.10566</td>\n","      <td>-14.95883</td>\n","      <td>-5.87200</td>\n","      <td>-21.68979</td>\n","      <td>4.87374</td>\n","      <td>-18.01800</td>\n","      <td>1.52141</td>\n","      <td>-6.81668</td>\n","      <td>6.80117</td>\n","      <td>21.17530</td>\n","      <td>50.35368</td>\n","      <td>6431.63159</td>\n","      <td>3755.88724</td>\n","      <td>850.19745</td>\n","      <td>2540.53952</td>\n","      <td>400.64901</td>\n","      <td>614.21009</td>\n","      <td>324.55934</td>\n","      <td>395.25844</td>\n","      <td>241.66513</td>\n","      <td>99.66852</td>\n","      <td>489.65963</td>\n","      <td>-466.60181</td>\n","      <td>4015.57689</td>\n","      <td>1042.51072</td>\n","      <td>787.79078</td>\n","      <td>-240.53556</td>\n","      <td>-82.21473</td>\n","      <td>92.62933</td>\n","      <td>25.79866</td>\n","      <td>47.82510</td>\n","      <td>-24.31114</td>\n","      <td>39.59818</td>\n","      <td>-318.96984</td>\n","      <td>1174.17699</td>\n","      <td>2574.00913</td>\n","      <td>-116.86438</td>\n","      <td>...</td>\n","      <td>-10.44655</td>\n","      <td>-86.37776</td>\n","      <td>-2.66989</td>\n","      <td>-150.73054</td>\n","      <td>-309.01020</td>\n","      <td>-263.88605</td>\n","      <td>-511.63129</td>\n","      <td>-331.45866</td>\n","      <td>524.10757</td>\n","      <td>-103.81583</td>\n","      <td>-11.11616</td>\n","      <td>146.64710</td>\n","      <td>47.03819</td>\n","      <td>-555.49233</td>\n","      <td>-312.62275</td>\n","      <td>17.44093</td>\n","      <td>114.07544</td>\n","      <td>30.33500</td>\n","      <td>233.32721</td>\n","      <td>36.70535</td>\n","      <td>-449.22124</td>\n","      <td>507.86020</td>\n","      <td>179.49985</td>\n","      <td>-62.12860</td>\n","      <td>20.81610</td>\n","      <td>37.22555</td>\n","      <td>821.44873</td>\n","      <td>179.59958</td>\n","      <td>-37.21579</td>\n","      <td>-632.61945</td>\n","      <td>-67.57637</td>\n","      <td>234.27192</td>\n","      <td>-72.34557</td>\n","      <td>-362.25101</td>\n","      <td>-25.55019</td>\n","      <td>-89.08971</td>\n","      <td>-891.58937</td>\n","      <td>14.11648</td>\n","      <td>-1030.99180</td>\n","      <td>99.28967</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0</td>\n","      <td>39.11695</td>\n","      <td>-8.29767</td>\n","      <td>-51.37966</td>\n","      <td>-4.42668</td>\n","      <td>-30.06506</td>\n","      <td>-11.95916</td>\n","      <td>-0.85322</td>\n","      <td>-8.86179</td>\n","      <td>11.36680</td>\n","      <td>3.78199</td>\n","      <td>1.54568</td>\n","      <td>0.58662</td>\n","      <td>28.49070</td>\n","      <td>7965.81458</td>\n","      <td>1411.06042</td>\n","      <td>1218.84885</td>\n","      <td>645.93064</td>\n","      <td>1138.49365</td>\n","      <td>552.35786</td>\n","      <td>652.69173</td>\n","      <td>281.26048</td>\n","      <td>396.15845</td>\n","      <td>231.01221</td>\n","      <td>215.80169</td>\n","      <td>345.50690</td>\n","      <td>-1163.24259</td>\n","      <td>-117.88194</td>\n","      <td>162.42114</td>\n","      <td>339.56477</td>\n","      <td>32.54661</td>\n","      <td>-78.76608</td>\n","      <td>9.22956</td>\n","      <td>-7.94960</td>\n","      <td>-15.24124</td>\n","      <td>29.36270</td>\n","      <td>-19.91118</td>\n","      <td>530.29303</td>\n","      <td>284.81573</td>\n","      <td>2.50910</td>\n","      <td>...</td>\n","      <td>50.89571</td>\n","      <td>18.59393</td>\n","      <td>53.83297</td>\n","      <td>-89.69260</td>\n","      <td>-31.45442</td>\n","      <td>188.56576</td>\n","      <td>70.85815</td>\n","      <td>-259.86825</td>\n","      <td>65.62430</td>\n","      <td>-208.96960</td>\n","      <td>-50.55450</td>\n","      <td>1.06368</td>\n","      <td>0.31427</td>\n","      <td>396.93348</td>\n","      <td>105.80580</td>\n","      <td>72.06757</td>\n","      <td>-160.48858</td>\n","      <td>-97.33616</td>\n","      <td>-69.36746</td>\n","      <td>37.07706</td>\n","      <td>-113.27781</td>\n","      <td>-95.78902</td>\n","      <td>155.13377</td>\n","      <td>-22.10761</td>\n","      <td>-1.08949</td>\n","      <td>13.36146</td>\n","      <td>833.00787</td>\n","      <td>-88.78803</td>\n","      <td>30.70543</td>\n","      <td>-63.16836</td>\n","      <td>42.22923</td>\n","      <td>478.26580</td>\n","      <td>-10.33823</td>\n","      <td>-103.76858</td>\n","      <td>39.19511</td>\n","      <td>-98.76636</td>\n","      <td>-122.81061</td>\n","      <td>-2.14942</td>\n","      <td>-211.48202</td>\n","      <td>-12.81569</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>35.05129</td>\n","      <td>-67.97714</td>\n","      <td>-14.20239</td>\n","      <td>-6.68696</td>\n","      <td>-0.61230</td>\n","      <td>-18.70341</td>\n","      <td>-1.31928</td>\n","      <td>-9.46370</td>\n","      <td>5.53492</td>\n","      <td>2.79989</td>\n","      <td>3.34150</td>\n","      <td>18.01445</td>\n","      <td>48.14418</td>\n","      <td>1938.39116</td>\n","      <td>755.44238</td>\n","      <td>1350.57299</td>\n","      <td>793.09536</td>\n","      <td>793.43819</td>\n","      <td>585.72478</td>\n","      <td>280.00734</td>\n","      <td>260.63960</td>\n","      <td>168.98085</td>\n","      <td>148.49783</td>\n","      <td>539.22389</td>\n","      <td>-99.68273</td>\n","      <td>198.55553</td>\n","      <td>332.84941</td>\n","      <td>-139.15915</td>\n","      <td>-166.48695</td>\n","      <td>-186.65004</td>\n","      <td>143.44962</td>\n","      <td>-22.08680</td>\n","      <td>37.34160</td>\n","      <td>-30.72572</td>\n","      <td>64.79269</td>\n","      <td>57.94387</td>\n","      <td>332.16642</td>\n","      <td>77.96230</td>\n","      <td>312.90695</td>\n","      <td>...</td>\n","      <td>52.35627</td>\n","      <td>13.93583</td>\n","      <td>-2.03342</td>\n","      <td>-107.12775</td>\n","      <td>-6.42999</td>\n","      <td>215.24319</td>\n","      <td>122.33200</td>\n","      <td>-183.75334</td>\n","      <td>89.84912</td>\n","      <td>3.34699</td>\n","      <td>-33.62856</td>\n","      <td>38.22842</td>\n","      <td>77.13977</td>\n","      <td>150.36511</td>\n","      <td>-56.21045</td>\n","      <td>34.03026</td>\n","      <td>-34.86760</td>\n","      <td>60.25658</td>\n","      <td>2.52359</td>\n","      <td>-56.10479</td>\n","      <td>-21.25331</td>\n","      <td>77.13640</td>\n","      <td>134.26121</td>\n","      <td>26.21428</td>\n","      <td>-152.23191</td>\n","      <td>-52.85864</td>\n","      <td>-101.18125</td>\n","      <td>55.52082</td>\n","      <td>-64.97701</td>\n","      <td>227.31886</td>\n","      <td>10.25585</td>\n","      <td>94.90539</td>\n","      <td>15.95689</td>\n","      <td>-98.15732</td>\n","      <td>-9.64859</td>\n","      <td>-93.52834</td>\n","      <td>-95.82981</td>\n","      <td>20.73063</td>\n","      <td>-562.07671</td>\n","      <td>43.44696</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1</td>\n","      <td>33.63129</td>\n","      <td>-96.14912</td>\n","      <td>-89.38216</td>\n","      <td>-12.11699</td>\n","      <td>13.77252</td>\n","      <td>-6.69377</td>\n","      <td>-33.36843</td>\n","      <td>-24.81437</td>\n","      <td>21.22757</td>\n","      <td>0.26310</td>\n","      <td>0.42982</td>\n","      <td>-6.59226</td>\n","      <td>27.25596</td>\n","      <td>2407.95516</td>\n","      <td>1257.41135</td>\n","      <td>795.17898</td>\n","      <td>645.83254</td>\n","      <td>1014.07214</td>\n","      <td>474.67866</td>\n","      <td>697.26135</td>\n","      <td>334.85298</td>\n","      <td>358.30744</td>\n","      <td>324.45833</td>\n","      <td>165.58489</td>\n","      <td>-8.08451</td>\n","      <td>101.52431</td>\n","      <td>39.60959</td>\n","      <td>7.06185</td>\n","      <td>-78.21043</td>\n","      <td>88.00470</td>\n","      <td>129.65276</td>\n","      <td>50.90538</td>\n","      <td>-21.26991</td>\n","      <td>32.76996</td>\n","      <td>-29.96947</td>\n","      <td>48.47300</td>\n","      <td>441.40686</td>\n","      <td>454.01182</td>\n","      <td>95.66968</td>\n","      <td>...</td>\n","      <td>55.84917</td>\n","      <td>92.63264</td>\n","      <td>30.35791</td>\n","      <td>-79.20043</td>\n","      <td>-7.46621</td>\n","      <td>602.37381</td>\n","      <td>188.96249</td>\n","      <td>-98.71438</td>\n","      <td>176.37440</td>\n","      <td>-248.00977</td>\n","      <td>44.62138</td>\n","      <td>24.37125</td>\n","      <td>30.40357</td>\n","      <td>79.40089</td>\n","      <td>230.05627</td>\n","      <td>-2.13554</td>\n","      <td>30.01898</td>\n","      <td>-41.89611</td>\n","      <td>-1.77960</td>\n","      <td>46.71421</td>\n","      <td>149.76135</td>\n","      <td>177.92434</td>\n","      <td>-8.49394</td>\n","      <td>47.67341</td>\n","      <td>85.99877</td>\n","      <td>6.63650</td>\n","      <td>-213.70857</td>\n","      <td>90.57614</td>\n","      <td>-15.13552</td>\n","      <td>-99.22940</td>\n","      <td>49.93249</td>\n","      <td>-14.47489</td>\n","      <td>40.70590</td>\n","      <td>58.63692</td>\n","      <td>8.81522</td>\n","      <td>27.28474</td>\n","      <td>5.78046</td>\n","      <td>3.44539</td>\n","      <td>259.10825</td>\n","      <td>10.28525</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0</td>\n","      <td>41.38639</td>\n","      <td>-20.78665</td>\n","      <td>51.80155</td>\n","      <td>17.21415</td>\n","      <td>-36.44189</td>\n","      <td>-11.53169</td>\n","      <td>11.75252</td>\n","      <td>-7.62428</td>\n","      <td>-3.65488</td>\n","      <td>-5.08109</td>\n","      <td>4.37624</td>\n","      <td>-1.39493</td>\n","      <td>18.28400</td>\n","      <td>3157.88796</td>\n","      <td>1116.07117</td>\n","      <td>913.19211</td>\n","      <td>638.12946</td>\n","      <td>535.61444</td>\n","      <td>440.68379</td>\n","      <td>435.75048</td>\n","      <td>545.13836</td>\n","      <td>286.93019</td>\n","      <td>154.24744</td>\n","      <td>278.14609</td>\n","      <td>168.02465</td>\n","      <td>-392.26915</td>\n","      <td>-109.59025</td>\n","      <td>-243.33016</td>\n","      <td>57.76966</td>\n","      <td>7.51054</td>\n","      <td>84.23430</td>\n","      <td>-67.00385</td>\n","      <td>80.72070</td>\n","      <td>-0.00321</td>\n","      <td>2.12590</td>\n","      <td>41.51370</td>\n","      <td>-389.17834</td>\n","      <td>-421.90682</td>\n","      <td>-294.54854</td>\n","      <td>...</td>\n","      <td>127.18184</td>\n","      <td>-106.70486</td>\n","      <td>-18.10968</td>\n","      <td>19.19753</td>\n","      <td>-2.79115</td>\n","      <td>31.93573</td>\n","      <td>10.40156</td>\n","      <td>-226.50662</td>\n","      <td>-2.38144</td>\n","      <td>-157.93284</td>\n","      <td>0.46577</td>\n","      <td>27.78523</td>\n","      <td>6.41002</td>\n","      <td>-173.66782</td>\n","      <td>29.73562</td>\n","      <td>-44.29195</td>\n","      <td>-81.53220</td>\n","      <td>50.49506</td>\n","      <td>65.00208</td>\n","      <td>-17.76363</td>\n","      <td>-37.10280</td>\n","      <td>264.10009</td>\n","      <td>100.74257</td>\n","      <td>-57.74383</td>\n","      <td>-22.25492</td>\n","      <td>-17.44555</td>\n","      <td>335.82434</td>\n","      <td>80.63635</td>\n","      <td>-37.20103</td>\n","      <td>7.30588</td>\n","      <td>50.37614</td>\n","      <td>-40.48205</td>\n","      <td>48.07805</td>\n","      <td>-7.62399</td>\n","      <td>6.51934</td>\n","      <td>-30.46090</td>\n","      <td>-53.87264</td>\n","      <td>4.44627</td>\n","      <td>58.16913</td>\n","      <td>-0.02409</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0</td>\n","      <td>37.45034</td>\n","      <td>11.42615</td>\n","      <td>56.28982</td>\n","      <td>19.58426</td>\n","      <td>-16.43530</td>\n","      <td>2.22457</td>\n","      <td>1.02668</td>\n","      <td>-7.34736</td>\n","      <td>-0.01184</td>\n","      <td>1.24013</td>\n","      <td>2.57660</td>\n","      <td>2.79598</td>\n","      <td>30.02393</td>\n","      <td>1249.33039</td>\n","      <td>3438.98500</td>\n","      <td>962.85885</td>\n","      <td>1654.99981</td>\n","      <td>1066.80390</td>\n","      <td>606.24732</td>\n","      <td>278.63327</td>\n","      <td>273.36684</td>\n","      <td>250.97765</td>\n","      <td>257.04632</td>\n","      <td>294.90282</td>\n","      <td>41.74146</td>\n","      <td>-455.47604</td>\n","      <td>473.68747</td>\n","      <td>-72.16130</td>\n","      <td>409.83317</td>\n","      <td>111.23583</td>\n","      <td>98.74929</td>\n","      <td>19.15393</td>\n","      <td>-38.69791</td>\n","      <td>-20.10176</td>\n","      <td>57.44465</td>\n","      <td>-127.58760</td>\n","      <td>-189.37100</td>\n","      <td>1126.01208</td>\n","      <td>-98.48301</td>\n","      <td>...</td>\n","      <td>55.86703</td>\n","      <td>-15.58939</td>\n","      <td>-40.62299</td>\n","      <td>-45.18282</td>\n","      <td>-85.17021</td>\n","      <td>180.09138</td>\n","      <td>491.26688</td>\n","      <td>6.42139</td>\n","      <td>263.30124</td>\n","      <td>-152.86190</td>\n","      <td>-16.74493</td>\n","      <td>-33.13223</td>\n","      <td>-10.64964</td>\n","      <td>-367.08218</td>\n","      <td>49.57986</td>\n","      <td>17.60102</td>\n","      <td>-112.39761</td>\n","      <td>79.62965</td>\n","      <td>-92.92839</td>\n","      <td>-49.77861</td>\n","      <td>-53.65950</td>\n","      <td>184.29601</td>\n","      <td>-37.85258</td>\n","      <td>-218.50833</td>\n","      <td>-37.55140</td>\n","      <td>-17.84213</td>\n","      <td>-59.35940</td>\n","      <td>-168.74190</td>\n","      <td>-48.89748</td>\n","      <td>-99.69609</td>\n","      <td>-22.46207</td>\n","      <td>-25.77228</td>\n","      <td>-322.42841</td>\n","      <td>-146.57408</td>\n","      <td>13.61588</td>\n","      <td>92.22918</td>\n","      <td>-439.80259</td>\n","      <td>25.73235</td>\n","      <td>157.22967</td>\n","      <td>38.70617</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","      <td>39.71092</td>\n","      <td>-4.92800</td>\n","      <td>12.88590</td>\n","      <td>-11.87773</td>\n","      <td>2.48031</td>\n","      <td>-16.11028</td>\n","      <td>-16.40421</td>\n","      <td>-8.29657</td>\n","      <td>9.86817</td>\n","      <td>-0.17431</td>\n","      <td>0.98765</td>\n","      <td>7.37903</td>\n","      <td>13.84987</td>\n","      <td>1256.34345</td>\n","      <td>800.61127</td>\n","      <td>715.08117</td>\n","      <td>632.19829</td>\n","      <td>357.08486</td>\n","      <td>218.48245</td>\n","      <td>278.90104</td>\n","      <td>229.11644</td>\n","      <td>228.51202</td>\n","      <td>87.35304</td>\n","      <td>144.36923</td>\n","      <td>34.96307</td>\n","      <td>-123.85672</td>\n","      <td>-10.33970</td>\n","      <td>-171.19156</td>\n","      <td>-89.00293</td>\n","      <td>39.72671</td>\n","      <td>-17.78437</td>\n","      <td>4.56124</td>\n","      <td>17.07687</td>\n","      <td>-20.75794</td>\n","      <td>16.49367</td>\n","      <td>63.67926</td>\n","      <td>-113.39747</td>\n","      <td>12.27230</td>\n","      <td>-188.87545</td>\n","      <td>...</td>\n","      <td>-15.69508</td>\n","      <td>1.68706</td>\n","      <td>17.69997</td>\n","      <td>-3.35888</td>\n","      <td>4.21699</td>\n","      <td>25.45729</td>\n","      <td>147.84361</td>\n","      <td>-69.50808</td>\n","      <td>81.47487</td>\n","      <td>-91.89773</td>\n","      <td>9.59723</td>\n","      <td>2.52910</td>\n","      <td>-9.48278</td>\n","      <td>-81.54172</td>\n","      <td>-30.46621</td>\n","      <td>8.10179</td>\n","      <td>9.51480</td>\n","      <td>25.88937</td>\n","      <td>28.29212</td>\n","      <td>5.73829</td>\n","      <td>158.30039</td>\n","      <td>96.20428</td>\n","      <td>55.04618</td>\n","      <td>18.25457</td>\n","      <td>-18.37851</td>\n","      <td>1.06499</td>\n","      <td>-76.40237</td>\n","      <td>111.81246</td>\n","      <td>-57.71450</td>\n","      <td>29.55411</td>\n","      <td>11.92816</td>\n","      <td>-73.72412</td>\n","      <td>16.19039</td>\n","      <td>9.79606</td>\n","      <td>9.71693</td>\n","      <td>-9.90907</td>\n","      <td>-20.65851</td>\n","      <td>2.34002</td>\n","      <td>-31.57015</td>\n","      <td>1.58400</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20 rows × 91 columns</p>\n","</div>"],"text/plain":["    year      var1       var2  ...     var88       var89     var90\n","0      1  49.94357   21.47114  ...  -1.82223   -27.46348   2.26327\n","1      1  48.73215   18.42930  ...  12.04941    58.43453  26.92061\n","2      1  50.95714   31.85602  ...  -0.05859    39.67068  -0.66345\n","3      1  48.24750   -1.89837  ...   9.90558   199.62971  18.85382\n","4      1  50.97020   42.20998  ...   7.88713    55.66926  28.74903\n","5      1  50.54767    0.31568  ...   5.00283   -11.02257   0.02263\n","6      1  50.57546   33.17843  ...   4.50056    -4.62739   1.40192\n","7      1  48.26892    8.97526  ...  -0.30633     3.98364  -3.72556\n","8      1  49.75468   33.99581  ...   5.48708    -9.13495   6.08680\n","9      1  45.17809   46.34234  ...  11.49326   -89.21804 -15.09719\n","10     1  39.13076  -23.01763  ...  -2.59543   109.19723  23.36143\n","11     1  37.66498  -34.05910  ...  -5.19009     8.83617  -7.16056\n","12     1  26.51957 -148.15762  ...  23.00230  -164.02536  51.54138\n","13     1  37.68491  -26.84185  ...  14.11648 -1030.99180  99.28967\n","14     0  39.11695   -8.29767  ...  -2.14942  -211.48202 -12.81569\n","15     1  35.05129  -67.97714  ...  20.73063  -562.07671  43.44696\n","16     1  33.63129  -96.14912  ...   3.44539   259.10825  10.28525\n","17     0  41.38639  -20.78665  ...   4.44627    58.16913  -0.02409\n","18     0  37.45034   11.42615  ...  25.73235   157.22967  38.70617\n","19     0  39.71092   -4.92800  ...   2.34002   -31.57015   1.58400\n","\n","[20 rows x 91 columns]"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"ncjxI4WdzGrA"},"source":["### Part (a) -- 7%\n","\n","The data set description text asks us to respect the below train/test split to\n","avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n","ends up in both the training and test set.\n","\n","Explain why it would be problematic to have\n","some songs from an artist in the training set, and other songs from the same artist in the\n","test set. (Hint: Remember that we want our test accuracy to predict how well the model\n","will perform in practice on a song it hasn't learned about.)"]},{"cell_type":"code","metadata":{"id":"2NiYlxpFzGrB","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"ok","timestamp":1636559846638,"user_tz":-120,"elapsed":8,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"2684728a-6966-40fc-fa69-03abadbe3dca"},"source":["df_train = df[:463715]\n","df_test = df[463715:]\n","\n","# convert to numpy\n","train_xs = df_train[x_labels].to_numpy()\n","train_ts = df_train[t_label].to_numpy()\n","test_xs = df_test[x_labels].to_numpy()\n","test_ts = df_test[t_label].to_numpy()\n","\n","# Write your explanation here\n","'''\n","To evaluate our model properly, we want to know how well the model will perform over a new songs that it hasn't encountered before.\n","Using songs from the same artist for both the training set and the test set can lead to a situation in which the model learns to extract features based on the singer,\n","e.g., the singer's voice, and will decide based on those features.\n","In this scenario, the model might perform well on samples of songs in the test where their corresponding singers also appeared in the train set.\n","But this doesn't guarantee that the model will generalize for songs in the test set that their singers didn't appear in the train set.\n","'''\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nTo evaluate our model properly, we want to know how well the model will perform over a new songs that it hasn't encountered before.\\nUsing songs from the same artist for both the training set and the test set can lead to a situation in which the model learns to extract features based on the singer,\\ne.g., the singer's voice, and will decide based on those features.\\nIn this scenario, the model might perform well on samples of songs in the test where their corresponding singers also appeared in the train set.\\nBut this doesn't guarantee that the model will generalize for songs in the test set that their singers didn't appear in the train set.\\n\""]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"BYSzd4XUzGrB"},"source":["### Part (b) -- 7%\n","\n","It can be beneficial to **normalize** the columns, so that each column (feature)\n","has the *same* mean and standard deviation."]},{"cell_type":"code","metadata":{"id":"TPuWLksJzGrB"},"source":["feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n","feature_stds  = df_train.std()[1:].to_numpy()\n","\n","train_norm_xs = (train_xs - feature_means) / feature_stds\n","test_norm_xs = (test_xs - feature_means) / feature_stds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4zmZk6ezGrC"},"source":["Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n","This is *not* a bug.\n","\n","Explain why it would be improper to compute and use test set means\n","and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"]},{"cell_type":"code","metadata":{"id":"CxZy6brwzGrC","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"ok","timestamp":1636559847020,"user_tz":-120,"elapsed":5,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"a12cd022-c901-475c-fb08-b5ec241ed1ab"},"source":["# Write your explanation here\n","\"\"\"\n","We normalize the data with respect to the training set to make the input data used for training \"more identical distributed\" during training.\n","We are using test data to evaluate our model performance over new unseen data.\n","If we are using the average and standard deviation of this test set to normalize it, we enforce dependency between different samples in the test set. \n","This harms the idea of model evaluation over new unseen data, e.g., if we evaluate our model for two different test sets, both contain some identical samples.\n","Following this approach will lead to different outcomes for the same input sample (because the average and the standard deviation will be different), which demonstrates the problem with this approach.\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nWe normalize the data with respect to the training set to make the input data used for training \"more identical distributed\" during training.\\nWe are using test data to evaluate our model performance over new unseen data.\\nIf we are using the average and standard deviation of this test set to normalize it, we enforce dependency between different samples in the test set. \\nThis harms the idea of model evaluation over new unseen data, e.g., if we evaluate our model for two different test sets, both contain some identical samples.\\nFollowing this approach will lead to different outcomes for the same input sample (because the average and the standard deviation will be different), which demonstrates the problem with this approach.\\n'"]},"metadata":{},"execution_count":75}]},{"cell_type":"markdown","metadata":{"id":"k4GqL5J_zGrC"},"source":["### Part (c) -- 7%\n","\n","Finally, we'll move some of the data in our training set into a validation set.\n","\n","Explain why we should limit how many times we use the test set, and that we should use the validation\n","set during the model building process."]},{"cell_type":"code","metadata":{"id":"HsXv1U3gzGrC","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1636559848394,"user_tz":-120,"elapsed":1378,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"7eb19b39-18ab-4258-8a6c-8d5a95db4b94"},"source":["# shuffle the training set\n","reindex = np.random.permutation(len(train_xs))\n","train_xs = train_xs[reindex]\n","train_norm_xs = train_norm_xs[reindex]\n","train_ts = train_ts[reindex]\n","\n","# use the first 50000 elements of `train_xs` as the validation set\n","train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n","train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n","train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n","\n","# Write your explanation here\n","'''\n","If we will not limit the times that we are using the test set, we may encounter overfitting phenomena over the test data, \n","which in turn will lead to good preformance over the test set, this phenomena may occur becuase we are tuning our model to achieve\n","good preformance over a specific set of test samples. \n","This is a problem because the main reason that we are using a test set is to evaluate our model preformance over an unseen data,\n","i.e., we test how well the training procedure will be able to generalize for unseen samples. \n","We can avoid this issue by examing our model preformance (during the bulding procedure) via\n","another set which we call \"validation\" set. The validation dataset is different from the test dataset, \n","both datasets are held back from the training of the model,but the validation set used to give an unbiased preformance estimation\n","of the final tuned model when comparing between different final models.\"\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nIf we will not limit the times that we are using the test set, we may encounter overfitting phenomena over the test data, \\nwhich in turn will lead to good preformance over the test set, this phenomena may occur becuase we are tuning our model to achieve\\ngood preformance over a specific set of test samples. \\nThis is a problem because the main reason that we are using a test set is to evaluate our model preformance over an unseen data,\\ni.e., we test how well the training procedure will be able to generalize for unseen samples. \\nWe can avoid this issue by examing our model preformance (during the bulding procedure) via\\nanother set which we call \"validation\" set. The validation dataset is different from the test dataset, \\nboth datasets are held back from the training of the model,but the validation set used to give an unbiased preformance estimation\\nof the final tuned model when comparing between different final models.\"\\n'"]},"metadata":{},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"Gy4lt445zGrD"},"source":["## Part 2. Classification (79%)\n","\n","We will first build a *classification* model to perform decade classification.\n","These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."]},{"cell_type":"code","metadata":{"id":"E6BA_s-kzGrD"},"source":["def sigmoid(z):\n","  return 1 / (1 + np.exp(-z))\n","    \n","def cross_entropy(t, y):\n","  epsilon=1e-15 #to avoid log(0)\n","  return -t * np.log(y+epsilon) - (1 - t) * np.log(1 - y+epsilon)\n","\n","def cost(y, t):\n","  return np.mean(cross_entropy(t, y))\n","\n","def get_accuracy(y, t):\n","  acc = 0\n","  N = 0\n","  for i in range(len(y)):\n","    N += 1\n","    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n","      acc += 1\n","  return acc / N"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8ZIfooBzGrD"},"source":["### Part (a) -- 7%\n","\n","Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by: \n","\\begin{equation}\n","y = \\sigma({\\bf w}^T {\\bf x} + b),\n","\\end{equation}\n","where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."]},{"cell_type":"code","metadata":{"id":"naY5mT4_zGrD"},"source":["def pred(w, b, X):\n","  \"\"\"\n","  Returns the prediction `y` of the target based on the weights `w` and scalar \n","  bias `b`.\n","\n","  Preconditions: np.shape(w) == (90,)\n","                 type(b) == float\n","                 np.shape(X) = (N, 90) for some N\n","\n","  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n","  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last\n","  decimals\n","  \"\"\"\n","  return sigmoid(np.dot(X,w)+b)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxNdmSd3zGrE"},"source":["### Part (b) -- 7%\n","\n","Write a function `derivative_cost` that computes and returns the gradients \n","$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n","$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"P80bu7qmzGrE"},"source":["def derivative_cost(X, y, t):\n","  \"\"\"\n","  Returns a tuple containing the gradients dLdw and dLdb.\n","\n","  Precondition: np.shape(X) == (N, 90) for some N\n","                np.shape(y) == (N,)\n","                np.shape(t) == (N,)\n","\n","  Postcondition: np.shape(dLdw) = (90,)\n","           type(dLdb) = float\n","  \"\"\"\n","  N = np.shape(y)[0]\n","  dLdb = np.mean(y - t)\n","  dLdw = 1/N * np.dot(y - t,X)\n","  return (dLdw, dLdb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okPRGM3BjKe2"},"source":["# **Explenation on Gradients**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kHfmPVdsg0eX"},"source":["First we define our loss function as followed:\n","\n","$L(w,b)=-\\frac{1}{N}\\sum_{n=1}^{N}t_n\\log(y_n)+(1-t_n)\\log(1-y_n)$.\n","\n","Let $z_n = \\mathbf{w}^T\\mathbf{x}_n+b$\n","\n","where we denote bold letter as a vector.\n","\n","By the chain-rule:\n","\n","$\\frac{\\partial L}{\\partial w}= \\sum_{n=1}^{N}\\frac{\\partial L}{\\partial y_n}\\cdot \\frac{\\partial y_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial w}$\n","\n","$\\frac{\\partial L}{\\partial b}= \\sum_{n=1}^{N} \\frac{\\partial L}{\\partial y_n}\\cdot \\frac{\\partial y_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial b}$ \n","\n","where:\n","\n","$\\frac{\\partial L}{\\partial y_n}=-\\frac{1}{N}(\\frac{t_n}{y_n}-\\frac{1-t_n}{1-y_n} )=\\frac{y_n-t_n}{N \\cdot y_n \\cdot (1-y_n)}$,\n","\n","$\\frac{\\partial y_n}{\\partial z_n} = \\sigma'(z_n) = \\sigma(z_n)(1-\\sigma(z_n))=y_n(1-y_n)$\n","\n","$\\frac{\\partial z_n}{\\partial b} =1$,\n","\n","$\\frac{\\partial z_n}{\\partial w}=\\mathbf{x}_n$\n","\n","These boils down to:\n","\n","$\\frac{\\partial L}{\\partial b} = \\sum_{n=1}^{N} \\frac{y_n-t_n}{N \\cdot y_n \\cdot (1-y_n)} \\cdot y_n(1-y_n) \\cdot 1 = \\frac{1}{N}\\sum_{n=1}^{N}y_n-t_n$\n","\n","$\\frac{\\partial L}{\\partial w} = \\sum_{n=1}^{N} \\frac{y_n-t_n}{N \\cdot y_n \\cdot (1-y_n)} \\cdot y_n(1-y_n) \\cdot \\mathbf{x}_b = \\frac{1}{N}\\sum_{n=1}^{N}(y_n-t_n)\\mathbf{x}_n$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XhQXAKd4zGrE"},"source":["### Part (c) -- 7%\n","\n","We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n","finite difference rule tells us that for small $h$, we should have\n","\n","$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n","\n","Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n","by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"]},{"cell_type":"code","metadata":{"id":"SpRTD-fozGrF"},"source":["# Your code goes here\n","def compute_analytical_deriviate(w,b,t,X,h=1e-05):\n","  '''\n","  First we calculate the derivative with respect to the bias term\n","  '''\n","  # Calculate the predications\n","  y = pred(w, b, X) \n","  # Calculate the loss of those predications\n","  L = cost(y,t)\n","  y_new = pred(w,b+h,X)\n","  L_new = cost(y_new,t)\n","\n","  # Calculate the analytic derivative\n","  deriviate_b =(L_new-L)/h\n","\n","  '''\n","  Now we calculate the derivative with respect to the weights\n","  '''\n","  L_new = np.zeros(len(w))\n","  deriviate_w = np.zeros(len(w))\n","  for weight_idx in range(len(w)):\n","    # Compute cost of w[weight_idx] + h\n","    w_new = np.copy(w)\n","    w_new[weight_idx] = w_new[weight_idx] + h\n","    # Calculate the predications\n","    y_new = pred(w_new,b,X)\n","    # Calculate the loss of those predications\n","    L_new[weight_idx]=cost(y_new,t)\n","    deriviate_w[weight_idx] = (L_new[weight_idx]-L)/h\n","  \n","  return deriviate_w, deriviate_b\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BU__UXAnkWU9"},"source":["Now we are going to check our derivative using an example"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HiL3lpuzlO03","executionInfo":{"status":"ok","timestamp":1636559848396,"user_tz":-120,"elapsed":35,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"38e10c5c-f004-4320-dd68-d59e37cef231"},"source":["w=np.zeros(90)\n","h=1e-05\n","b=1\n","t=np.array([1,1])\n","X=np.ones([2, 90])\n","y = pred(w, b, X)\n","dw,db = derivative_cost(X,y,t)\n","dw_analytic, db_analytic = compute_analytical_deriviate(w,b,t,X,h)\n","print(\"The bias analytical derivative results is -\", db_analytic)\n","print(\"The bias algorithm derivative results is - \", db)\n","print(f\"The error is {np.abs(db-db_analytic)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The bias analytical derivative results is - -0.26894043830827385\n","The bias algorithm derivative results is -  -0.2689414213699951\n","The error is 9.830617212491788e-07\n"]}]},{"cell_type":"markdown","metadata":{"id":"MTiplTPhzGrF"},"source":["### Part (d) -- 7%\n","\n","Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."]},{"cell_type":"code","metadata":{"id":"oVTsHgnPzGrF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636559848397,"user_tz":-120,"elapsed":35,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"9df700fb-c240-406d-c3ad-49cf96495300"},"source":["print(\"The weights analytical derivative results is -\", dw_analytic)\n","print(\"The weights algorithm derivative results is - \", dw)\n","print(f\"The maximal error is {np.max(np.abs(dw-dw_analytic))}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The weights analytical derivative results is - [-0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044\n"," -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044 -0.26894044]\n","The weights algorithm derivative results is -  [-0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142\n"," -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142 -0.26894142]\n","The maximal error is 9.830617212491788e-07\n"]}]},{"cell_type":"markdown","metadata":{"id":"saUYqefgmc4J"},"source":["Now we will check over couple of real examples from the training set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7Ry_wcZmJnd","executionInfo":{"status":"ok","timestamp":1636559848397,"user_tz":-120,"elapsed":28,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"a65dbd76-52e0-4161-ee35-ddc1275a224d"},"source":["h = 1e-9;\n","# First we initialize the weights by random numbers\n","w=np.random.rand(90)\n","b=np.random.rand(1)\n","\n","# Now we extract the actual samples from the dataset\n","t = (train_ts[:1]).squeeze()\n","X = train_norm_xs[:1]\n","y = pred(w, b, X)\n","\n","dw,db = derivative_cost(X,y,t)\n","dw_analytic, db_analytic = compute_analytical_deriviate(w,b,t,X,h)\n","print(\"The analytical derivative bias results is -\", db_analytic)\n","print(\"The algorithm derivative bias results is - \", db)\n","print(f\"Bias Error is {np.abs(db-db_analytic)}\")\n","\n","print(\"The analytical derivative weights results is -\", dw_analytic)\n","print(\"The algorithm derivative weights results is - \", dw)\n","print(f\"Maximal weights Error is {np.max(np.abs(dw-dw_analytic))}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The analytical derivative bias results is - -0.9672760370449395\n","The algorithm derivative bias results is -  -0.9672759967800327\n","Bias Error is 4.0264906742137896e-08\n","The analytical derivative weights results is - [ 0.97880459  1.33712197  1.5150321   1.17287824 -0.1407332  -0.28086733\n","  0.99645137  1.66069425  0.34143222  2.53612864 -0.60000049  0.2660081\n"," -0.16358959  0.27339908 -0.18315438 -0.02620393  0.57133454  0.29290792\n"," -0.22191848 -0.26613023  0.32624214 -0.12356471  0.2247269  -0.1315108\n","  0.79538109  0.54556981  0.37574432  0.19421131  0.50898086  0.31249137\n"," -0.95565378 -0.54595706  0.15626922  0.11944934 -0.86183682 -0.96469277\n"," -0.30012703 -0.25347191 -0.2640399  -0.20129987  0.12283019  0.02585665\n"," -0.75847728  0.77280982  0.88499963 -0.2634799   0.25427704  0.86388496\n"," -0.74310602 -0.05215162  0.02074074 -0.18614754  0.64476735  0.68632078\n"," -0.79160367 -0.5105103  -0.51600768  0.62609251 -0.36517989  1.05336495\n","  0.91098018  0.16509771  0.39282799  0.36208059 -1.05262732  0.20108226\n","  0.01534106 -0.28936853  0.35263614 -0.85813934  0.51580074 -0.01067191\n"," -0.41914161  0.52804738 -0.2147611  -0.83647089  0.07340262 -1.70637771\n"," -0.25002356  0.29500091  0.20058133 -0.08705836  0.93793728  0.12571144\n"," -1.7977615  -0.31657921  0.64228134  0.73033668 -0.51113247  0.58821525]\n","The algorithm derivative weights results is -  [ 0.97880513  1.33712198  1.51503252  1.17287858 -0.14073285 -0.28086701\n","  0.99645136  1.66069465  0.34143218  2.53612882 -0.60000029  0.26600803\n"," -0.16358953  0.27339904 -0.18315388 -0.02620335  0.57133477  0.29290862\n"," -0.22191785 -0.26613003  0.3262425  -0.12356409  0.22472752 -0.13151021\n","  0.79538094  0.54556964  0.37574461  0.19421122  0.5089815   0.31249165\n"," -0.95565349 -0.54595737  0.15626943  0.11944995 -0.86183685 -0.96469248\n"," -0.30012698 -0.25347199 -0.26404002 -0.20129932  0.12282997  0.02585716\n"," -0.75847651  0.77280993  0.88499994 -0.26347972  0.25427681  0.86388554\n"," -0.74310507 -0.05215134  0.02074135 -0.18614762  0.64476718  0.68632109\n"," -0.79160365 -0.5105102  -0.51600786  0.62609255 -0.36517956  1.05336573\n","  0.91098042  0.16509771  0.39282848  0.3620809  -1.05262721  0.20108315\n","  0.01534155 -0.28936764  0.35263587 -0.8581389   0.51580074 -0.01067161\n"," -0.41914165  0.52804727 -0.21476141 -0.83647112  0.07340269 -1.70637775\n"," -0.25002302  0.29500102  0.20058141 -0.08705784  0.93793721  0.12571158\n"," -1.79776144 -0.31657967  0.64228073  0.73033717 -0.51113169  0.58821569]\n","Maximal weights Error is 9.504835127849276e-07\n"]}]},{"cell_type":"markdown","metadata":{"id":"pgBTPF_2zGrG"},"source":["### Part (e) -- 7%\n","\n","Now that you have a gradient function that works, we can actually run gradient descent. \n","Complete the following code that will run stochastic: gradient descent training:"]},{"cell_type":"code","metadata":{"id":"nW4DEuuPzGrG"},"source":["def run_gradient_descent(w0, b0, mu=0.1, batch_size=100, max_iters=100):\n","  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n","  We use:\n","    - train_norm_xs and train_ts as the training set\n","    - val_norm_xs and val_ts as the test set\n","    - mu as the learning rate\n","    - (w0, b0) as the initial values of (w, b)\n","\n","  Precondition: np.shape(w0) == (90,)\n","                type(b0) == float\n"," \n","  Postcondition: np.shape(w) == (90,)\n","                 type(b) == float\n","  \"\"\"\n","  w = w0\n","  b = b0\n","  iter = 0\n","  val_loss = []\n","  \n","  while iter < max_iters:\n","    # shuffle the training set (there is code above for how to do this)\n","    new_indices = np.random.permutation(len(train_norm_xs))\n","    train_norm_xs_temp = train_norm_xs[new_indices]\n","    train_ts_temp = train_ts[new_indices]\n","\n","    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n","      # minibatch that we are working with:\n","      X = train_norm_xs_temp[i:(i + batch_size)]\n","      t = train_ts_temp[i:(i + batch_size), 0]\n","\n","      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n","      # the \"last\" minibatch\n","      if np.shape(X)[0] != batch_size:\n","        continue\n","\n","      # compute the prediction\n","      y = pred(w, b, X)\n","      # update w and b\n","      dw,db = derivative_cost(X,y,t)\n","      w -= mu*dw\n","      b -= mu*db\n","\n","      # increment the iteration count\n","      iter += 1\n","      # compute and print the *validation* loss and accuracy\n","      if (iter % 10 == 0):\n","        # Calculate the predications over the validation data\n","        y_val = pred(w,b,val_norm_xs)\n","        # Calculate the cost over the predications \n","        val_cost = cost(y_val,val_ts.squeeze())\n","        val_acc = get_accuracy(y_val, val_ts)\n","        print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (iter, val_acc * 100, val_cost))\n","        val_loss.append(val_cost)\n","      if iter >= max_iters:\n","        break\n","\n","      # Think what parameters you should return for further use\n","      # We choose to return in addition to w and b the validation loss in order to evaluate our model training procedure over unseen data.\n","  return w, b, val_loss\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MqzT0jGzGrH"},"source":["### Part (f) -- 7%\n","\n","Call `run_gradient_descent` with the weights and biases all initialized to zero.\n","Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n","Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."]},{"cell_type":"code","metadata":{"id":"tE32Iqo6zGrH","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1636559894208,"user_tz":-120,"elapsed":45828,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"9743d183-ab6c-4ea7-e9da-dc81e683110c"},"source":["w0 = np.zeros(90)\n","b0 = 0\n","Iterations = 1000\n","mus=[0.005,0.1,5]\n","for mu in mus:\n","  print(\"mu=\",mu,\":\")\n","  w,b,losses=run_gradient_descent(w0,b0,mu, max_iters=Iterations)\n","  print(\"-------------------------\")\n","  plt.semilogy(range(0,Iterations,10),losses)\n","\n","plt.legend(mus,loc='best')\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Loss\")\n","plt.grid()\n","plt.title(\"Learning rate convergance properties\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mu= 0.005 :\n","Iter 10. [Val Acc 64%, Loss 0.689537]\n","Iter 20. [Val Acc 64%, Loss 0.686243]\n","Iter 30. [Val Acc 65%, Loss 0.683223]\n","Iter 40. [Val Acc 66%, Loss 0.680522]\n","Iter 50. [Val Acc 66%, Loss 0.677757]\n","Iter 60. [Val Acc 66%, Loss 0.675389]\n","Iter 70. [Val Acc 66%, Loss 0.673272]\n","Iter 80. [Val Acc 66%, Loss 0.670915]\n","Iter 90. [Val Acc 66%, Loss 0.668803]\n","Iter 100. [Val Acc 67%, Loss 0.666691]\n","Iter 110. [Val Acc 67%, Loss 0.664770]\n","Iter 120. [Val Acc 67%, Loss 0.663073]\n","Iter 130. [Val Acc 67%, Loss 0.661280]\n","Iter 140. [Val Acc 67%, Loss 0.659441]\n","Iter 150. [Val Acc 67%, Loss 0.657792]\n","Iter 160. [Val Acc 67%, Loss 0.656151]\n","Iter 170. [Val Acc 67%, Loss 0.654577]\n","Iter 180. [Val Acc 68%, Loss 0.653243]\n","Iter 190. [Val Acc 68%, Loss 0.651831]\n","Iter 200. [Val Acc 68%, Loss 0.650391]\n","Iter 210. [Val Acc 68%, Loss 0.649131]\n","Iter 220. [Val Acc 68%, Loss 0.647609]\n","Iter 230. [Val Acc 68%, Loss 0.646184]\n","Iter 240. [Val Acc 68%, Loss 0.645205]\n","Iter 250. [Val Acc 68%, Loss 0.644035]\n","Iter 260. [Val Acc 68%, Loss 0.642843]\n","Iter 270. [Val Acc 68%, Loss 0.641809]\n","Iter 280. [Val Acc 68%, Loss 0.640775]\n","Iter 290. [Val Acc 68%, Loss 0.639631]\n","Iter 300. [Val Acc 69%, Loss 0.638522]\n","Iter 310. [Val Acc 69%, Loss 0.637720]\n","Iter 320. [Val Acc 69%, Loss 0.636843]\n","Iter 330. [Val Acc 69%, Loss 0.635823]\n","Iter 340. [Val Acc 69%, Loss 0.635013]\n","Iter 350. [Val Acc 69%, Loss 0.634134]\n","Iter 360. [Val Acc 69%, Loss 0.633340]\n","Iter 370. [Val Acc 69%, Loss 0.632425]\n","Iter 380. [Val Acc 69%, Loss 0.631660]\n","Iter 390. [Val Acc 69%, Loss 0.630979]\n","Iter 400. [Val Acc 69%, Loss 0.630316]\n","Iter 410. [Val Acc 69%, Loss 0.629490]\n","Iter 420. [Val Acc 69%, Loss 0.628746]\n","Iter 430. [Val Acc 69%, Loss 0.628168]\n","Iter 440. [Val Acc 69%, Loss 0.627399]\n","Iter 450. [Val Acc 69%, Loss 0.626611]\n","Iter 460. [Val Acc 69%, Loss 0.625946]\n","Iter 470. [Val Acc 69%, Loss 0.625162]\n","Iter 480. [Val Acc 69%, Loss 0.624443]\n","Iter 490. [Val Acc 69%, Loss 0.623948]\n","Iter 500. [Val Acc 69%, Loss 0.623216]\n","Iter 510. [Val Acc 69%, Loss 0.622644]\n","Iter 520. [Val Acc 70%, Loss 0.622084]\n","Iter 530. [Val Acc 70%, Loss 0.621376]\n","Iter 540. [Val Acc 70%, Loss 0.620889]\n","Iter 550. [Val Acc 70%, Loss 0.620391]\n","Iter 560. [Val Acc 70%, Loss 0.619677]\n","Iter 570. [Val Acc 70%, Loss 0.619092]\n","Iter 580. [Val Acc 70%, Loss 0.618571]\n","Iter 590. [Val Acc 70%, Loss 0.617980]\n","Iter 600. [Val Acc 70%, Loss 0.617424]\n","Iter 610. [Val Acc 70%, Loss 0.616836]\n","Iter 620. [Val Acc 70%, Loss 0.616278]\n","Iter 630. [Val Acc 70%, Loss 0.615821]\n","Iter 640. [Val Acc 70%, Loss 0.615303]\n","Iter 650. [Val Acc 70%, Loss 0.614726]\n","Iter 660. [Val Acc 70%, Loss 0.614270]\n","Iter 670. [Val Acc 70%, Loss 0.613770]\n","Iter 680. [Val Acc 70%, Loss 0.613442]\n","Iter 690. [Val Acc 70%, Loss 0.612879]\n","Iter 700. [Val Acc 70%, Loss 0.612325]\n","Iter 710. [Val Acc 70%, Loss 0.612043]\n","Iter 720. [Val Acc 70%, Loss 0.611601]\n","Iter 730. [Val Acc 70%, Loss 0.611160]\n","Iter 740. [Val Acc 70%, Loss 0.610755]\n","Iter 750. [Val Acc 70%, Loss 0.610356]\n","Iter 760. [Val Acc 70%, Loss 0.609983]\n","Iter 770. [Val Acc 70%, Loss 0.609521]\n","Iter 780. [Val Acc 70%, Loss 0.609124]\n","Iter 790. [Val Acc 70%, Loss 0.608796]\n","Iter 800. [Val Acc 70%, Loss 0.608408]\n","Iter 810. [Val Acc 70%, Loss 0.608064]\n","Iter 820. [Val Acc 70%, Loss 0.607669]\n","Iter 830. [Val Acc 70%, Loss 0.607306]\n","Iter 840. [Val Acc 70%, Loss 0.606962]\n","Iter 850. [Val Acc 70%, Loss 0.606637]\n","Iter 860. [Val Acc 70%, Loss 0.606213]\n","Iter 870. [Val Acc 70%, Loss 0.605830]\n","Iter 880. [Val Acc 70%, Loss 0.605357]\n","Iter 890. [Val Acc 70%, Loss 0.605005]\n","Iter 900. [Val Acc 71%, Loss 0.604605]\n","Iter 910. [Val Acc 71%, Loss 0.604260]\n","Iter 920. [Val Acc 71%, Loss 0.604015]\n","Iter 930. [Val Acc 71%, Loss 0.603699]\n","Iter 940. [Val Acc 71%, Loss 0.603412]\n","Iter 950. [Val Acc 71%, Loss 0.603159]\n","Iter 960. [Val Acc 71%, Loss 0.602894]\n","Iter 970. [Val Acc 71%, Loss 0.602661]\n","Iter 980. [Val Acc 71%, Loss 0.602427]\n","Iter 990. [Val Acc 71%, Loss 0.602187]\n","Iter 1000. [Val Acc 71%, Loss 0.601942]\n","-------------------------\n","mu= 0.1 :\n","Iter 10. [Val Acc 71%, Loss 0.601829]\n","Iter 20. [Val Acc 71%, Loss 0.597514]\n","Iter 30. [Val Acc 71%, Loss 0.592957]\n","Iter 40. [Val Acc 71%, Loss 0.590058]\n","Iter 50. [Val Acc 72%, Loss 0.585832]\n","Iter 60. [Val Acc 72%, Loss 0.586104]\n","Iter 70. [Val Acc 72%, Loss 0.581339]\n","Iter 80. [Val Acc 72%, Loss 0.578768]\n","Iter 90. [Val Acc 71%, Loss 0.582677]\n","Iter 100. [Val Acc 72%, Loss 0.576505]\n","Iter 110. [Val Acc 72%, Loss 0.575099]\n","Iter 120. [Val Acc 73%, Loss 0.572667]\n","Iter 130. [Val Acc 73%, Loss 0.572912]\n","Iter 140. [Val Acc 73%, Loss 0.570543]\n","Iter 150. [Val Acc 73%, Loss 0.570110]\n","Iter 160. [Val Acc 73%, Loss 0.569673]\n","Iter 170. [Val Acc 73%, Loss 0.568572]\n","Iter 180. [Val Acc 73%, Loss 0.568890]\n","Iter 190. [Val Acc 73%, Loss 0.569118]\n","Iter 200. [Val Acc 73%, Loss 0.567459]\n","Iter 210. [Val Acc 73%, Loss 0.566941]\n","Iter 220. [Val Acc 73%, Loss 0.567065]\n","Iter 230. [Val Acc 73%, Loss 0.564534]\n","Iter 240. [Val Acc 73%, Loss 0.566419]\n","Iter 250. [Val Acc 73%, Loss 0.565836]\n","Iter 260. [Val Acc 73%, Loss 0.567727]\n","Iter 270. [Val Acc 73%, Loss 0.565815]\n","Iter 280. [Val Acc 73%, Loss 0.565426]\n","Iter 290. [Val Acc 73%, Loss 0.566482]\n","Iter 300. [Val Acc 73%, Loss 0.563875]\n","Iter 310. [Val Acc 73%, Loss 0.565938]\n","Iter 320. [Val Acc 73%, Loss 0.565502]\n","Iter 330. [Val Acc 73%, Loss 0.564350]\n","Iter 340. [Val Acc 73%, Loss 0.564619]\n","Iter 350. [Val Acc 73%, Loss 0.565174]\n","Iter 360. [Val Acc 73%, Loss 0.565855]\n","Iter 370. [Val Acc 73%, Loss 0.566714]\n","Iter 380. [Val Acc 73%, Loss 0.563141]\n","Iter 390. [Val Acc 73%, Loss 0.563240]\n","Iter 400. [Val Acc 73%, Loss 0.563809]\n","Iter 410. [Val Acc 73%, Loss 0.564465]\n","Iter 420. [Val Acc 73%, Loss 0.563139]\n","Iter 430. [Val Acc 73%, Loss 0.561894]\n","Iter 440. [Val Acc 73%, Loss 0.563165]\n","Iter 450. [Val Acc 73%, Loss 0.562376]\n","Iter 460. [Val Acc 73%, Loss 0.561198]\n","Iter 470. [Val Acc 73%, Loss 0.562118]\n","Iter 480. [Val Acc 73%, Loss 0.563457]\n","Iter 490. [Val Acc 73%, Loss 0.563346]\n","Iter 500. [Val Acc 73%, Loss 0.562997]\n","Iter 510. [Val Acc 73%, Loss 0.562334]\n","Iter 520. [Val Acc 73%, Loss 0.563949]\n","Iter 530. [Val Acc 73%, Loss 0.565185]\n","Iter 540. [Val Acc 73%, Loss 0.561996]\n","Iter 550. [Val Acc 73%, Loss 0.562082]\n","Iter 560. [Val Acc 73%, Loss 0.561230]\n","Iter 570. [Val Acc 73%, Loss 0.564939]\n","Iter 580. [Val Acc 73%, Loss 0.565364]\n","Iter 590. [Val Acc 73%, Loss 0.562021]\n","Iter 600. [Val Acc 73%, Loss 0.562121]\n","Iter 610. [Val Acc 73%, Loss 0.563909]\n","Iter 620. [Val Acc 73%, Loss 0.566711]\n","Iter 630. [Val Acc 73%, Loss 0.566379]\n","Iter 640. [Val Acc 73%, Loss 0.562145]\n","Iter 650. [Val Acc 73%, Loss 0.561149]\n","Iter 660. [Val Acc 73%, Loss 0.563261]\n","Iter 670. [Val Acc 73%, Loss 0.562966]\n","Iter 680. [Val Acc 73%, Loss 0.560195]\n","Iter 690. [Val Acc 73%, Loss 0.560050]\n","Iter 700. [Val Acc 73%, Loss 0.560278]\n","Iter 710. [Val Acc 73%, Loss 0.561291]\n","Iter 720. [Val Acc 74%, Loss 0.558807]\n","Iter 730. [Val Acc 73%, Loss 0.559066]\n","Iter 740. [Val Acc 73%, Loss 0.559120]\n","Iter 750. [Val Acc 73%, Loss 0.560010]\n","Iter 760. [Val Acc 73%, Loss 0.561255]\n","Iter 770. [Val Acc 73%, Loss 0.558800]\n","Iter 780. [Val Acc 73%, Loss 0.559677]\n","Iter 790. [Val Acc 73%, Loss 0.559619]\n","Iter 800. [Val Acc 73%, Loss 0.561323]\n","Iter 810. [Val Acc 73%, Loss 0.563050]\n","Iter 820. [Val Acc 73%, Loss 0.561049]\n","Iter 830. [Val Acc 73%, Loss 0.560999]\n","Iter 840. [Val Acc 73%, Loss 0.561243]\n","Iter 850. [Val Acc 73%, Loss 0.560951]\n","Iter 860. [Val Acc 73%, Loss 0.560676]\n","Iter 870. [Val Acc 73%, Loss 0.563061]\n","Iter 880. [Val Acc 73%, Loss 0.562676]\n","Iter 890. [Val Acc 73%, Loss 0.563640]\n","Iter 900. [Val Acc 73%, Loss 0.561361]\n","Iter 910. [Val Acc 73%, Loss 0.559478]\n","Iter 920. [Val Acc 73%, Loss 0.559973]\n","Iter 930. [Val Acc 73%, Loss 0.561349]\n","Iter 940. [Val Acc 73%, Loss 0.559364]\n","Iter 950. [Val Acc 73%, Loss 0.558207]\n","Iter 960. [Val Acc 73%, Loss 0.559179]\n","Iter 970. [Val Acc 74%, Loss 0.559837]\n","Iter 980. [Val Acc 74%, Loss 0.559884]\n","Iter 990. [Val Acc 73%, Loss 0.560026]\n","Iter 1000. [Val Acc 74%, Loss 0.558236]\n","-------------------------\n","mu= 5 :\n","Iter 10. [Val Acc 59%, Loss 3.023958]\n","Iter 20. [Val Acc 57%, Loss 1.914329]\n","Iter 30. [Val Acc 62%, Loss 2.002755]\n","Iter 40. [Val Acc 64%, Loss 2.435203]\n","Iter 50. [Val Acc 63%, Loss 2.626566]\n","Iter 60. [Val Acc 66%, Loss 2.039638]\n","Iter 70. [Val Acc 67%, Loss 1.439940]\n","Iter 80. [Val Acc 68%, Loss 1.891332]\n","Iter 90. [Val Acc 65%, Loss 1.640083]\n","Iter 100. [Val Acc 61%, Loss 2.337751]\n","Iter 110. [Val Acc 59%, Loss 2.873740]\n","Iter 120. [Val Acc 67%, Loss 1.488335]\n","Iter 130. [Val Acc 67%, Loss 1.477086]\n","Iter 140. [Val Acc 63%, Loss 2.021669]\n","Iter 150. [Val Acc 66%, Loss 1.442532]\n","Iter 160. [Val Acc 65%, Loss 1.874325]\n","Iter 170. [Val Acc 61%, Loss 2.141212]\n","Iter 180. [Val Acc 66%, Loss 1.613201]\n","Iter 190. [Val Acc 62%, Loss 2.725409]\n","Iter 200. [Val Acc 62%, Loss 1.916633]\n","Iter 210. [Val Acc 61%, Loss 2.360604]\n","Iter 220. [Val Acc 62%, Loss 1.854672]\n","Iter 230. [Val Acc 66%, Loss 1.481538]\n","Iter 240. [Val Acc 64%, Loss 1.888021]\n","Iter 250. [Val Acc 57%, Loss 4.709104]\n","Iter 260. [Val Acc 64%, Loss 2.027701]\n","Iter 270. [Val Acc 67%, Loss 1.461865]\n","Iter 280. [Val Acc 66%, Loss 1.532466]\n","Iter 290. [Val Acc 67%, Loss 1.459057]\n","Iter 300. [Val Acc 62%, Loss 2.208295]\n","Iter 310. [Val Acc 69%, Loss 1.556971]\n","Iter 320. [Val Acc 68%, Loss 1.326834]\n","Iter 330. [Val Acc 67%, Loss 1.871055]\n","Iter 340. [Val Acc 64%, Loss 1.723846]\n","Iter 350. [Val Acc 66%, Loss 1.364312]\n","Iter 360. [Val Acc 62%, Loss 3.223462]\n","Iter 370. [Val Acc 62%, Loss 2.537601]\n","Iter 380. [Val Acc 57%, Loss 2.789633]\n","Iter 390. [Val Acc 67%, Loss 1.511727]\n","Iter 400. [Val Acc 65%, Loss 2.208063]\n","Iter 410. [Val Acc 56%, Loss 3.532175]\n","Iter 420. [Val Acc 67%, Loss 1.538770]\n","Iter 430. [Val Acc 57%, Loss 3.027161]\n","Iter 440. [Val Acc 62%, Loss 2.283982]\n","Iter 450. [Val Acc 67%, Loss 1.421928]\n","Iter 460. [Val Acc 63%, Loss 2.200051]\n","Iter 470. [Val Acc 64%, Loss 1.882144]\n","Iter 480. [Val Acc 57%, Loss 2.370721]\n","Iter 490. [Val Acc 57%, Loss 2.502125]\n","Iter 500. [Val Acc 67%, Loss 1.588496]\n","Iter 510. [Val Acc 61%, Loss 2.959032]\n","Iter 520. [Val Acc 68%, Loss 1.879302]\n","Iter 530. [Val Acc 68%, Loss 1.164676]\n","Iter 540. [Val Acc 64%, Loss 1.824828]\n","Iter 550. [Val Acc 57%, Loss 3.520049]\n","Iter 560. [Val Acc 69%, Loss 1.475708]\n","Iter 570. [Val Acc 68%, Loss 1.303260]\n","Iter 580. [Val Acc 68%, Loss 1.693122]\n","Iter 590. [Val Acc 63%, Loss 2.487929]\n","Iter 600. [Val Acc 63%, Loss 2.252570]\n","Iter 610. [Val Acc 62%, Loss 1.945547]\n","Iter 620. [Val Acc 60%, Loss 2.248732]\n","Iter 630. [Val Acc 63%, Loss 2.068087]\n","Iter 640. [Val Acc 66%, Loss 1.637215]\n","Iter 650. [Val Acc 68%, Loss 1.478781]\n","Iter 660. [Val Acc 65%, Loss 1.856137]\n","Iter 670. [Val Acc 66%, Loss 1.817354]\n","Iter 680. [Val Acc 62%, Loss 2.266390]\n","Iter 690. [Val Acc 59%, Loss 2.458833]\n","Iter 700. [Val Acc 67%, Loss 1.485949]\n","Iter 710. [Val Acc 65%, Loss 1.720554]\n","Iter 720. [Val Acc 64%, Loss 2.393605]\n","Iter 730. [Val Acc 63%, Loss 2.371198]\n","Iter 740. [Val Acc 62%, Loss 2.561722]\n","Iter 750. [Val Acc 68%, Loss 1.306933]\n","Iter 760. [Val Acc 69%, Loss 1.331141]\n","Iter 770. [Val Acc 64%, Loss 2.654016]\n","Iter 780. [Val Acc 67%, Loss 1.854800]\n","Iter 790. [Val Acc 68%, Loss 1.690965]\n","Iter 800. [Val Acc 65%, Loss 1.717231]\n","Iter 810. [Val Acc 64%, Loss 1.676793]\n","Iter 820. [Val Acc 62%, Loss 2.508890]\n","Iter 830. [Val Acc 63%, Loss 1.871474]\n","Iter 840. [Val Acc 62%, Loss 1.912280]\n","Iter 850. [Val Acc 64%, Loss 1.950877]\n","Iter 860. [Val Acc 63%, Loss 2.517294]\n","Iter 870. [Val Acc 70%, Loss 1.272444]\n","Iter 880. [Val Acc 61%, Loss 2.958198]\n","Iter 890. [Val Acc 61%, Loss 2.191611]\n","Iter 900. [Val Acc 62%, Loss 1.816850]\n","Iter 910. [Val Acc 68%, Loss 1.400787]\n","Iter 920. [Val Acc 68%, Loss 1.559446]\n","Iter 930. [Val Acc 65%, Loss 2.116104]\n","Iter 940. [Val Acc 65%, Loss 1.849257]\n","Iter 950. [Val Acc 67%, Loss 1.597749]\n","Iter 960. [Val Acc 63%, Loss 1.887086]\n","Iter 970. [Val Acc 65%, Loss 2.198277]\n","Iter 980. [Val Acc 70%, Loss 1.238432]\n","Iter 990. [Val Acc 68%, Loss 1.616702]\n","Iter 1000. [Val Acc 59%, Loss 2.957832]\n","-------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Learning rate convergance properties')"]},"metadata":{},"execution_count":85},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"S44GmUZ12o-g"},"source":["We conclude from the figure above that for a small learning rate, the algorithm may be capable to converge but very slowly due to the small changes in the parameters space. On the other hand, we see that by using too large learning rate, the algorithm is incapable to converge due to the large changes in the parameters space. Thus, we conclude from the empirical results that choosing a learning rate somewhere in between those learning rates leads to the best performance. \n","We explain this phenomenon by the fact that the algorithm can change its parameters efficiently towards a good optimum. Meaning that we have found a good trade-off between a small enough step size that allows convergence but not too small such that it leads to faster convergence rate to a good enough optimum.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EZ5HlDFAzGrH"},"source":["### Part (g) -- 7%\n","\n","Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n","the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."]},{"cell_type":"code","metadata":{"id":"1dFOFSwgzGrI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636560138204,"user_tz":-120,"elapsed":244001,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"8616255b-30ff-459c-ae4e-2fcd6503557e"},"source":["'''\n","Hyperparameters search\n","'''\n","\n","mus = [0.01, 0.1, 0.5, 1]\n","batch_sizes = [1, 64, 1024, 50000]\n","Iterations = 1000\n","loss_results = np.zeros((len(mus),len(batch_sizes)))\n","w_history = np.zeros((len(mus),len(batch_sizes), 90))\n","b_history = np.zeros((len(mus),len(batch_sizes), 1))\n","\n","w0 = np.random.randn(90)\n","b0 = np.random.randn(1)[0]\n","\n","for mu_idx,mu in enumerate(mus):\n","  for minibatch_idx, minibatch_size in enumerate(batch_sizes):\n","    print(\"mu=\",mu, 'batch_size=', minibatch_size, ':')\n","    w,b,losses=run_gradient_descent(w0,b0,mu, max_iters=Iterations)\n","    print(\"-------------------------\")\n","    loss_results[mu_idx][minibatch_idx] = losses[-1]\n","    w_history[mu_idx][minibatch_idx] = w\n","    b_history[mu_idx][minibatch_idx] = b\n","\n","best_mu_idx, best_batch_size_idx = np.unravel_index(np.argmin(loss_results, axis=None), loss_results.shape)\n","print(\"optimal mu=\",mus[best_mu_idx], 'optimal batch_size=', batch_sizes[best_batch_size_idx])\n","'''\n","Save the optimal weight and bias\n","'''\n","opt_w = w_history[best_mu_idx][best_batch_size_idx]\n","opt_b = b_history[best_mu_idx][best_batch_size_idx]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mu= 0.01 batch_size= 1 :\n","Iter 10. [Val Acc 47%, Loss 2.747708]\n","Iter 20. [Val Acc 47%, Loss 2.714000]\n","Iter 30. [Val Acc 47%, Loss 2.686253]\n","Iter 40. [Val Acc 47%, Loss 2.659955]\n","Iter 50. [Val Acc 47%, Loss 2.633081]\n","Iter 60. [Val Acc 47%, Loss 2.603889]\n","Iter 70. [Val Acc 48%, Loss 2.580692]\n","Iter 80. [Val Acc 48%, Loss 2.557785]\n","Iter 90. [Val Acc 48%, Loss 2.530571]\n","Iter 100. [Val Acc 48%, Loss 2.503728]\n","Iter 110. [Val Acc 48%, Loss 2.479285]\n","Iter 120. [Val Acc 48%, Loss 2.455511]\n","Iter 130. [Val Acc 48%, Loss 2.436268]\n","Iter 140. [Val Acc 48%, Loss 2.415122]\n","Iter 150. [Val Acc 48%, Loss 2.394333]\n","Iter 160. [Val Acc 48%, Loss 2.377478]\n","Iter 170. [Val Acc 48%, Loss 2.357796]\n","Iter 180. [Val Acc 48%, Loss 2.338882]\n","Iter 190. [Val Acc 48%, Loss 2.319985]\n","Iter 200. [Val Acc 48%, Loss 2.298555]\n","Iter 210. [Val Acc 48%, Loss 2.279912]\n","Iter 220. [Val Acc 49%, Loss 2.262655]\n","Iter 230. [Val Acc 49%, Loss 2.244858]\n","Iter 240. [Val Acc 49%, Loss 2.228405]\n","Iter 250. [Val Acc 49%, Loss 2.213221]\n","Iter 260. [Val Acc 49%, Loss 2.195344]\n","Iter 270. [Val Acc 49%, Loss 2.180319]\n","Iter 280. [Val Acc 49%, Loss 2.164565]\n","Iter 290. [Val Acc 49%, Loss 2.145138]\n","Iter 300. [Val Acc 49%, Loss 2.128512]\n","Iter 310. [Val Acc 49%, Loss 2.111651]\n","Iter 320. [Val Acc 49%, Loss 2.095608]\n","Iter 330. [Val Acc 49%, Loss 2.079349]\n","Iter 340. [Val Acc 50%, Loss 2.063636]\n","Iter 350. [Val Acc 50%, Loss 2.049249]\n","Iter 360. [Val Acc 50%, Loss 2.034857]\n","Iter 370. [Val Acc 50%, Loss 2.018879]\n","Iter 380. [Val Acc 50%, Loss 2.003994]\n","Iter 390. [Val Acc 50%, Loss 1.988755]\n","Iter 400. [Val Acc 50%, Loss 1.975895]\n","Iter 410. [Val Acc 50%, Loss 1.960965]\n","Iter 420. [Val Acc 50%, Loss 1.946616]\n","Iter 430. [Val Acc 50%, Loss 1.931607]\n","Iter 440. [Val Acc 50%, Loss 1.917479]\n","Iter 450. [Val Acc 50%, Loss 1.902136]\n","Iter 460. [Val Acc 50%, Loss 1.888529]\n","Iter 470. [Val Acc 51%, Loss 1.877311]\n","Iter 480. [Val Acc 51%, Loss 1.865819]\n","Iter 490. [Val Acc 51%, Loss 1.851953]\n","Iter 500. [Val Acc 51%, Loss 1.839929]\n","Iter 510. [Val Acc 51%, Loss 1.826131]\n","Iter 520. [Val Acc 51%, Loss 1.814031]\n","Iter 530. [Val Acc 51%, Loss 1.801829]\n","Iter 540. [Val Acc 51%, Loss 1.788531]\n","Iter 550. [Val Acc 51%, Loss 1.775909]\n","Iter 560. [Val Acc 51%, Loss 1.763521]\n","Iter 570. [Val Acc 51%, Loss 1.751194]\n","Iter 580. [Val Acc 51%, Loss 1.739099]\n","Iter 590. [Val Acc 52%, Loss 1.727596]\n","Iter 600. [Val Acc 52%, Loss 1.716846]\n","Iter 610. [Val Acc 52%, Loss 1.705644]\n","Iter 620. [Val Acc 52%, Loss 1.694009]\n","Iter 630. [Val Acc 52%, Loss 1.683048]\n","Iter 640. [Val Acc 52%, Loss 1.672278]\n","Iter 650. [Val Acc 52%, Loss 1.660642]\n","Iter 660. [Val Acc 52%, Loss 1.649790]\n","Iter 670. [Val Acc 52%, Loss 1.639040]\n","Iter 680. [Val Acc 53%, Loss 1.626574]\n","Iter 690. [Val Acc 53%, Loss 1.616777]\n","Iter 700. [Val Acc 53%, Loss 1.605414]\n","Iter 710. [Val Acc 53%, Loss 1.594303]\n","Iter 720. [Val Acc 53%, Loss 1.585565]\n","Iter 730. [Val Acc 53%, Loss 1.576315]\n","Iter 740. [Val Acc 53%, Loss 1.566632]\n","Iter 750. [Val Acc 53%, Loss 1.557272]\n","Iter 760. [Val Acc 53%, Loss 1.547641]\n","Iter 770. [Val Acc 53%, Loss 1.539211]\n","Iter 780. [Val Acc 53%, Loss 1.530813]\n","Iter 790. [Val Acc 54%, Loss 1.521222]\n","Iter 800. [Val Acc 54%, Loss 1.512906]\n","Iter 810. [Val Acc 54%, Loss 1.503275]\n","Iter 820. [Val Acc 54%, Loss 1.494597]\n","Iter 830. [Val Acc 54%, Loss 1.485597]\n","Iter 840. [Val Acc 54%, Loss 1.476089]\n","Iter 850. [Val Acc 54%, Loss 1.468392]\n","Iter 860. [Val Acc 54%, Loss 1.460788]\n","Iter 870. [Val Acc 54%, Loss 1.452696]\n","Iter 880. [Val Acc 54%, Loss 1.443291]\n","Iter 890. [Val Acc 54%, Loss 1.436108]\n","Iter 900. [Val Acc 54%, Loss 1.426818]\n","Iter 910. [Val Acc 54%, Loss 1.418355]\n","Iter 920. [Val Acc 55%, Loss 1.410911]\n","Iter 930. [Val Acc 55%, Loss 1.402065]\n","Iter 940. [Val Acc 55%, Loss 1.394395]\n","Iter 950. [Val Acc 55%, Loss 1.386117]\n","Iter 960. [Val Acc 55%, Loss 1.377559]\n","Iter 970. [Val Acc 55%, Loss 1.370999]\n","Iter 980. [Val Acc 55%, Loss 1.363807]\n","Iter 990. [Val Acc 55%, Loss 1.356797]\n","Iter 1000. [Val Acc 55%, Loss 1.349479]\n","-------------------------\n","mu= 0.01 batch_size= 64 :\n","Iter 10. [Val Acc 55%, Loss 1.345617]\n","Iter 20. [Val Acc 56%, Loss 1.338529]\n","Iter 30. [Val Acc 56%, Loss 1.331585]\n","Iter 40. [Val Acc 56%, Loss 1.325503]\n","Iter 50. [Val Acc 56%, Loss 1.318606]\n","Iter 60. [Val Acc 56%, Loss 1.310718]\n","Iter 70. [Val Acc 56%, Loss 1.304447]\n","Iter 80. [Val Acc 56%, Loss 1.297898]\n","Iter 90. [Val Acc 56%, Loss 1.290197]\n","Iter 100. [Val Acc 56%, Loss 1.283876]\n","Iter 110. [Val Acc 56%, Loss 1.277738]\n","Iter 120. [Val Acc 56%, Loss 1.271026]\n","Iter 130. [Val Acc 56%, Loss 1.263781]\n","Iter 140. [Val Acc 56%, Loss 1.257526]\n","Iter 150. [Val Acc 57%, Loss 1.251464]\n","Iter 160. [Val Acc 57%, Loss 1.245631]\n","Iter 170. [Val Acc 57%, Loss 1.239308]\n","Iter 180. [Val Acc 57%, Loss 1.232742]\n","Iter 190. [Val Acc 57%, Loss 1.227313]\n","Iter 200. [Val Acc 57%, Loss 1.221064]\n","Iter 210. [Val Acc 57%, Loss 1.214861]\n","Iter 220. [Val Acc 57%, Loss 1.208799]\n","Iter 230. [Val Acc 57%, Loss 1.202713]\n","Iter 240. [Val Acc 57%, Loss 1.197273]\n","Iter 250. [Val Acc 57%, Loss 1.191149]\n","Iter 260. [Val Acc 57%, Loss 1.185029]\n","Iter 270. [Val Acc 57%, Loss 1.179632]\n","Iter 280. [Val Acc 57%, Loss 1.174148]\n","Iter 290. [Val Acc 57%, Loss 1.168859]\n","Iter 300. [Val Acc 57%, Loss 1.163904]\n","Iter 310. [Val Acc 58%, Loss 1.158641]\n","Iter 320. [Val Acc 58%, Loss 1.153226]\n","Iter 330. [Val Acc 58%, Loss 1.148096]\n","Iter 340. [Val Acc 58%, Loss 1.142611]\n","Iter 350. [Val Acc 58%, Loss 1.137139]\n","Iter 360. [Val Acc 58%, Loss 1.132120]\n","Iter 370. [Val Acc 58%, Loss 1.126368]\n","Iter 380. [Val Acc 58%, Loss 1.121926]\n","Iter 390. [Val Acc 58%, Loss 1.116211]\n","Iter 400. [Val Acc 58%, Loss 1.111316]\n","Iter 410. [Val Acc 58%, Loss 1.107111]\n","Iter 420. [Val Acc 58%, Loss 1.102484]\n","Iter 430. [Val Acc 58%, Loss 1.098378]\n","Iter 440. [Val Acc 58%, Loss 1.094737]\n","Iter 450. [Val Acc 58%, Loss 1.090213]\n","Iter 460. [Val Acc 59%, Loss 1.085753]\n","Iter 470. [Val Acc 59%, Loss 1.081230]\n","Iter 480. [Val Acc 59%, Loss 1.076412]\n","Iter 490. [Val Acc 59%, Loss 1.072144]\n","Iter 500. [Val Acc 59%, Loss 1.067320]\n","Iter 510. [Val Acc 59%, Loss 1.061990]\n","Iter 520. [Val Acc 59%, Loss 1.057249]\n","Iter 530. [Val Acc 59%, Loss 1.053846]\n","Iter 540. [Val Acc 59%, Loss 1.049935]\n","Iter 550. [Val Acc 59%, Loss 1.046121]\n","Iter 560. [Val Acc 59%, Loss 1.041967]\n","Iter 570. [Val Acc 59%, Loss 1.038177]\n","Iter 580. [Val Acc 59%, Loss 1.033472]\n","Iter 590. [Val Acc 59%, Loss 1.029379]\n","Iter 600. [Val Acc 59%, Loss 1.024871]\n","Iter 610. [Val Acc 59%, Loss 1.020544]\n","Iter 620. [Val Acc 59%, Loss 1.017094]\n","Iter 630. [Val Acc 60%, Loss 1.013561]\n","Iter 640. [Val Acc 60%, Loss 1.009934]\n","Iter 650. [Val Acc 60%, Loss 1.005945]\n","Iter 660. [Val Acc 60%, Loss 1.002079]\n","Iter 670. [Val Acc 60%, Loss 0.998660]\n","Iter 680. [Val Acc 60%, Loss 0.994886]\n","Iter 690. [Val Acc 60%, Loss 0.991315]\n","Iter 700. [Val Acc 60%, Loss 0.987764]\n","Iter 710. [Val Acc 60%, Loss 0.984638]\n","Iter 720. [Val Acc 60%, Loss 0.981326]\n","Iter 730. [Val Acc 60%, Loss 0.977929]\n","Iter 740. [Val Acc 60%, Loss 0.974272]\n","Iter 750. [Val Acc 60%, Loss 0.971192]\n","Iter 760. [Val Acc 60%, Loss 0.968324]\n","Iter 770. [Val Acc 60%, Loss 0.965537]\n","Iter 780. [Val Acc 60%, Loss 0.962614]\n","Iter 790. [Val Acc 61%, Loss 0.959136]\n","Iter 800. [Val Acc 61%, Loss 0.956208]\n","Iter 810. [Val Acc 61%, Loss 0.953530]\n","Iter 820. [Val Acc 61%, Loss 0.950446]\n","Iter 830. [Val Acc 61%, Loss 0.946679]\n","Iter 840. [Val Acc 61%, Loss 0.943529]\n","Iter 850. [Val Acc 61%, Loss 0.940725]\n","Iter 860. [Val Acc 61%, Loss 0.937726]\n","Iter 870. [Val Acc 61%, Loss 0.934797]\n","Iter 880. [Val Acc 61%, Loss 0.931442]\n","Iter 890. [Val Acc 61%, Loss 0.928389]\n","Iter 900. [Val Acc 61%, Loss 0.925764]\n","Iter 910. [Val Acc 61%, Loss 0.922628]\n","Iter 920. [Val Acc 61%, Loss 0.919816]\n","Iter 930. [Val Acc 61%, Loss 0.917270]\n","Iter 940. [Val Acc 62%, Loss 0.914550]\n","Iter 950. [Val Acc 62%, Loss 0.911570]\n","Iter 960. [Val Acc 62%, Loss 0.909385]\n","Iter 970. [Val Acc 62%, Loss 0.906585]\n","Iter 980. [Val Acc 62%, Loss 0.904106]\n","Iter 990. [Val Acc 62%, Loss 0.901269]\n","Iter 1000. [Val Acc 62%, Loss 0.898525]\n","-------------------------\n","mu= 0.01 batch_size= 1024 :\n","Iter 10. [Val Acc 62%, Loss 0.906328]\n","Iter 20. [Val Acc 62%, Loss 0.903532]\n","Iter 30. [Val Acc 62%, Loss 0.901044]\n","Iter 40. [Val Acc 62%, Loss 0.898690]\n","Iter 50. [Val Acc 62%, Loss 0.896345]\n","Iter 60. [Val Acc 62%, Loss 0.893715]\n","Iter 70. [Val Acc 62%, Loss 0.891497]\n","Iter 80. [Val Acc 62%, Loss 0.888453]\n","Iter 90. [Val Acc 62%, Loss 0.886079]\n","Iter 100. [Val Acc 62%, Loss 0.883580]\n","Iter 110. [Val Acc 62%, Loss 0.880946]\n","Iter 120. [Val Acc 62%, Loss 0.878768]\n","Iter 130. [Val Acc 62%, Loss 0.876273]\n","Iter 140. [Val Acc 62%, Loss 0.874109]\n","Iter 150. [Val Acc 63%, Loss 0.871913]\n","Iter 160. [Val Acc 63%, Loss 0.869762]\n","Iter 170. [Val Acc 63%, Loss 0.867458]\n","Iter 180. [Val Acc 63%, Loss 0.865235]\n","Iter 190. [Val Acc 63%, Loss 0.863043]\n","Iter 200. [Val Acc 63%, Loss 0.860969]\n","Iter 210. [Val Acc 63%, Loss 0.859043]\n","Iter 220. [Val Acc 63%, Loss 0.856785]\n","Iter 230. [Val Acc 63%, Loss 0.854760]\n","Iter 240. [Val Acc 63%, Loss 0.852633]\n","Iter 250. [Val Acc 63%, Loss 0.850425]\n","Iter 260. [Val Acc 63%, Loss 0.848362]\n","Iter 270. [Val Acc 63%, Loss 0.846445]\n","Iter 280. [Val Acc 63%, Loss 0.844571]\n","Iter 290. [Val Acc 63%, Loss 0.842505]\n","Iter 300. [Val Acc 63%, Loss 0.840632]\n","Iter 310. [Val Acc 63%, Loss 0.838513]\n","Iter 320. [Val Acc 63%, Loss 0.836432]\n","Iter 330. [Val Acc 63%, Loss 0.834393]\n","Iter 340. [Val Acc 63%, Loss 0.832571]\n","Iter 350. [Val Acc 64%, Loss 0.830582]\n","Iter 360. [Val Acc 64%, Loss 0.828517]\n","Iter 370. [Val Acc 64%, Loss 0.826750]\n","Iter 380. [Val Acc 64%, Loss 0.824939]\n","Iter 390. [Val Acc 64%, Loss 0.823122]\n","Iter 400. [Val Acc 64%, Loss 0.821391]\n","Iter 410. [Val Acc 64%, Loss 0.819657]\n","Iter 420. [Val Acc 64%, Loss 0.818024]\n","Iter 430. [Val Acc 64%, Loss 0.815976]\n","Iter 440. [Val Acc 64%, Loss 0.814484]\n","Iter 450. [Val Acc 64%, Loss 0.813169]\n","Iter 460. [Val Acc 64%, Loss 0.811369]\n","Iter 470. [Val Acc 64%, Loss 0.809472]\n","Iter 480. [Val Acc 64%, Loss 0.807631]\n","Iter 490. [Val Acc 64%, Loss 0.805938]\n","Iter 500. [Val Acc 64%, Loss 0.804531]\n","Iter 510. [Val Acc 64%, Loss 0.803300]\n","Iter 520. [Val Acc 64%, Loss 0.801580]\n","Iter 530. [Val Acc 64%, Loss 0.800029]\n","Iter 540. [Val Acc 64%, Loss 0.798580]\n","Iter 550. [Val Acc 64%, Loss 0.797229]\n","Iter 560. [Val Acc 64%, Loss 0.795180]\n","Iter 570. [Val Acc 64%, Loss 0.793616]\n","Iter 580. [Val Acc 64%, Loss 0.792142]\n","Iter 590. [Val Acc 64%, Loss 0.790722]\n","Iter 600. [Val Acc 64%, Loss 0.789357]\n","Iter 610. [Val Acc 64%, Loss 0.787887]\n","Iter 620. [Val Acc 64%, Loss 0.786438]\n","Iter 630. [Val Acc 65%, Loss 0.784737]\n","Iter 640. [Val Acc 65%, Loss 0.783578]\n","Iter 650. [Val Acc 65%, Loss 0.782101]\n","Iter 660. [Val Acc 65%, Loss 0.780594]\n","Iter 670. [Val Acc 65%, Loss 0.779144]\n","Iter 680. [Val Acc 65%, Loss 0.777746]\n","Iter 690. [Val Acc 65%, Loss 0.776177]\n","Iter 700. [Val Acc 65%, Loss 0.774936]\n","Iter 710. [Val Acc 65%, Loss 0.773153]\n","Iter 720. [Val Acc 65%, Loss 0.771496]\n","Iter 730. [Val Acc 65%, Loss 0.770301]\n","Iter 740. [Val Acc 65%, Loss 0.768918]\n","Iter 750. [Val Acc 65%, Loss 0.767392]\n","Iter 760. [Val Acc 65%, Loss 0.766347]\n","Iter 770. [Val Acc 65%, Loss 0.765385]\n","Iter 780. [Val Acc 65%, Loss 0.764265]\n","Iter 790. [Val Acc 65%, Loss 0.762733]\n","Iter 800. [Val Acc 65%, Loss 0.761390]\n","Iter 810. [Val Acc 65%, Loss 0.760108]\n","Iter 820. [Val Acc 65%, Loss 0.758848]\n","Iter 830. [Val Acc 65%, Loss 0.757540]\n","Iter 840. [Val Acc 65%, Loss 0.756335]\n","Iter 850. [Val Acc 65%, Loss 0.754976]\n","Iter 860. [Val Acc 65%, Loss 0.754003]\n","Iter 870. [Val Acc 65%, Loss 0.752844]\n","Iter 880. [Val Acc 65%, Loss 0.751843]\n","Iter 890. [Val Acc 65%, Loss 0.750819]\n","Iter 900. [Val Acc 65%, Loss 0.749556]\n","Iter 910. [Val Acc 65%, Loss 0.748219]\n","Iter 920. [Val Acc 65%, Loss 0.747076]\n","Iter 930. [Val Acc 66%, Loss 0.745959]\n","Iter 940. [Val Acc 66%, Loss 0.744811]\n","Iter 950. [Val Acc 66%, Loss 0.743666]\n","Iter 960. [Val Acc 66%, Loss 0.742686]\n","Iter 970. [Val Acc 66%, Loss 0.741863]\n","Iter 980. [Val Acc 66%, Loss 0.740832]\n","Iter 990. [Val Acc 66%, Loss 0.739786]\n","Iter 1000. [Val Acc 66%, Loss 0.738588]\n","-------------------------\n","mu= 0.01 batch_size= 50000 :\n","Iter 10. [Val Acc 65%, Loss 0.755230]\n","Iter 20. [Val Acc 65%, Loss 0.753799]\n","Iter 30. [Val Acc 65%, Loss 0.752042]\n","Iter 40. [Val Acc 66%, Loss 0.750682]\n","Iter 50. [Val Acc 66%, Loss 0.748914]\n","Iter 60. [Val Acc 66%, Loss 0.747344]\n","Iter 70. [Val Acc 66%, Loss 0.745985]\n","Iter 80. [Val Acc 66%, Loss 0.744480]\n","Iter 90. [Val Acc 66%, Loss 0.743040]\n","Iter 100. [Val Acc 66%, Loss 0.741452]\n","Iter 110. [Val Acc 66%, Loss 0.740027]\n","Iter 120. [Val Acc 66%, Loss 0.738746]\n","Iter 130. [Val Acc 66%, Loss 0.737431]\n","Iter 140. [Val Acc 66%, Loss 0.736303]\n","Iter 150. [Val Acc 66%, Loss 0.734928]\n","Iter 160. [Val Acc 66%, Loss 0.733650]\n","Iter 170. [Val Acc 66%, Loss 0.732754]\n","Iter 180. [Val Acc 66%, Loss 0.731638]\n","Iter 190. [Val Acc 66%, Loss 0.730632]\n","Iter 200. [Val Acc 66%, Loss 0.729331]\n","Iter 210. [Val Acc 66%, Loss 0.728069]\n","Iter 220. [Val Acc 66%, Loss 0.726973]\n","Iter 230. [Val Acc 66%, Loss 0.725977]\n","Iter 240. [Val Acc 66%, Loss 0.724940]\n","Iter 250. [Val Acc 67%, Loss 0.723628]\n","Iter 260. [Val Acc 67%, Loss 0.722392]\n","Iter 270. [Val Acc 67%, Loss 0.721598]\n","Iter 280. [Val Acc 67%, Loss 0.720821]\n","Iter 290. [Val Acc 67%, Loss 0.719714]\n","Iter 300. [Val Acc 67%, Loss 0.718522]\n","Iter 310. [Val Acc 67%, Loss 0.717457]\n","Iter 320. [Val Acc 67%, Loss 0.716587]\n","Iter 330. [Val Acc 67%, Loss 0.715639]\n","Iter 340. [Val Acc 67%, Loss 0.714676]\n","Iter 350. [Val Acc 67%, Loss 0.713573]\n","Iter 360. [Val Acc 67%, Loss 0.712725]\n","Iter 370. [Val Acc 67%, Loss 0.711915]\n","Iter 380. [Val Acc 67%, Loss 0.710702]\n","Iter 390. [Val Acc 67%, Loss 0.709541]\n","Iter 400. [Val Acc 67%, Loss 0.708734]\n","Iter 410. [Val Acc 67%, Loss 0.707741]\n","Iter 420. [Val Acc 67%, Loss 0.706738]\n","Iter 430. [Val Acc 67%, Loss 0.705661]\n","Iter 440. [Val Acc 67%, Loss 0.704696]\n","Iter 450. [Val Acc 67%, Loss 0.703845]\n","Iter 460. [Val Acc 67%, Loss 0.703125]\n","Iter 470. [Val Acc 67%, Loss 0.702134]\n","Iter 480. [Val Acc 67%, Loss 0.701258]\n","Iter 490. [Val Acc 67%, Loss 0.700424]\n","Iter 500. [Val Acc 67%, Loss 0.699536]\n","Iter 510. [Val Acc 67%, Loss 0.698573]\n","Iter 520. [Val Acc 67%, Loss 0.697681]\n","Iter 530. [Val Acc 67%, Loss 0.696743]\n","Iter 540. [Val Acc 67%, Loss 0.695802]\n","Iter 550. [Val Acc 67%, Loss 0.694991]\n","Iter 560. [Val Acc 67%, Loss 0.694154]\n","Iter 570. [Val Acc 67%, Loss 0.693397]\n","Iter 580. [Val Acc 67%, Loss 0.692732]\n","Iter 590. [Val Acc 67%, Loss 0.691846]\n","Iter 600. [Val Acc 67%, Loss 0.691009]\n","Iter 610. [Val Acc 67%, Loss 0.690373]\n","Iter 620. [Val Acc 67%, Loss 0.689547]\n","Iter 630. [Val Acc 67%, Loss 0.688896]\n","Iter 640. [Val Acc 68%, Loss 0.688155]\n","Iter 650. [Val Acc 68%, Loss 0.687193]\n","Iter 660. [Val Acc 68%, Loss 0.686396]\n","Iter 670. [Val Acc 68%, Loss 0.685603]\n","Iter 680. [Val Acc 68%, Loss 0.684812]\n","Iter 690. [Val Acc 68%, Loss 0.684158]\n","Iter 700. [Val Acc 68%, Loss 0.683343]\n","Iter 710. [Val Acc 68%, Loss 0.682598]\n","Iter 720. [Val Acc 68%, Loss 0.682021]\n","Iter 730. [Val Acc 68%, Loss 0.681436]\n","Iter 740. [Val Acc 68%, Loss 0.680796]\n","Iter 750. [Val Acc 68%, Loss 0.679902]\n","Iter 760. [Val Acc 68%, Loss 0.679117]\n","Iter 770. [Val Acc 68%, Loss 0.678494]\n","Iter 780. [Val Acc 68%, Loss 0.677840]\n","Iter 790. [Val Acc 68%, Loss 0.677069]\n","Iter 800. [Val Acc 68%, Loss 0.676521]\n","Iter 810. [Val Acc 68%, Loss 0.675886]\n","Iter 820. [Val Acc 68%, Loss 0.675161]\n","Iter 830. [Val Acc 68%, Loss 0.674611]\n","Iter 840. [Val Acc 68%, Loss 0.674039]\n","Iter 850. [Val Acc 68%, Loss 0.673409]\n","Iter 860. [Val Acc 68%, Loss 0.672753]\n","Iter 870. [Val Acc 68%, Loss 0.672043]\n","Iter 880. [Val Acc 68%, Loss 0.671548]\n","Iter 890. [Val Acc 68%, Loss 0.671124]\n","Iter 900. [Val Acc 68%, Loss 0.670509]\n","Iter 910. [Val Acc 68%, Loss 0.669867]\n","Iter 920. [Val Acc 68%, Loss 0.669327]\n","Iter 930. [Val Acc 68%, Loss 0.668823]\n","Iter 940. [Val Acc 68%, Loss 0.668254]\n","Iter 950. [Val Acc 68%, Loss 0.667773]\n","Iter 960. [Val Acc 68%, Loss 0.667096]\n","Iter 970. [Val Acc 68%, Loss 0.666527]\n","Iter 980. [Val Acc 68%, Loss 0.666055]\n","Iter 990. [Val Acc 68%, Loss 0.665470]\n","Iter 1000. [Val Acc 68%, Loss 0.664856]\n","-------------------------\n","mu= 0.1 batch_size= 1 :\n","Iter 10. [Val Acc 68%, Loss 0.679409]\n","Iter 20. [Val Acc 68%, Loss 0.671601]\n","Iter 30. [Val Acc 68%, Loss 0.665270]\n","Iter 40. [Val Acc 69%, Loss 0.660963]\n","Iter 50. [Val Acc 69%, Loss 0.652989]\n","Iter 60. [Val Acc 69%, Loss 0.647111]\n","Iter 70. [Val Acc 69%, Loss 0.645079]\n","Iter 80. [Val Acc 70%, Loss 0.639254]\n","Iter 90. [Val Acc 70%, Loss 0.635633]\n","Iter 100. [Val Acc 70%, Loss 0.632096]\n","Iter 110. [Val Acc 70%, Loss 0.628129]\n","Iter 120. [Val Acc 70%, Loss 0.625318]\n","Iter 130. [Val Acc 70%, Loss 0.623155]\n","Iter 140. [Val Acc 71%, Loss 0.621235]\n","Iter 150. [Val Acc 71%, Loss 0.616948]\n","Iter 160. [Val Acc 71%, Loss 0.615482]\n","Iter 170. [Val Acc 71%, Loss 0.612431]\n","Iter 180. [Val Acc 70%, Loss 0.612584]\n","Iter 190. [Val Acc 71%, Loss 0.610688]\n","Iter 200. [Val Acc 71%, Loss 0.608137]\n","Iter 210. [Val Acc 71%, Loss 0.603869]\n","Iter 220. [Val Acc 71%, Loss 0.603650]\n","Iter 230. [Val Acc 71%, Loss 0.602367]\n","Iter 240. [Val Acc 71%, Loss 0.599944]\n","Iter 250. [Val Acc 71%, Loss 0.597481]\n","Iter 260. [Val Acc 71%, Loss 0.596554]\n","Iter 270. [Val Acc 71%, Loss 0.596659]\n","Iter 280. [Val Acc 71%, Loss 0.595849]\n","Iter 290. [Val Acc 71%, Loss 0.597362]\n","Iter 300. [Val Acc 72%, Loss 0.592679]\n","Iter 310. [Val Acc 72%, Loss 0.591685]\n","Iter 320. [Val Acc 72%, Loss 0.592053]\n","Iter 330. [Val Acc 72%, Loss 0.591132]\n","Iter 340. [Val Acc 72%, Loss 0.589139]\n","Iter 350. [Val Acc 72%, Loss 0.586347]\n","Iter 360. [Val Acc 72%, Loss 0.584569]\n","Iter 370. [Val Acc 72%, Loss 0.586159]\n","Iter 380. [Val Acc 72%, Loss 0.585876]\n","Iter 390. [Val Acc 72%, Loss 0.584457]\n","Iter 400. [Val Acc 72%, Loss 0.583038]\n","Iter 410. [Val Acc 72%, Loss 0.583724]\n","Iter 420. [Val Acc 72%, Loss 0.583272]\n","Iter 430. [Val Acc 72%, Loss 0.580109]\n","Iter 440. [Val Acc 72%, Loss 0.580403]\n","Iter 450. [Val Acc 72%, Loss 0.580265]\n","Iter 460. [Val Acc 72%, Loss 0.581923]\n","Iter 470. [Val Acc 73%, Loss 0.578253]\n","Iter 480. [Val Acc 73%, Loss 0.579597]\n","Iter 490. [Val Acc 73%, Loss 0.577945]\n","Iter 500. [Val Acc 73%, Loss 0.576872]\n","Iter 510. [Val Acc 72%, Loss 0.580532]\n","Iter 520. [Val Acc 73%, Loss 0.577793]\n","Iter 530. [Val Acc 73%, Loss 0.575839]\n","Iter 540. [Val Acc 73%, Loss 0.573407]\n","Iter 550. [Val Acc 73%, Loss 0.572914]\n","Iter 560. [Val Acc 73%, Loss 0.571298]\n","Iter 570. [Val Acc 73%, Loss 0.573441]\n","Iter 580. [Val Acc 73%, Loss 0.573641]\n","Iter 590. [Val Acc 73%, Loss 0.569347]\n","Iter 600. [Val Acc 73%, Loss 0.570235]\n","Iter 610. [Val Acc 73%, Loss 0.570195]\n","Iter 620. [Val Acc 73%, Loss 0.570531]\n","Iter 630. [Val Acc 73%, Loss 0.569130]\n","Iter 640. [Val Acc 73%, Loss 0.570484]\n","Iter 650. [Val Acc 73%, Loss 0.570825]\n","Iter 660. [Val Acc 73%, Loss 0.570190]\n","Iter 670. [Val Acc 73%, Loss 0.568838]\n","Iter 680. [Val Acc 73%, Loss 0.568787]\n","Iter 690. [Val Acc 73%, Loss 0.573686]\n","Iter 700. [Val Acc 73%, Loss 0.569949]\n","Iter 710. [Val Acc 73%, Loss 0.569296]\n","Iter 720. [Val Acc 73%, Loss 0.567528]\n","Iter 730. [Val Acc 73%, Loss 0.567788]\n","Iter 740. [Val Acc 73%, Loss 0.565928]\n","Iter 750. [Val Acc 73%, Loss 0.567437]\n","Iter 760. [Val Acc 73%, Loss 0.566625]\n","Iter 770. [Val Acc 73%, Loss 0.565927]\n","Iter 780. [Val Acc 73%, Loss 0.565327]\n","Iter 790. [Val Acc 73%, Loss 0.564760]\n","Iter 800. [Val Acc 73%, Loss 0.565287]\n","Iter 810. [Val Acc 73%, Loss 0.564235]\n","Iter 820. [Val Acc 73%, Loss 0.564770]\n","Iter 830. [Val Acc 73%, Loss 0.565330]\n","Iter 840. [Val Acc 73%, Loss 0.570959]\n","Iter 850. [Val Acc 73%, Loss 0.567317]\n","Iter 860. [Val Acc 73%, Loss 0.566348]\n","Iter 870. [Val Acc 73%, Loss 0.570058]\n","Iter 880. [Val Acc 73%, Loss 0.565754]\n","Iter 890. [Val Acc 73%, Loss 0.567168]\n","Iter 900. [Val Acc 73%, Loss 0.567377]\n","Iter 910. [Val Acc 73%, Loss 0.570124]\n","Iter 920. [Val Acc 73%, Loss 0.569124]\n","Iter 930. [Val Acc 73%, Loss 0.563923]\n","Iter 940. [Val Acc 73%, Loss 0.567770]\n","Iter 950. [Val Acc 73%, Loss 0.565879]\n","Iter 960. [Val Acc 73%, Loss 0.564209]\n","Iter 970. [Val Acc 73%, Loss 0.565112]\n","Iter 980. [Val Acc 73%, Loss 0.563434]\n","Iter 990. [Val Acc 73%, Loss 0.563550]\n","Iter 1000. [Val Acc 73%, Loss 0.563739]\n","-------------------------\n","mu= 0.1 batch_size= 64 :\n","Iter 10. [Val Acc 70%, Loss 0.592290]\n","Iter 20. [Val Acc 70%, Loss 0.585235]\n","Iter 30. [Val Acc 71%, Loss 0.579702]\n","Iter 40. [Val Acc 72%, Loss 0.572464]\n","Iter 50. [Val Acc 72%, Loss 0.571763]\n","Iter 60. [Val Acc 73%, Loss 0.571309]\n","Iter 70. [Val Acc 73%, Loss 0.571737]\n","Iter 80. [Val Acc 73%, Loss 0.567088]\n","Iter 90. [Val Acc 73%, Loss 0.565484]\n","Iter 100. [Val Acc 73%, Loss 0.565041]\n","Iter 110. [Val Acc 73%, Loss 0.563843]\n","Iter 120. [Val Acc 73%, Loss 0.567036]\n","Iter 130. [Val Acc 73%, Loss 0.563507]\n","Iter 140. [Val Acc 73%, Loss 0.563263]\n","Iter 150. [Val Acc 73%, Loss 0.562121]\n","Iter 160. [Val Acc 73%, Loss 0.562797]\n","Iter 170. [Val Acc 73%, Loss 0.564114]\n","Iter 180. [Val Acc 73%, Loss 0.563105]\n","Iter 190. [Val Acc 73%, Loss 0.562744]\n","Iter 200. [Val Acc 73%, Loss 0.562479]\n","Iter 210. [Val Acc 73%, Loss 0.562616]\n","Iter 220. [Val Acc 73%, Loss 0.566107]\n","Iter 230. [Val Acc 73%, Loss 0.564263]\n","Iter 240. [Val Acc 73%, Loss 0.562913]\n","Iter 250. [Val Acc 73%, Loss 0.562721]\n","Iter 260. [Val Acc 73%, Loss 0.561437]\n","Iter 270. [Val Acc 73%, Loss 0.562036]\n","Iter 280. [Val Acc 73%, Loss 0.561121]\n","Iter 290. [Val Acc 73%, Loss 0.562473]\n","Iter 300. [Val Acc 73%, Loss 0.562500]\n","Iter 310. [Val Acc 73%, Loss 0.563313]\n","Iter 320. [Val Acc 73%, Loss 0.562950]\n","Iter 330. [Val Acc 73%, Loss 0.561976]\n","Iter 340. [Val Acc 73%, Loss 0.561059]\n","Iter 350. [Val Acc 73%, Loss 0.560430]\n","Iter 360. [Val Acc 73%, Loss 0.561136]\n","Iter 370. [Val Acc 73%, Loss 0.560419]\n","Iter 380. [Val Acc 73%, Loss 0.560722]\n","Iter 390. [Val Acc 73%, Loss 0.560222]\n","Iter 400. [Val Acc 73%, Loss 0.561431]\n","Iter 410. [Val Acc 73%, Loss 0.560787]\n","Iter 420. [Val Acc 73%, Loss 0.560813]\n","Iter 430. [Val Acc 73%, Loss 0.563041]\n","Iter 440. [Val Acc 73%, Loss 0.562902]\n","Iter 450. [Val Acc 73%, Loss 0.562116]\n","Iter 460. [Val Acc 73%, Loss 0.562575]\n","Iter 470. [Val Acc 73%, Loss 0.562176]\n","Iter 480. [Val Acc 73%, Loss 0.561807]\n","Iter 490. [Val Acc 73%, Loss 0.561357]\n","Iter 500. [Val Acc 73%, Loss 0.561262]\n","Iter 510. [Val Acc 73%, Loss 0.564092]\n","Iter 520. [Val Acc 73%, Loss 0.561313]\n","Iter 530. [Val Acc 73%, Loss 0.560520]\n","Iter 540. [Val Acc 73%, Loss 0.561145]\n","Iter 550. [Val Acc 73%, Loss 0.563100]\n","Iter 560. [Val Acc 73%, Loss 0.562265]\n","Iter 570. [Val Acc 73%, Loss 0.562030]\n","Iter 580. [Val Acc 73%, Loss 0.559475]\n","Iter 590. [Val Acc 73%, Loss 0.560141]\n","Iter 600. [Val Acc 73%, Loss 0.561045]\n","Iter 610. [Val Acc 73%, Loss 0.561885]\n","Iter 620. [Val Acc 73%, Loss 0.560077]\n","Iter 630. [Val Acc 74%, Loss 0.559479]\n","Iter 640. [Val Acc 74%, Loss 0.559959]\n","Iter 650. [Val Acc 73%, Loss 0.560518]\n","Iter 660. [Val Acc 73%, Loss 0.561056]\n","Iter 670. [Val Acc 73%, Loss 0.559532]\n","Iter 680. [Val Acc 74%, Loss 0.559924]\n","Iter 690. [Val Acc 73%, Loss 0.561509]\n","Iter 700. [Val Acc 73%, Loss 0.560791]\n","Iter 710. [Val Acc 73%, Loss 0.560777]\n","Iter 720. [Val Acc 74%, Loss 0.559533]\n","Iter 730. [Val Acc 74%, Loss 0.559723]\n","Iter 740. [Val Acc 74%, Loss 0.560264]\n","Iter 750. [Val Acc 74%, Loss 0.560015]\n","Iter 760. [Val Acc 73%, Loss 0.562035]\n","Iter 770. [Val Acc 73%, Loss 0.561162]\n","Iter 780. [Val Acc 73%, Loss 0.561993]\n","Iter 790. [Val Acc 73%, Loss 0.562308]\n","Iter 800. [Val Acc 74%, Loss 0.560469]\n","Iter 810. [Val Acc 73%, Loss 0.560603]\n","Iter 820. [Val Acc 73%, Loss 0.562177]\n","Iter 830. [Val Acc 73%, Loss 0.559160]\n","Iter 840. [Val Acc 73%, Loss 0.560149]\n","Iter 850. [Val Acc 74%, Loss 0.560336]\n","Iter 860. [Val Acc 73%, Loss 0.560631]\n","Iter 870. [Val Acc 73%, Loss 0.560824]\n","Iter 880. [Val Acc 73%, Loss 0.561900]\n","Iter 890. [Val Acc 74%, Loss 0.560320]\n","Iter 900. [Val Acc 73%, Loss 0.561141]\n","Iter 910. [Val Acc 73%, Loss 0.561781]\n","Iter 920. [Val Acc 73%, Loss 0.562192]\n","Iter 930. [Val Acc 73%, Loss 0.561363]\n","Iter 940. [Val Acc 74%, Loss 0.560408]\n","Iter 950. [Val Acc 73%, Loss 0.560634]\n","Iter 960. [Val Acc 73%, Loss 0.561237]\n","Iter 970. [Val Acc 73%, Loss 0.560777]\n","Iter 980. [Val Acc 73%, Loss 0.560328]\n","Iter 990. [Val Acc 73%, Loss 0.561638]\n","Iter 1000. [Val Acc 73%, Loss 0.561736]\n","-------------------------\n","mu= 0.1 batch_size= 1024 :\n","Iter 10. [Val Acc 70%, Loss 0.588655]\n","Iter 20. [Val Acc 71%, Loss 0.579326]\n","Iter 30. [Val Acc 72%, Loss 0.572884]\n","Iter 40. [Val Acc 72%, Loss 0.568879]\n","Iter 50. [Val Acc 73%, Loss 0.566292]\n","Iter 60. [Val Acc 73%, Loss 0.564891]\n","Iter 70. [Val Acc 73%, Loss 0.561464]\n","Iter 80. [Val Acc 73%, Loss 0.561306]\n","Iter 90. [Val Acc 73%, Loss 0.561099]\n","Iter 100. [Val Acc 73%, Loss 0.562469]\n","Iter 110. [Val Acc 73%, Loss 0.563372]\n","Iter 120. [Val Acc 73%, Loss 0.561993]\n","Iter 130. [Val Acc 73%, Loss 0.561547]\n","Iter 140. [Val Acc 73%, Loss 0.561684]\n","Iter 150. [Val Acc 73%, Loss 0.558833]\n","Iter 160. [Val Acc 73%, Loss 0.559328]\n","Iter 170. [Val Acc 73%, Loss 0.559064]\n","Iter 180. [Val Acc 73%, Loss 0.562386]\n","Iter 190. [Val Acc 73%, Loss 0.562403]\n","Iter 200. [Val Acc 73%, Loss 0.561534]\n","Iter 210. [Val Acc 73%, Loss 0.561327]\n","Iter 220. [Val Acc 73%, Loss 0.560810]\n","Iter 230. [Val Acc 74%, Loss 0.560575]\n","Iter 240. [Val Acc 74%, Loss 0.559611]\n","Iter 250. [Val Acc 74%, Loss 0.559275]\n","Iter 260. [Val Acc 73%, Loss 0.559620]\n","Iter 270. [Val Acc 73%, Loss 0.562252]\n","Iter 280. [Val Acc 73%, Loss 0.560616]\n","Iter 290. [Val Acc 74%, Loss 0.562359]\n","Iter 300. [Val Acc 73%, Loss 0.560249]\n","Iter 310. [Val Acc 73%, Loss 0.561329]\n","Iter 320. [Val Acc 73%, Loss 0.564209]\n","Iter 330. [Val Acc 73%, Loss 0.561896]\n","Iter 340. [Val Acc 73%, Loss 0.560753]\n","Iter 350. [Val Acc 73%, Loss 0.560915]\n","Iter 360. [Val Acc 73%, Loss 0.560551]\n","Iter 370. [Val Acc 73%, Loss 0.560028]\n","Iter 380. [Val Acc 73%, Loss 0.559377]\n","Iter 390. [Val Acc 74%, Loss 0.559433]\n","Iter 400. [Val Acc 73%, Loss 0.560036]\n","Iter 410. [Val Acc 73%, Loss 0.559280]\n","Iter 420. [Val Acc 73%, Loss 0.559554]\n","Iter 430. [Val Acc 73%, Loss 0.559622]\n","Iter 440. [Val Acc 73%, Loss 0.562370]\n","Iter 450. [Val Acc 73%, Loss 0.560087]\n","Iter 460. [Val Acc 73%, Loss 0.561199]\n","Iter 470. [Val Acc 73%, Loss 0.560547]\n","Iter 480. [Val Acc 73%, Loss 0.560935]\n","Iter 490. [Val Acc 73%, Loss 0.560348]\n","Iter 500. [Val Acc 73%, Loss 0.560621]\n","Iter 510. [Val Acc 73%, Loss 0.560382]\n","Iter 520. [Val Acc 73%, Loss 0.561751]\n","Iter 530. [Val Acc 73%, Loss 0.563205]\n","Iter 540. [Val Acc 73%, Loss 0.564479]\n","Iter 550. [Val Acc 73%, Loss 0.565028]\n","Iter 560. [Val Acc 73%, Loss 0.561909]\n","Iter 570. [Val Acc 73%, Loss 0.561870]\n","Iter 580. [Val Acc 73%, Loss 0.563272]\n","Iter 590. [Val Acc 73%, Loss 0.561354]\n","Iter 600. [Val Acc 73%, Loss 0.561165]\n","Iter 610. [Val Acc 73%, Loss 0.559553]\n","Iter 620. [Val Acc 73%, Loss 0.562268]\n","Iter 630. [Val Acc 73%, Loss 0.559616]\n","Iter 640. [Val Acc 73%, Loss 0.560678]\n","Iter 650. [Val Acc 73%, Loss 0.560354]\n","Iter 660. [Val Acc 73%, Loss 0.563479]\n","Iter 670. [Val Acc 74%, Loss 0.560747]\n","Iter 680. [Val Acc 73%, Loss 0.559797]\n","Iter 690. [Val Acc 73%, Loss 0.561270]\n","Iter 700. [Val Acc 74%, Loss 0.559975]\n","Iter 710. [Val Acc 73%, Loss 0.565731]\n","Iter 720. [Val Acc 74%, Loss 0.562695]\n","Iter 730. [Val Acc 73%, Loss 0.560558]\n","Iter 740. [Val Acc 73%, Loss 0.561093]\n","Iter 750. [Val Acc 74%, Loss 0.561564]\n","Iter 760. [Val Acc 74%, Loss 0.560283]\n","Iter 770. [Val Acc 73%, Loss 0.560645]\n","Iter 780. [Val Acc 73%, Loss 0.561482]\n","Iter 790. [Val Acc 74%, Loss 0.559948]\n","Iter 800. [Val Acc 73%, Loss 0.560110]\n","Iter 810. [Val Acc 73%, Loss 0.561499]\n","Iter 820. [Val Acc 73%, Loss 0.560626]\n","Iter 830. [Val Acc 73%, Loss 0.561546]\n","Iter 840. [Val Acc 73%, Loss 0.562858]\n","Iter 850. [Val Acc 73%, Loss 0.560825]\n","Iter 860. [Val Acc 73%, Loss 0.561020]\n","Iter 870. [Val Acc 73%, Loss 0.560428]\n","Iter 880. [Val Acc 73%, Loss 0.558847]\n","Iter 890. [Val Acc 73%, Loss 0.560968]\n","Iter 900. [Val Acc 73%, Loss 0.560754]\n","Iter 910. [Val Acc 73%, Loss 0.559668]\n","Iter 920. [Val Acc 73%, Loss 0.559342]\n","Iter 930. [Val Acc 73%, Loss 0.560140]\n","Iter 940. [Val Acc 73%, Loss 0.558062]\n","Iter 950. [Val Acc 73%, Loss 0.559619]\n","Iter 960. [Val Acc 73%, Loss 0.561443]\n","Iter 970. [Val Acc 73%, Loss 0.559474]\n","Iter 980. [Val Acc 73%, Loss 0.560724]\n","Iter 990. [Val Acc 73%, Loss 0.563589]\n","Iter 1000. [Val Acc 73%, Loss 0.559996]\n","-------------------------\n","mu= 0.1 batch_size= 50000 :\n","Iter 10. [Val Acc 70%, Loss 0.587093]\n","Iter 20. [Val Acc 71%, Loss 0.581235]\n","Iter 30. [Val Acc 72%, Loss 0.574132]\n","Iter 40. [Val Acc 72%, Loss 0.571098]\n","Iter 50. [Val Acc 72%, Loss 0.568353]\n","Iter 60. [Val Acc 73%, Loss 0.564279]\n","Iter 70. [Val Acc 73%, Loss 0.564496]\n","Iter 80. [Val Acc 73%, Loss 0.562809]\n","Iter 90. [Val Acc 73%, Loss 0.561855]\n","Iter 100. [Val Acc 73%, Loss 0.560910]\n","Iter 110. [Val Acc 73%, Loss 0.560603]\n","Iter 120. [Val Acc 73%, Loss 0.561356]\n","Iter 130. [Val Acc 73%, Loss 0.562828]\n","Iter 140. [Val Acc 73%, Loss 0.561949]\n","Iter 150. [Val Acc 73%, Loss 0.560387]\n","Iter 160. [Val Acc 73%, Loss 0.561294]\n","Iter 170. [Val Acc 73%, Loss 0.560468]\n","Iter 180. [Val Acc 73%, Loss 0.564345]\n","Iter 190. [Val Acc 73%, Loss 0.559498]\n","Iter 200. [Val Acc 74%, Loss 0.562014]\n","Iter 210. [Val Acc 73%, Loss 0.559234]\n","Iter 220. [Val Acc 73%, Loss 0.559281]\n","Iter 230. [Val Acc 73%, Loss 0.559418]\n","Iter 240. [Val Acc 73%, Loss 0.561295]\n","Iter 250. [Val Acc 73%, Loss 0.559458]\n","Iter 260. [Val Acc 74%, Loss 0.559566]\n","Iter 270. [Val Acc 73%, Loss 0.560627]\n","Iter 280. [Val Acc 73%, Loss 0.559881]\n","Iter 290. [Val Acc 73%, Loss 0.558768]\n","Iter 300. [Val Acc 73%, Loss 0.559939]\n","Iter 310. [Val Acc 73%, Loss 0.560570]\n","Iter 320. [Val Acc 73%, Loss 0.561866]\n","Iter 330. [Val Acc 73%, Loss 0.559524]\n","Iter 340. [Val Acc 73%, Loss 0.559664]\n","Iter 350. [Val Acc 73%, Loss 0.559450]\n","Iter 360. [Val Acc 73%, Loss 0.559615]\n","Iter 370. [Val Acc 73%, Loss 0.559145]\n","Iter 380. [Val Acc 74%, Loss 0.559157]\n","Iter 390. [Val Acc 73%, Loss 0.559673]\n","Iter 400. [Val Acc 74%, Loss 0.559804]\n","Iter 410. [Val Acc 73%, Loss 0.562760]\n","Iter 420. [Val Acc 73%, Loss 0.560370]\n","Iter 430. [Val Acc 73%, Loss 0.561171]\n","Iter 440. [Val Acc 73%, Loss 0.564657]\n","Iter 450. [Val Acc 73%, Loss 0.561261]\n","Iter 460. [Val Acc 73%, Loss 0.563374]\n","Iter 470. [Val Acc 73%, Loss 0.559754]\n","Iter 480. [Val Acc 73%, Loss 0.560187]\n","Iter 490. [Val Acc 73%, Loss 0.559167]\n","Iter 500. [Val Acc 73%, Loss 0.561825]\n","Iter 510. [Val Acc 73%, Loss 0.558672]\n","Iter 520. [Val Acc 73%, Loss 0.560738]\n","Iter 530. [Val Acc 73%, Loss 0.560045]\n","Iter 540. [Val Acc 73%, Loss 0.559471]\n","Iter 550. [Val Acc 73%, Loss 0.560768]\n","Iter 560. [Val Acc 73%, Loss 0.560978]\n","Iter 570. [Val Acc 74%, Loss 0.559859]\n","Iter 580. [Val Acc 73%, Loss 0.560666]\n","Iter 590. [Val Acc 74%, Loss 0.559373]\n","Iter 600. [Val Acc 74%, Loss 0.558546]\n","Iter 610. [Val Acc 74%, Loss 0.558270]\n","Iter 620. [Val Acc 74%, Loss 0.559078]\n","Iter 630. [Val Acc 73%, Loss 0.559734]\n","Iter 640. [Val Acc 74%, Loss 0.559508]\n","Iter 650. [Val Acc 73%, Loss 0.564200]\n","Iter 660. [Val Acc 73%, Loss 0.560632]\n","Iter 670. [Val Acc 73%, Loss 0.559314]\n","Iter 680. [Val Acc 73%, Loss 0.561126]\n","Iter 690. [Val Acc 74%, Loss 0.559224]\n","Iter 700. [Val Acc 73%, Loss 0.560274]\n","Iter 710. [Val Acc 73%, Loss 0.560439]\n","Iter 720. [Val Acc 74%, Loss 0.558450]\n","Iter 730. [Val Acc 74%, Loss 0.558533]\n","Iter 740. [Val Acc 74%, Loss 0.558783]\n","Iter 750. [Val Acc 73%, Loss 0.560155]\n","Iter 760. [Val Acc 74%, Loss 0.558911]\n","Iter 770. [Val Acc 73%, Loss 0.564054]\n","Iter 780. [Val Acc 73%, Loss 0.560115]\n","Iter 790. [Val Acc 73%, Loss 0.559701]\n","Iter 800. [Val Acc 73%, Loss 0.560570]\n","Iter 810. [Val Acc 73%, Loss 0.559442]\n","Iter 820. [Val Acc 73%, Loss 0.560083]\n","Iter 830. [Val Acc 73%, Loss 0.560481]\n","Iter 840. [Val Acc 73%, Loss 0.559980]\n","Iter 850. [Val Acc 73%, Loss 0.559379]\n","Iter 860. [Val Acc 73%, Loss 0.559846]\n","Iter 870. [Val Acc 73%, Loss 0.561364]\n","Iter 880. [Val Acc 73%, Loss 0.562525]\n","Iter 890. [Val Acc 73%, Loss 0.561186]\n","Iter 900. [Val Acc 73%, Loss 0.559891]\n","Iter 910. [Val Acc 73%, Loss 0.560325]\n","Iter 920. [Val Acc 73%, Loss 0.561209]\n","Iter 930. [Val Acc 73%, Loss 0.559837]\n","Iter 940. [Val Acc 73%, Loss 0.558987]\n","Iter 950. [Val Acc 73%, Loss 0.561734]\n","Iter 960. [Val Acc 73%, Loss 0.559366]\n","Iter 970. [Val Acc 73%, Loss 0.560490]\n","Iter 980. [Val Acc 73%, Loss 0.559810]\n","Iter 990. [Val Acc 74%, Loss 0.558847]\n","Iter 1000. [Val Acc 74%, Loss 0.558762]\n","-------------------------\n","mu= 0.5 batch_size= 1 :\n","Iter 10. [Val Acc 71%, Loss 0.581826]\n","Iter 20. [Val Acc 71%, Loss 0.587028]\n","Iter 30. [Val Acc 72%, Loss 0.583999]\n","Iter 40. [Val Acc 73%, Loss 0.574338]\n","Iter 50. [Val Acc 72%, Loss 0.581830]\n","Iter 60. [Val Acc 71%, Loss 0.592591]\n","Iter 70. [Val Acc 71%, Loss 0.584839]\n","Iter 80. [Val Acc 72%, Loss 0.582638]\n","Iter 90. [Val Acc 72%, Loss 0.580157]\n","Iter 100. [Val Acc 72%, Loss 0.582450]\n","Iter 110. [Val Acc 72%, Loss 0.582991]\n","Iter 120. [Val Acc 71%, Loss 0.584508]\n","Iter 130. [Val Acc 71%, Loss 0.580782]\n","Iter 140. [Val Acc 73%, Loss 0.578765]\n","Iter 150. [Val Acc 72%, Loss 0.577924]\n","Iter 160. [Val Acc 72%, Loss 0.589741]\n","Iter 170. [Val Acc 71%, Loss 0.595559]\n","Iter 180. [Val Acc 71%, Loss 0.589188]\n","Iter 190. [Val Acc 72%, Loss 0.581944]\n","Iter 200. [Val Acc 70%, Loss 0.629927]\n","Iter 210. [Val Acc 72%, Loss 0.579404]\n","Iter 220. [Val Acc 71%, Loss 0.592928]\n","Iter 230. [Val Acc 72%, Loss 0.583686]\n","Iter 240. [Val Acc 71%, Loss 0.603126]\n","Iter 250. [Val Acc 72%, Loss 0.580994]\n","Iter 260. [Val Acc 72%, Loss 0.588131]\n","Iter 270. [Val Acc 72%, Loss 0.586901]\n","Iter 280. [Val Acc 72%, Loss 0.584389]\n","Iter 290. [Val Acc 73%, Loss 0.583983]\n","Iter 300. [Val Acc 71%, Loss 0.592173]\n","Iter 310. [Val Acc 72%, Loss 0.581581]\n","Iter 320. [Val Acc 72%, Loss 0.589940]\n","Iter 330. [Val Acc 72%, Loss 0.585270]\n","Iter 340. [Val Acc 71%, Loss 0.599005]\n","Iter 350. [Val Acc 72%, Loss 0.584067]\n","Iter 360. [Val Acc 72%, Loss 0.575166]\n","Iter 370. [Val Acc 72%, Loss 0.587395]\n","Iter 380. [Val Acc 71%, Loss 0.588687]\n","Iter 390. [Val Acc 72%, Loss 0.588800]\n","Iter 400. [Val Acc 72%, Loss 0.586720]\n","Iter 410. [Val Acc 72%, Loss 0.582128]\n","Iter 420. [Val Acc 71%, Loss 0.598927]\n","Iter 430. [Val Acc 71%, Loss 0.602086]\n","Iter 440. [Val Acc 73%, Loss 0.572028]\n","Iter 450. [Val Acc 72%, Loss 0.581195]\n","Iter 460. [Val Acc 72%, Loss 0.584306]\n","Iter 470. [Val Acc 72%, Loss 0.575389]\n","Iter 480. [Val Acc 71%, Loss 0.592165]\n","Iter 490. [Val Acc 70%, Loss 0.604073]\n","Iter 500. [Val Acc 72%, Loss 0.587972]\n","Iter 510. [Val Acc 70%, Loss 0.607682]\n","Iter 520. [Val Acc 72%, Loss 0.580047]\n","Iter 530. [Val Acc 72%, Loss 0.578526]\n","Iter 540. [Val Acc 72%, Loss 0.588844]\n","Iter 550. [Val Acc 73%, Loss 0.579851]\n","Iter 560. [Val Acc 72%, Loss 0.589152]\n","Iter 570. [Val Acc 72%, Loss 0.585632]\n","Iter 580. [Val Acc 72%, Loss 0.580526]\n","Iter 590. [Val Acc 72%, Loss 0.580519]\n","Iter 600. [Val Acc 72%, Loss 0.576636]\n","Iter 610. [Val Acc 72%, Loss 0.574260]\n","Iter 620. [Val Acc 73%, Loss 0.573735]\n","Iter 630. [Val Acc 72%, Loss 0.577194]\n","Iter 640. [Val Acc 73%, Loss 0.578603]\n","Iter 650. [Val Acc 72%, Loss 0.595883]\n","Iter 660. [Val Acc 72%, Loss 0.582291]\n","Iter 670. [Val Acc 73%, Loss 0.578250]\n","Iter 680. [Val Acc 70%, Loss 0.602881]\n","Iter 690. [Val Acc 72%, Loss 0.584581]\n","Iter 700. [Val Acc 72%, Loss 0.577787]\n","Iter 710. [Val Acc 72%, Loss 0.580681]\n","Iter 720. [Val Acc 71%, Loss 0.597347]\n","Iter 730. [Val Acc 72%, Loss 0.578597]\n","Iter 740. [Val Acc 72%, Loss 0.578396]\n","Iter 750. [Val Acc 72%, Loss 0.590653]\n","Iter 760. [Val Acc 71%, Loss 0.591098]\n","Iter 770. [Val Acc 72%, Loss 0.579618]\n","Iter 780. [Val Acc 72%, Loss 0.581651]\n","Iter 790. [Val Acc 72%, Loss 0.584673]\n","Iter 800. [Val Acc 72%, Loss 0.590660]\n","Iter 810. [Val Acc 71%, Loss 0.594152]\n","Iter 820. [Val Acc 70%, Loss 0.595766]\n","Iter 830. [Val Acc 71%, Loss 0.585496]\n","Iter 840. [Val Acc 72%, Loss 0.580777]\n","Iter 850. [Val Acc 70%, Loss 0.592646]\n","Iter 860. [Val Acc 72%, Loss 0.581417]\n","Iter 870. [Val Acc 72%, Loss 0.580703]\n","Iter 880. [Val Acc 72%, Loss 0.585556]\n","Iter 890. [Val Acc 72%, Loss 0.575730]\n","Iter 900. [Val Acc 71%, Loss 0.603648]\n","Iter 910. [Val Acc 72%, Loss 0.587088]\n","Iter 920. [Val Acc 72%, Loss 0.579448]\n","Iter 930. [Val Acc 71%, Loss 0.597108]\n","Iter 940. [Val Acc 71%, Loss 0.594602]\n","Iter 950. [Val Acc 70%, Loss 0.611177]\n","Iter 960. [Val Acc 70%, Loss 0.606051]\n","Iter 970. [Val Acc 72%, Loss 0.580752]\n","Iter 980. [Val Acc 72%, Loss 0.580853]\n","Iter 990. [Val Acc 72%, Loss 0.576678]\n","Iter 1000. [Val Acc 72%, Loss 0.585513]\n","-------------------------\n","mu= 0.5 batch_size= 64 :\n","Iter 10. [Val Acc 70%, Loss 0.606173]\n","Iter 20. [Val Acc 72%, Loss 0.577952]\n","Iter 30. [Val Acc 72%, Loss 0.588444]\n","Iter 40. [Val Acc 72%, Loss 0.594655]\n","Iter 50. [Val Acc 72%, Loss 0.588764]\n","Iter 60. [Val Acc 72%, Loss 0.584706]\n","Iter 70. [Val Acc 73%, Loss 0.575156]\n","Iter 80. [Val Acc 72%, Loss 0.583978]\n","Iter 90. [Val Acc 72%, Loss 0.584858]\n","Iter 100. [Val Acc 72%, Loss 0.576421]\n","Iter 110. [Val Acc 72%, Loss 0.585372]\n","Iter 120. [Val Acc 72%, Loss 0.584066]\n","Iter 130. [Val Acc 72%, Loss 0.587507]\n","Iter 140. [Val Acc 71%, Loss 0.595414]\n","Iter 150. [Val Acc 72%, Loss 0.579779]\n","Iter 160. [Val Acc 73%, Loss 0.575165]\n","Iter 170. [Val Acc 73%, Loss 0.575980]\n","Iter 180. [Val Acc 72%, Loss 0.573750]\n","Iter 190. [Val Acc 73%, Loss 0.576006]\n","Iter 200. [Val Acc 71%, Loss 0.588216]\n","Iter 210. [Val Acc 72%, Loss 0.572224]\n","Iter 220. [Val Acc 72%, Loss 0.591985]\n","Iter 230. [Val Acc 72%, Loss 0.583819]\n","Iter 240. [Val Acc 70%, Loss 0.606819]\n","Iter 250. [Val Acc 72%, Loss 0.577677]\n","Iter 260. [Val Acc 73%, Loss 0.573635]\n","Iter 270. [Val Acc 72%, Loss 0.588160]\n","Iter 280. [Val Acc 72%, Loss 0.581591]\n","Iter 290. [Val Acc 73%, Loss 0.577572]\n","Iter 300. [Val Acc 73%, Loss 0.571340]\n","Iter 310. [Val Acc 72%, Loss 0.583058]\n","Iter 320. [Val Acc 71%, Loss 0.598982]\n","Iter 330. [Val Acc 72%, Loss 0.582725]\n","Iter 340. [Val Acc 72%, Loss 0.586177]\n","Iter 350. [Val Acc 71%, Loss 0.595499]\n","Iter 360. [Val Acc 71%, Loss 0.599800]\n","Iter 370. [Val Acc 70%, Loss 0.636335]\n","Iter 380. [Val Acc 71%, Loss 0.590918]\n","Iter 390. [Val Acc 72%, Loss 0.575994]\n","Iter 400. [Val Acc 72%, Loss 0.578398]\n","Iter 410. [Val Acc 71%, Loss 0.589907]\n","Iter 420. [Val Acc 72%, Loss 0.591513]\n","Iter 430. [Val Acc 72%, Loss 0.581741]\n","Iter 440. [Val Acc 69%, Loss 0.629751]\n","Iter 450. [Val Acc 73%, Loss 0.572014]\n","Iter 460. [Val Acc 71%, Loss 0.582875]\n","Iter 470. [Val Acc 71%, Loss 0.586935]\n","Iter 480. [Val Acc 72%, Loss 0.575920]\n","Iter 490. [Val Acc 72%, Loss 0.586978]\n","Iter 500. [Val Acc 73%, Loss 0.573966]\n","Iter 510. [Val Acc 73%, Loss 0.576928]\n","Iter 520. [Val Acc 72%, Loss 0.579794]\n","Iter 530. [Val Acc 70%, Loss 0.594891]\n","Iter 540. [Val Acc 73%, Loss 0.567873]\n","Iter 550. [Val Acc 71%, Loss 0.597763]\n","Iter 560. [Val Acc 72%, Loss 0.586892]\n","Iter 570. [Val Acc 69%, Loss 0.660220]\n","Iter 580. [Val Acc 72%, Loss 0.591923]\n","Iter 590. [Val Acc 72%, Loss 0.584776]\n","Iter 600. [Val Acc 72%, Loss 0.583607]\n","Iter 610. [Val Acc 72%, Loss 0.586966]\n","Iter 620. [Val Acc 72%, Loss 0.581606]\n","Iter 630. [Val Acc 72%, Loss 0.589886]\n","Iter 640. [Val Acc 73%, Loss 0.572823]\n","Iter 650. [Val Acc 71%, Loss 0.597025]\n","Iter 660. [Val Acc 72%, Loss 0.584365]\n","Iter 670. [Val Acc 72%, Loss 0.591095]\n","Iter 680. [Val Acc 72%, Loss 0.601383]\n","Iter 690. [Val Acc 72%, Loss 0.581686]\n","Iter 700. [Val Acc 73%, Loss 0.579637]\n","Iter 710. [Val Acc 72%, Loss 0.585211]\n","Iter 720. [Val Acc 73%, Loss 0.578331]\n","Iter 730. [Val Acc 73%, Loss 0.574596]\n","Iter 740. [Val Acc 72%, Loss 0.582471]\n","Iter 750. [Val Acc 71%, Loss 0.600619]\n","Iter 760. [Val Acc 72%, Loss 0.575043]\n","Iter 770. [Val Acc 73%, Loss 0.582886]\n","Iter 780. [Val Acc 73%, Loss 0.589236]\n","Iter 790. [Val Acc 73%, Loss 0.583253]\n","Iter 800. [Val Acc 71%, Loss 0.603794]\n","Iter 810. [Val Acc 73%, Loss 0.591337]\n","Iter 820. [Val Acc 71%, Loss 0.588915]\n","Iter 830. [Val Acc 71%, Loss 0.607162]\n","Iter 840. [Val Acc 72%, Loss 0.579243]\n","Iter 850. [Val Acc 72%, Loss 0.579465]\n","Iter 860. [Val Acc 71%, Loss 0.593722]\n","Iter 870. [Val Acc 72%, Loss 0.590360]\n","Iter 880. [Val Acc 70%, Loss 0.619208]\n","Iter 890. [Val Acc 72%, Loss 0.579833]\n","Iter 900. [Val Acc 72%, Loss 0.581384]\n","Iter 910. [Val Acc 72%, Loss 0.581583]\n","Iter 920. [Val Acc 72%, Loss 0.586999]\n","Iter 930. [Val Acc 72%, Loss 0.587639]\n","Iter 940. [Val Acc 71%, Loss 0.590603]\n","Iter 950. [Val Acc 72%, Loss 0.578699]\n","Iter 960. [Val Acc 72%, Loss 0.599932]\n","Iter 970. [Val Acc 73%, Loss 0.573691]\n","Iter 980. [Val Acc 72%, Loss 0.588930]\n","Iter 990. [Val Acc 72%, Loss 0.589555]\n","Iter 1000. [Val Acc 71%, Loss 0.591204]\n","-------------------------\n","mu= 0.5 batch_size= 1024 :\n","Iter 10. [Val Acc 71%, Loss 0.589210]\n","Iter 20. [Val Acc 72%, Loss 0.603029]\n","Iter 30. [Val Acc 65%, Loss 0.744261]\n","Iter 40. [Val Acc 71%, Loss 0.602223]\n","Iter 50. [Val Acc 72%, Loss 0.590086]\n","Iter 60. [Val Acc 73%, Loss 0.583704]\n","Iter 70. [Val Acc 72%, Loss 0.587972]\n","Iter 80. [Val Acc 73%, Loss 0.577430]\n","Iter 90. [Val Acc 73%, Loss 0.569298]\n","Iter 100. [Val Acc 71%, Loss 0.598593]\n","Iter 110. [Val Acc 73%, Loss 0.581486]\n","Iter 120. [Val Acc 72%, Loss 0.599007]\n","Iter 130. [Val Acc 72%, Loss 0.575619]\n","Iter 140. [Val Acc 71%, Loss 0.591104]\n","Iter 150. [Val Acc 72%, Loss 0.580432]\n","Iter 160. [Val Acc 72%, Loss 0.580477]\n","Iter 170. [Val Acc 72%, Loss 0.581285]\n","Iter 180. [Val Acc 72%, Loss 0.591494]\n","Iter 190. [Val Acc 71%, Loss 0.582207]\n","Iter 200. [Val Acc 72%, Loss 0.586299]\n","Iter 210. [Val Acc 72%, Loss 0.599889]\n","Iter 220. [Val Acc 72%, Loss 0.580448]\n","Iter 230. [Val Acc 72%, Loss 0.580767]\n","Iter 240. [Val Acc 73%, Loss 0.577602]\n","Iter 250. [Val Acc 71%, Loss 0.600360]\n","Iter 260. [Val Acc 72%, Loss 0.590913]\n","Iter 270. [Val Acc 72%, Loss 0.589140]\n","Iter 280. [Val Acc 71%, Loss 0.593471]\n","Iter 290. [Val Acc 72%, Loss 0.578113]\n","Iter 300. [Val Acc 72%, Loss 0.572821]\n","Iter 310. [Val Acc 72%, Loss 0.591579]\n","Iter 320. [Val Acc 73%, Loss 0.582045]\n","Iter 330. [Val Acc 73%, Loss 0.576280]\n","Iter 340. [Val Acc 72%, Loss 0.579601]\n","Iter 350. [Val Acc 71%, Loss 0.605440]\n","Iter 360. [Val Acc 72%, Loss 0.584608]\n","Iter 370. [Val Acc 72%, Loss 0.582736]\n","Iter 380. [Val Acc 72%, Loss 0.575237]\n","Iter 390. [Val Acc 72%, Loss 0.575676]\n","Iter 400. [Val Acc 71%, Loss 0.608766]\n","Iter 410. [Val Acc 73%, Loss 0.576581]\n","Iter 420. [Val Acc 72%, Loss 0.581078]\n","Iter 430. [Val Acc 71%, Loss 0.594714]\n","Iter 440. [Val Acc 72%, Loss 0.583900]\n","Iter 450. [Val Acc 69%, Loss 0.656565]\n","Iter 460. [Val Acc 71%, Loss 0.589886]\n","Iter 470. [Val Acc 71%, Loss 0.600099]\n","Iter 480. [Val Acc 71%, Loss 0.606505]\n","Iter 490. [Val Acc 71%, Loss 0.593084]\n","Iter 500. [Val Acc 72%, Loss 0.580861]\n","Iter 510. [Val Acc 72%, Loss 0.577896]\n","Iter 520. [Val Acc 72%, Loss 0.586358]\n","Iter 530. [Val Acc 73%, Loss 0.575788]\n","Iter 540. [Val Acc 72%, Loss 0.587966]\n","Iter 550. [Val Acc 72%, Loss 0.589470]\n","Iter 560. [Val Acc 72%, Loss 0.587236]\n","Iter 570. [Val Acc 72%, Loss 0.592914]\n","Iter 580. [Val Acc 72%, Loss 0.586646]\n","Iter 590. [Val Acc 72%, Loss 0.583243]\n","Iter 600. [Val Acc 72%, Loss 0.583611]\n","Iter 610. [Val Acc 72%, Loss 0.587218]\n","Iter 620. [Val Acc 70%, Loss 0.601483]\n","Iter 630. [Val Acc 71%, Loss 0.592276]\n","Iter 640. [Val Acc 72%, Loss 0.586130]\n","Iter 650. [Val Acc 71%, Loss 0.585050]\n","Iter 660. [Val Acc 71%, Loss 0.600727]\n","Iter 670. [Val Acc 72%, Loss 0.592090]\n","Iter 680. [Val Acc 71%, Loss 0.590011]\n","Iter 690. [Val Acc 73%, Loss 0.575244]\n","Iter 700. [Val Acc 72%, Loss 0.585503]\n","Iter 710. [Val Acc 72%, Loss 0.593004]\n","Iter 720. [Val Acc 71%, Loss 0.586811]\n","Iter 730. [Val Acc 70%, Loss 0.604519]\n","Iter 740. [Val Acc 72%, Loss 0.579787]\n","Iter 750. [Val Acc 72%, Loss 0.578156]\n","Iter 760. [Val Acc 71%, Loss 0.584004]\n","Iter 770. [Val Acc 72%, Loss 0.575973]\n","Iter 780. [Val Acc 72%, Loss 0.580794]\n","Iter 790. [Val Acc 71%, Loss 0.594087]\n","Iter 800. [Val Acc 72%, Loss 0.585583]\n","Iter 810. [Val Acc 72%, Loss 0.591155]\n","Iter 820. [Val Acc 72%, Loss 0.582567]\n","Iter 830. [Val Acc 71%, Loss 0.603483]\n","Iter 840. [Val Acc 72%, Loss 0.575608]\n","Iter 850. [Val Acc 72%, Loss 0.581330]\n","Iter 860. [Val Acc 73%, Loss 0.575886]\n","Iter 870. [Val Acc 72%, Loss 0.584772]\n","Iter 880. [Val Acc 72%, Loss 0.583165]\n","Iter 890. [Val Acc 72%, Loss 0.582396]\n","Iter 900. [Val Acc 72%, Loss 0.582213]\n","Iter 910. [Val Acc 72%, Loss 0.586524]\n","Iter 920. [Val Acc 72%, Loss 0.584792]\n","Iter 930. [Val Acc 71%, Loss 0.593124]\n","Iter 940. [Val Acc 71%, Loss 0.587475]\n","Iter 950. [Val Acc 72%, Loss 0.577246]\n","Iter 960. [Val Acc 72%, Loss 0.584198]\n","Iter 970. [Val Acc 72%, Loss 0.591193]\n","Iter 980. [Val Acc 73%, Loss 0.588141]\n","Iter 990. [Val Acc 72%, Loss 0.578465]\n","Iter 1000. [Val Acc 71%, Loss 0.600692]\n","-------------------------\n","mu= 0.5 batch_size= 50000 :\n","Iter 10. [Val Acc 72%, Loss 0.592683]\n","Iter 20. [Val Acc 72%, Loss 0.579416]\n","Iter 30. [Val Acc 72%, Loss 0.583694]\n","Iter 40. [Val Acc 73%, Loss 0.578678]\n","Iter 50. [Val Acc 73%, Loss 0.581302]\n","Iter 60. [Val Acc 72%, Loss 0.579771]\n","Iter 70. [Val Acc 71%, Loss 0.593267]\n","Iter 80. [Val Acc 72%, Loss 0.578987]\n","Iter 90. [Val Acc 72%, Loss 0.589618]\n","Iter 100. [Val Acc 71%, Loss 0.601089]\n","Iter 110. [Val Acc 69%, Loss 0.639295]\n","Iter 120. [Val Acc 72%, Loss 0.587517]\n","Iter 130. [Val Acc 72%, Loss 0.583127]\n","Iter 140. [Val Acc 72%, Loss 0.580135]\n","Iter 150. [Val Acc 73%, Loss 0.573530]\n","Iter 160. [Val Acc 71%, Loss 0.588180]\n","Iter 170. [Val Acc 72%, Loss 0.585047]\n","Iter 180. [Val Acc 72%, Loss 0.581876]\n","Iter 190. [Val Acc 72%, Loss 0.583647]\n","Iter 200. [Val Acc 72%, Loss 0.583407]\n","Iter 210. [Val Acc 72%, Loss 0.586119]\n","Iter 220. [Val Acc 72%, Loss 0.593284]\n","Iter 230. [Val Acc 73%, Loss 0.572868]\n","Iter 240. [Val Acc 72%, Loss 0.589600]\n","Iter 250. [Val Acc 72%, Loss 0.583290]\n","Iter 260. [Val Acc 72%, Loss 0.587017]\n","Iter 270. [Val Acc 73%, Loss 0.577588]\n","Iter 280. [Val Acc 72%, Loss 0.591181]\n","Iter 290. [Val Acc 73%, Loss 0.576520]\n","Iter 300. [Val Acc 72%, Loss 0.583725]\n","Iter 310. [Val Acc 73%, Loss 0.573857]\n","Iter 320. [Val Acc 72%, Loss 0.587531]\n","Iter 330. [Val Acc 73%, Loss 0.586787]\n","Iter 340. [Val Acc 70%, Loss 0.620427]\n","Iter 350. [Val Acc 70%, Loss 0.611747]\n","Iter 360. [Val Acc 73%, Loss 0.576199]\n","Iter 370. [Val Acc 73%, Loss 0.579737]\n","Iter 380. [Val Acc 73%, Loss 0.577388]\n","Iter 390. [Val Acc 73%, Loss 0.572786]\n","Iter 400. [Val Acc 71%, Loss 0.596543]\n","Iter 410. [Val Acc 72%, Loss 0.577185]\n","Iter 420. [Val Acc 71%, Loss 0.598082]\n","Iter 430. [Val Acc 73%, Loss 0.575929]\n","Iter 440. [Val Acc 72%, Loss 0.578314]\n","Iter 450. [Val Acc 72%, Loss 0.589949]\n","Iter 460. [Val Acc 72%, Loss 0.596015]\n","Iter 470. [Val Acc 72%, Loss 0.589569]\n","Iter 480. [Val Acc 72%, Loss 0.587915]\n","Iter 490. [Val Acc 71%, Loss 0.597030]\n","Iter 500. [Val Acc 72%, Loss 0.579541]\n","Iter 510. [Val Acc 72%, Loss 0.584864]\n","Iter 520. [Val Acc 72%, Loss 0.582931]\n","Iter 530. [Val Acc 72%, Loss 0.590454]\n","Iter 540. [Val Acc 71%, Loss 0.585117]\n","Iter 550. [Val Acc 72%, Loss 0.592542]\n","Iter 560. [Val Acc 72%, Loss 0.587635]\n","Iter 570. [Val Acc 71%, Loss 0.603857]\n","Iter 580. [Val Acc 72%, Loss 0.584066]\n","Iter 590. [Val Acc 73%, Loss 0.578845]\n","Iter 600. [Val Acc 72%, Loss 0.594258]\n","Iter 610. [Val Acc 72%, Loss 0.590250]\n","Iter 620. [Val Acc 72%, Loss 0.592393]\n","Iter 630. [Val Acc 72%, Loss 0.582076]\n","Iter 640. [Val Acc 71%, Loss 0.588331]\n","Iter 650. [Val Acc 71%, Loss 0.601237]\n","Iter 660. [Val Acc 71%, Loss 0.601165]\n","Iter 670. [Val Acc 71%, Loss 0.593265]\n","Iter 680. [Val Acc 71%, Loss 0.610716]\n","Iter 690. [Val Acc 72%, Loss 0.588713]\n","Iter 700. [Val Acc 73%, Loss 0.575681]\n","Iter 710. [Val Acc 72%, Loss 0.579087]\n","Iter 720. [Val Acc 73%, Loss 0.575245]\n","Iter 730. [Val Acc 72%, Loss 0.584133]\n","Iter 740. [Val Acc 72%, Loss 0.580486]\n","Iter 750. [Val Acc 73%, Loss 0.576796]\n","Iter 760. [Val Acc 71%, Loss 0.589501]\n","Iter 770. [Val Acc 72%, Loss 0.579829]\n","Iter 780. [Val Acc 72%, Loss 0.585343]\n","Iter 790. [Val Acc 72%, Loss 0.580408]\n","Iter 800. [Val Acc 73%, Loss 0.571175]\n","Iter 810. [Val Acc 72%, Loss 0.580470]\n","Iter 820. [Val Acc 72%, Loss 0.583257]\n","Iter 830. [Val Acc 72%, Loss 0.578053]\n","Iter 840. [Val Acc 73%, Loss 0.576516]\n","Iter 850. [Val Acc 72%, Loss 0.572452]\n","Iter 860. [Val Acc 72%, Loss 0.577578]\n","Iter 870. [Val Acc 72%, Loss 0.582210]\n","Iter 880. [Val Acc 72%, Loss 0.592086]\n","Iter 890. [Val Acc 73%, Loss 0.576531]\n","Iter 900. [Val Acc 73%, Loss 0.574552]\n","Iter 910. [Val Acc 72%, Loss 0.585882]\n","Iter 920. [Val Acc 72%, Loss 0.583272]\n","Iter 930. [Val Acc 72%, Loss 0.575140]\n","Iter 940. [Val Acc 72%, Loss 0.585038]\n","Iter 950. [Val Acc 73%, Loss 0.580774]\n","Iter 960. [Val Acc 71%, Loss 0.593172]\n","Iter 970. [Val Acc 71%, Loss 0.590174]\n","Iter 980. [Val Acc 72%, Loss 0.588020]\n","Iter 990. [Val Acc 72%, Loss 0.573353]\n","Iter 1000. [Val Acc 72%, Loss 0.583535]\n","-------------------------\n","mu= 1 batch_size= 1 :\n","Iter 10. [Val Acc 65%, Loss 0.835307]\n","Iter 20. [Val Acc 70%, Loss 0.624538]\n","Iter 30. [Val Acc 70%, Loss 0.648395]\n","Iter 40. [Val Acc 71%, Loss 0.611388]\n","Iter 50. [Val Acc 71%, Loss 0.606219]\n","Iter 60. [Val Acc 71%, Loss 0.622821]\n","Iter 70. [Val Acc 71%, Loss 0.606585]\n","Iter 80. [Val Acc 70%, Loss 0.636981]\n","Iter 90. [Val Acc 71%, Loss 0.607346]\n","Iter 100. [Val Acc 68%, Loss 0.674119]\n","Iter 110. [Val Acc 71%, Loss 0.631252]\n","Iter 120. [Val Acc 71%, Loss 0.606393]\n","Iter 130. [Val Acc 69%, Loss 0.632669]\n","Iter 140. [Val Acc 70%, Loss 0.626606]\n","Iter 150. [Val Acc 70%, Loss 0.633659]\n","Iter 160. [Val Acc 66%, Loss 0.704981]\n","Iter 170. [Val Acc 69%, Loss 0.640457]\n","Iter 180. [Val Acc 70%, Loss 0.638301]\n","Iter 190. [Val Acc 68%, Loss 0.678143]\n","Iter 200. [Val Acc 70%, Loss 0.635142]\n","Iter 210. [Val Acc 68%, Loss 0.664631]\n","Iter 220. [Val Acc 70%, Loss 0.632760]\n","Iter 230. [Val Acc 71%, Loss 0.621955]\n","Iter 240. [Val Acc 71%, Loss 0.628745]\n","Iter 250. [Val Acc 71%, Loss 0.618109]\n","Iter 260. [Val Acc 70%, Loss 0.646655]\n","Iter 270. [Val Acc 72%, Loss 0.611295]\n","Iter 280. [Val Acc 70%, Loss 0.642134]\n","Iter 290. [Val Acc 72%, Loss 0.603068]\n","Iter 300. [Val Acc 71%, Loss 0.612133]\n","Iter 310. [Val Acc 71%, Loss 0.634346]\n","Iter 320. [Val Acc 71%, Loss 0.627186]\n","Iter 330. [Val Acc 72%, Loss 0.599014]\n","Iter 340. [Val Acc 72%, Loss 0.599767]\n","Iter 350. [Val Acc 71%, Loss 0.620333]\n","Iter 360. [Val Acc 70%, Loss 0.640612]\n","Iter 370. [Val Acc 70%, Loss 0.632140]\n","Iter 380. [Val Acc 71%, Loss 0.605680]\n","Iter 390. [Val Acc 66%, Loss 0.692406]\n","Iter 400. [Val Acc 71%, Loss 0.618953]\n","Iter 410. [Val Acc 70%, Loss 0.631884]\n","Iter 420. [Val Acc 71%, Loss 0.610257]\n","Iter 430. [Val Acc 71%, Loss 0.615260]\n","Iter 440. [Val Acc 68%, Loss 0.674961]\n","Iter 450. [Val Acc 70%, Loss 0.636760]\n","Iter 460. [Val Acc 71%, Loss 0.622234]\n","Iter 470. [Val Acc 72%, Loss 0.614751]\n","Iter 480. [Val Acc 68%, Loss 0.679807]\n","Iter 490. [Val Acc 67%, Loss 0.676813]\n","Iter 500. [Val Acc 70%, Loss 0.633809]\n","Iter 510. [Val Acc 70%, Loss 0.634957]\n","Iter 520. [Val Acc 70%, Loss 0.677149]\n","Iter 530. [Val Acc 70%, Loss 0.632326]\n","Iter 540. [Val Acc 72%, Loss 0.598494]\n","Iter 550. [Val Acc 68%, Loss 0.649885]\n","Iter 560. [Val Acc 65%, Loss 0.747819]\n","Iter 570. [Val Acc 71%, Loss 0.632694]\n","Iter 580. [Val Acc 70%, Loss 0.623372]\n","Iter 590. [Val Acc 70%, Loss 0.664098]\n","Iter 600. [Val Acc 67%, Loss 0.690233]\n","Iter 610. [Val Acc 70%, Loss 0.650998]\n","Iter 620. [Val Acc 71%, Loss 0.624001]\n","Iter 630. [Val Acc 71%, Loss 0.615079]\n","Iter 640. [Val Acc 70%, Loss 0.623445]\n","Iter 650. [Val Acc 68%, Loss 0.666189]\n","Iter 660. [Val Acc 69%, Loss 0.665159]\n","Iter 670. [Val Acc 71%, Loss 0.609871]\n","Iter 680. [Val Acc 71%, Loss 0.627761]\n","Iter 690. [Val Acc 68%, Loss 0.768204]\n","Iter 700. [Val Acc 67%, Loss 0.682128]\n","Iter 710. [Val Acc 67%, Loss 0.711350]\n","Iter 720. [Val Acc 71%, Loss 0.615439]\n","Iter 730. [Val Acc 70%, Loss 0.642120]\n","Iter 740. [Val Acc 69%, Loss 0.666598]\n","Iter 750. [Val Acc 71%, Loss 0.626535]\n","Iter 760. [Val Acc 71%, Loss 0.621660]\n","Iter 770. [Val Acc 70%, Loss 0.640999]\n","Iter 780. [Val Acc 72%, Loss 0.608338]\n","Iter 790. [Val Acc 71%, Loss 0.633275]\n","Iter 800. [Val Acc 70%, Loss 0.626994]\n","Iter 810. [Val Acc 70%, Loss 0.619370]\n","Iter 820. [Val Acc 70%, Loss 0.632602]\n","Iter 830. [Val Acc 70%, Loss 0.632094]\n","Iter 840. [Val Acc 69%, Loss 0.631691]\n","Iter 850. [Val Acc 68%, Loss 0.653616]\n","Iter 860. [Val Acc 71%, Loss 0.644223]\n","Iter 870. [Val Acc 69%, Loss 0.627117]\n","Iter 880. [Val Acc 68%, Loss 0.663094]\n","Iter 890. [Val Acc 70%, Loss 0.634297]\n","Iter 900. [Val Acc 71%, Loss 0.625664]\n","Iter 910. [Val Acc 71%, Loss 0.637111]\n","Iter 920. [Val Acc 71%, Loss 0.627618]\n","Iter 930. [Val Acc 72%, Loss 0.614587]\n","Iter 940. [Val Acc 69%, Loss 0.648853]\n","Iter 950. [Val Acc 71%, Loss 0.610537]\n","Iter 960. [Val Acc 70%, Loss 0.645824]\n","Iter 970. [Val Acc 69%, Loss 0.663227]\n","Iter 980. [Val Acc 71%, Loss 0.609769]\n","Iter 990. [Val Acc 71%, Loss 0.609153]\n","Iter 1000. [Val Acc 71%, Loss 0.616064]\n","-------------------------\n","mu= 1 batch_size= 64 :\n","Iter 10. [Val Acc 70%, Loss 0.629198]\n","Iter 20. [Val Acc 66%, Loss 0.695369]\n","Iter 30. [Val Acc 71%, Loss 0.607919]\n","Iter 40. [Val Acc 69%, Loss 0.638659]\n","Iter 50. [Val Acc 67%, Loss 0.688053]\n","Iter 60. [Val Acc 68%, Loss 0.693892]\n","Iter 70. [Val Acc 71%, Loss 0.615990]\n","Iter 80. [Val Acc 72%, Loss 0.599219]\n","Iter 90. [Val Acc 71%, Loss 0.629675]\n","Iter 100. [Val Acc 71%, Loss 0.622717]\n","Iter 110. [Val Acc 69%, Loss 0.641184]\n","Iter 120. [Val Acc 71%, Loss 0.611580]\n","Iter 130. [Val Acc 69%, Loss 0.649739]\n","Iter 140. [Val Acc 70%, Loss 0.634307]\n","Iter 150. [Val Acc 69%, Loss 0.632484]\n","Iter 160. [Val Acc 71%, Loss 0.613134]\n","Iter 170. [Val Acc 69%, Loss 0.652090]\n","Iter 180. [Val Acc 67%, Loss 0.734856]\n","Iter 190. [Val Acc 71%, Loss 0.612898]\n","Iter 200. [Val Acc 70%, Loss 0.626283]\n","Iter 210. [Val Acc 70%, Loss 0.632082]\n","Iter 220. [Val Acc 72%, Loss 0.616310]\n","Iter 230. [Val Acc 71%, Loss 0.612020]\n","Iter 240. [Val Acc 68%, Loss 0.669219]\n","Iter 250. [Val Acc 71%, Loss 0.617777]\n","Iter 260. [Val Acc 64%, Loss 0.754732]\n","Iter 270. [Val Acc 70%, Loss 0.636857]\n","Iter 280. [Val Acc 71%, Loss 0.623842]\n","Iter 290. [Val Acc 72%, Loss 0.612712]\n","Iter 300. [Val Acc 67%, Loss 0.708748]\n","Iter 310. [Val Acc 71%, Loss 0.608174]\n","Iter 320. [Val Acc 65%, Loss 0.778001]\n","Iter 330. [Val Acc 71%, Loss 0.620782]\n","Iter 340. [Val Acc 71%, Loss 0.632840]\n","Iter 350. [Val Acc 71%, Loss 0.619756]\n","Iter 360. [Val Acc 71%, Loss 0.621645]\n","Iter 370. [Val Acc 72%, Loss 0.609539]\n","Iter 380. [Val Acc 70%, Loss 0.625794]\n","Iter 390. [Val Acc 71%, Loss 0.624004]\n","Iter 400. [Val Acc 71%, Loss 0.606671]\n","Iter 410. [Val Acc 70%, Loss 0.625599]\n","Iter 420. [Val Acc 70%, Loss 0.618382]\n","Iter 430. [Val Acc 71%, Loss 0.609106]\n","Iter 440. [Val Acc 70%, Loss 0.623340]\n","Iter 450. [Val Acc 71%, Loss 0.597850]\n","Iter 460. [Val Acc 70%, Loss 0.640124]\n","Iter 470. [Val Acc 70%, Loss 0.632079]\n","Iter 480. [Val Acc 70%, Loss 0.630415]\n","Iter 490. [Val Acc 70%, Loss 0.635406]\n","Iter 500. [Val Acc 69%, Loss 0.649818]\n","Iter 510. [Val Acc 71%, Loss 0.609786]\n","Iter 520. [Val Acc 71%, Loss 0.612525]\n","Iter 530. [Val Acc 70%, Loss 0.637019]\n","Iter 540. [Val Acc 71%, Loss 0.611348]\n","Iter 550. [Val Acc 70%, Loss 0.609801]\n","Iter 560. [Val Acc 66%, Loss 0.740150]\n","Iter 570. [Val Acc 71%, Loss 0.618078]\n","Iter 580. [Val Acc 71%, Loss 0.629268]\n","Iter 590. [Val Acc 71%, Loss 0.600015]\n","Iter 600. [Val Acc 69%, Loss 0.683220]\n","Iter 610. [Val Acc 68%, Loss 0.681337]\n","Iter 620. [Val Acc 70%, Loss 0.619363]\n","Iter 630. [Val Acc 70%, Loss 0.626569]\n","Iter 640. [Val Acc 70%, Loss 0.627416]\n","Iter 650. [Val Acc 70%, Loss 0.630546]\n","Iter 660. [Val Acc 66%, Loss 0.734672]\n","Iter 670. [Val Acc 66%, Loss 0.792481]\n","Iter 680. [Val Acc 71%, Loss 0.614539]\n","Iter 690. [Val Acc 68%, Loss 0.660943]\n","Iter 700. [Val Acc 71%, Loss 0.625004]\n","Iter 710. [Val Acc 71%, Loss 0.633742]\n","Iter 720. [Val Acc 71%, Loss 0.611698]\n","Iter 730. [Val Acc 70%, Loss 0.623131]\n","Iter 740. [Val Acc 71%, Loss 0.616610]\n","Iter 750. [Val Acc 70%, Loss 0.624855]\n","Iter 760. [Val Acc 66%, Loss 0.702875]\n","Iter 770. [Val Acc 68%, Loss 0.676050]\n","Iter 780. [Val Acc 71%, Loss 0.624335]\n","Iter 790. [Val Acc 70%, Loss 0.641198]\n","Iter 800. [Val Acc 71%, Loss 0.619213]\n","Iter 810. [Val Acc 67%, Loss 0.690588]\n","Iter 820. [Val Acc 72%, Loss 0.600636]\n","Iter 830. [Val Acc 72%, Loss 0.604822]\n","Iter 840. [Val Acc 70%, Loss 0.630850]\n","Iter 850. [Val Acc 70%, Loss 0.642288]\n","Iter 860. [Val Acc 68%, Loss 0.695363]\n","Iter 870. [Val Acc 69%, Loss 0.651445]\n","Iter 880. [Val Acc 71%, Loss 0.618260]\n","Iter 890. [Val Acc 67%, Loss 0.701989]\n","Iter 900. [Val Acc 72%, Loss 0.610612]\n","Iter 910. [Val Acc 72%, Loss 0.616255]\n","Iter 920. [Val Acc 70%, Loss 0.635534]\n","Iter 930. [Val Acc 70%, Loss 0.633450]\n","Iter 940. [Val Acc 68%, Loss 0.706726]\n","Iter 950. [Val Acc 70%, Loss 0.636933]\n","Iter 960. [Val Acc 62%, Loss 0.797707]\n","Iter 970. [Val Acc 72%, Loss 0.625017]\n","Iter 980. [Val Acc 70%, Loss 0.638366]\n","Iter 990. [Val Acc 70%, Loss 0.629172]\n","Iter 1000. [Val Acc 71%, Loss 0.604121]\n","-------------------------\n","mu= 1 batch_size= 1024 :\n","Iter 10. [Val Acc 68%, Loss 0.649252]\n","Iter 20. [Val Acc 71%, Loss 0.611786]\n","Iter 30. [Val Acc 71%, Loss 0.610930]\n","Iter 40. [Val Acc 70%, Loss 0.627276]\n","Iter 50. [Val Acc 65%, Loss 0.758272]\n","Iter 60. [Val Acc 71%, Loss 0.612275]\n","Iter 70. [Val Acc 70%, Loss 0.626646]\n","Iter 80. [Val Acc 70%, Loss 0.612988]\n","Iter 90. [Val Acc 71%, Loss 0.604877]\n","Iter 100. [Val Acc 69%, Loss 0.645123]\n","Iter 110. [Val Acc 70%, Loss 0.613425]\n","Iter 120. [Val Acc 71%, Loss 0.613258]\n","Iter 130. [Val Acc 70%, Loss 0.612942]\n","Iter 140. [Val Acc 67%, Loss 0.662790]\n","Iter 150. [Val Acc 68%, Loss 0.672127]\n","Iter 160. [Val Acc 70%, Loss 0.629385]\n","Iter 170. [Val Acc 71%, Loss 0.606426]\n","Iter 180. [Val Acc 70%, Loss 0.630142]\n","Iter 190. [Val Acc 71%, Loss 0.621830]\n","Iter 200. [Val Acc 71%, Loss 0.617339]\n","Iter 210. [Val Acc 69%, Loss 0.643586]\n","Iter 220. [Val Acc 72%, Loss 0.598228]\n","Iter 230. [Val Acc 69%, Loss 0.648760]\n","Iter 240. [Val Acc 72%, Loss 0.609973]\n","Iter 250. [Val Acc 68%, Loss 0.663813]\n","Iter 260. [Val Acc 71%, Loss 0.627936]\n","Iter 270. [Val Acc 71%, Loss 0.603384]\n","Iter 280. [Val Acc 70%, Loss 0.624479]\n","Iter 290. [Val Acc 69%, Loss 0.659565]\n","Iter 300. [Val Acc 70%, Loss 0.623267]\n","Iter 310. [Val Acc 69%, Loss 0.682700]\n","Iter 320. [Val Acc 71%, Loss 0.612237]\n","Iter 330. [Val Acc 63%, Loss 0.782602]\n","Iter 340. [Val Acc 70%, Loss 0.622531]\n","Iter 350. [Val Acc 66%, Loss 0.769405]\n","Iter 360. [Val Acc 70%, Loss 0.641148]\n","Iter 370. [Val Acc 71%, Loss 0.626583]\n","Iter 380. [Val Acc 71%, Loss 0.612345]\n","Iter 390. [Val Acc 70%, Loss 0.630485]\n","Iter 400. [Val Acc 72%, Loss 0.603227]\n","Iter 410. [Val Acc 70%, Loss 0.643332]\n","Iter 420. [Val Acc 69%, Loss 0.701056]\n","Iter 430. [Val Acc 70%, Loss 0.639313]\n","Iter 440. [Val Acc 70%, Loss 0.675662]\n","Iter 450. [Val Acc 71%, Loss 0.611969]\n","Iter 460. [Val Acc 71%, Loss 0.630756]\n","Iter 470. [Val Acc 68%, Loss 0.702735]\n","Iter 480. [Val Acc 71%, Loss 0.608782]\n","Iter 490. [Val Acc 72%, Loss 0.613070]\n","Iter 500. [Val Acc 69%, Loss 0.652675]\n","Iter 510. [Val Acc 70%, Loss 0.621619]\n","Iter 520. [Val Acc 70%, Loss 0.645772]\n","Iter 530. [Val Acc 70%, Loss 0.617525]\n","Iter 540. [Val Acc 70%, Loss 0.636999]\n","Iter 550. [Val Acc 67%, Loss 0.705442]\n","Iter 560. [Val Acc 71%, Loss 0.609372]\n","Iter 570. [Val Acc 71%, Loss 0.624787]\n","Iter 580. [Val Acc 63%, Loss 0.909281]\n","Iter 590. [Val Acc 69%, Loss 0.660922]\n","Iter 600. [Val Acc 70%, Loss 0.629545]\n","Iter 610. [Val Acc 70%, Loss 0.633079]\n","Iter 620. [Val Acc 71%, Loss 0.622469]\n","Iter 630. [Val Acc 70%, Loss 0.621392]\n","Iter 640. [Val Acc 70%, Loss 0.630855]\n","Iter 650. [Val Acc 71%, Loss 0.605529]\n","Iter 660. [Val Acc 71%, Loss 0.611042]\n","Iter 670. [Val Acc 71%, Loss 0.610900]\n","Iter 680. [Val Acc 69%, Loss 0.674227]\n","Iter 690. [Val Acc 72%, Loss 0.603755]\n","Iter 700. [Val Acc 70%, Loss 0.643773]\n","Iter 710. [Val Acc 70%, Loss 0.640813]\n","Iter 720. [Val Acc 71%, Loss 0.632316]\n","Iter 730. [Val Acc 70%, Loss 0.630254]\n","Iter 740. [Val Acc 70%, Loss 0.649163]\n","Iter 750. [Val Acc 71%, Loss 0.649936]\n","Iter 760. [Val Acc 70%, Loss 0.640333]\n","Iter 770. [Val Acc 70%, Loss 0.636234]\n","Iter 780. [Val Acc 69%, Loss 0.672229]\n","Iter 790. [Val Acc 70%, Loss 0.638844]\n","Iter 800. [Val Acc 71%, Loss 0.634575]\n","Iter 810. [Val Acc 71%, Loss 0.601655]\n","Iter 820. [Val Acc 70%, Loss 0.632649]\n","Iter 830. [Val Acc 72%, Loss 0.606974]\n","Iter 840. [Val Acc 71%, Loss 0.628278]\n","Iter 850. [Val Acc 67%, Loss 0.736416]\n","Iter 860. [Val Acc 71%, Loss 0.610310]\n","Iter 870. [Val Acc 70%, Loss 0.631057]\n","Iter 880. [Val Acc 66%, Loss 0.721716]\n","Iter 890. [Val Acc 70%, Loss 0.629276]\n","Iter 900. [Val Acc 71%, Loss 0.611361]\n","Iter 910. [Val Acc 71%, Loss 0.623801]\n","Iter 920. [Val Acc 70%, Loss 0.612657]\n","Iter 930. [Val Acc 71%, Loss 0.617298]\n","Iter 940. [Val Acc 70%, Loss 0.633442]\n","Iter 950. [Val Acc 68%, Loss 0.693318]\n","Iter 960. [Val Acc 67%, Loss 0.728475]\n","Iter 970. [Val Acc 71%, Loss 0.628805]\n","Iter 980. [Val Acc 72%, Loss 0.613488]\n","Iter 990. [Val Acc 70%, Loss 0.662641]\n","Iter 1000. [Val Acc 70%, Loss 0.650241]\n","-------------------------\n","mu= 1 batch_size= 50000 :\n","Iter 10. [Val Acc 71%, Loss 0.626459]\n","Iter 20. [Val Acc 70%, Loss 0.663622]\n","Iter 30. [Val Acc 70%, Loss 0.630805]\n","Iter 40. [Val Acc 70%, Loss 0.624077]\n","Iter 50. [Val Acc 70%, Loss 0.627647]\n","Iter 60. [Val Acc 71%, Loss 0.636453]\n","Iter 70. [Val Acc 71%, Loss 0.615262]\n","Iter 80. [Val Acc 70%, Loss 0.622871]\n","Iter 90. [Val Acc 69%, Loss 0.647021]\n","Iter 100. [Val Acc 63%, Loss 0.914931]\n","Iter 110. [Val Acc 67%, Loss 0.679086]\n","Iter 120. [Val Acc 69%, Loss 0.648313]\n","Iter 130. [Val Acc 69%, Loss 0.663981]\n","Iter 140. [Val Acc 69%, Loss 0.696844]\n","Iter 150. [Val Acc 71%, Loss 0.624825]\n","Iter 160. [Val Acc 71%, Loss 0.617030]\n","Iter 170. [Val Acc 70%, Loss 0.633178]\n","Iter 180. [Val Acc 69%, Loss 0.628524]\n","Iter 190. [Val Acc 70%, Loss 0.649084]\n","Iter 200. [Val Acc 70%, Loss 0.623422]\n","Iter 210. [Val Acc 71%, Loss 0.609200]\n","Iter 220. [Val Acc 70%, Loss 0.627624]\n","Iter 230. [Val Acc 70%, Loss 0.664023]\n","Iter 240. [Val Acc 71%, Loss 0.620402]\n","Iter 250. [Val Acc 71%, Loss 0.629401]\n","Iter 260. [Val Acc 70%, Loss 0.624368]\n","Iter 270. [Val Acc 71%, Loss 0.628890]\n","Iter 280. [Val Acc 71%, Loss 0.610450]\n","Iter 290. [Val Acc 70%, Loss 0.627236]\n","Iter 300. [Val Acc 69%, Loss 0.651140]\n","Iter 310. [Val Acc 68%, Loss 0.664734]\n","Iter 320. [Val Acc 70%, Loss 0.618743]\n","Iter 330. [Val Acc 69%, Loss 0.637422]\n","Iter 340. [Val Acc 66%, Loss 0.753548]\n","Iter 350. [Val Acc 69%, Loss 0.666854]\n","Iter 360. [Val Acc 71%, Loss 0.634307]\n","Iter 370. [Val Acc 71%, Loss 0.614273]\n","Iter 380. [Val Acc 71%, Loss 0.610944]\n","Iter 390. [Val Acc 71%, Loss 0.609151]\n","Iter 400. [Val Acc 69%, Loss 0.647926]\n","Iter 410. [Val Acc 70%, Loss 0.637192]\n","Iter 420. [Val Acc 70%, Loss 0.625743]\n","Iter 430. [Val Acc 69%, Loss 0.642357]\n","Iter 440. [Val Acc 71%, Loss 0.619092]\n","Iter 450. [Val Acc 71%, Loss 0.624259]\n","Iter 460. [Val Acc 72%, Loss 0.599912]\n","Iter 470. [Val Acc 69%, Loss 0.647424]\n","Iter 480. [Val Acc 70%, Loss 0.638805]\n","Iter 490. [Val Acc 72%, Loss 0.597873]\n","Iter 500. [Val Acc 71%, Loss 0.633345]\n","Iter 510. [Val Acc 72%, Loss 0.613755]\n","Iter 520. [Val Acc 69%, Loss 0.635977]\n","Iter 530. [Val Acc 69%, Loss 0.668317]\n","Iter 540. [Val Acc 71%, Loss 0.619044]\n","Iter 550. [Val Acc 71%, Loss 0.609404]\n","Iter 560. [Val Acc 70%, Loss 0.631507]\n","Iter 570. [Val Acc 71%, Loss 0.613583]\n","Iter 580. [Val Acc 71%, Loss 0.611281]\n","Iter 590. [Val Acc 70%, Loss 0.636085]\n","Iter 600. [Val Acc 69%, Loss 0.644006]\n","Iter 610. [Val Acc 70%, Loss 0.650189]\n","Iter 620. [Val Acc 71%, Loss 0.647499]\n","Iter 630. [Val Acc 70%, Loss 0.660596]\n","Iter 640. [Val Acc 72%, Loss 0.622102]\n","Iter 650. [Val Acc 71%, Loss 0.607361]\n","Iter 660. [Val Acc 69%, Loss 0.653456]\n","Iter 670. [Val Acc 71%, Loss 0.626598]\n","Iter 680. [Val Acc 72%, Loss 0.608227]\n","Iter 690. [Val Acc 71%, Loss 0.612732]\n","Iter 700. [Val Acc 71%, Loss 0.621411]\n","Iter 710. [Val Acc 70%, Loss 0.645016]\n","Iter 720. [Val Acc 70%, Loss 0.636396]\n","Iter 730. [Val Acc 71%, Loss 0.635381]\n","Iter 740. [Val Acc 69%, Loss 0.656871]\n","Iter 750. [Val Acc 71%, Loss 0.616633]\n","Iter 760. [Val Acc 71%, Loss 0.627937]\n","Iter 770. [Val Acc 70%, Loss 0.628213]\n","Iter 780. [Val Acc 72%, Loss 0.610602]\n","Iter 790. [Val Acc 69%, Loss 0.667862]\n","Iter 800. [Val Acc 70%, Loss 0.633817]\n","Iter 810. [Val Acc 70%, Loss 0.640995]\n","Iter 820. [Val Acc 71%, Loss 0.607677]\n","Iter 830. [Val Acc 71%, Loss 0.621537]\n","Iter 840. [Val Acc 69%, Loss 0.637984]\n","Iter 850. [Val Acc 71%, Loss 0.645876]\n","Iter 860. [Val Acc 66%, Loss 0.776783]\n","Iter 870. [Val Acc 70%, Loss 0.620939]\n","Iter 880. [Val Acc 71%, Loss 0.621915]\n","Iter 890. [Val Acc 71%, Loss 0.614946]\n","Iter 900. [Val Acc 70%, Loss 0.652420]\n","Iter 910. [Val Acc 69%, Loss 0.652943]\n","Iter 920. [Val Acc 70%, Loss 0.617375]\n","Iter 930. [Val Acc 69%, Loss 0.675866]\n","Iter 940. [Val Acc 70%, Loss 0.624393]\n","Iter 950. [Val Acc 68%, Loss 0.700144]\n","Iter 960. [Val Acc 67%, Loss 0.682843]\n","Iter 970. [Val Acc 70%, Loss 0.628097]\n","Iter 980. [Val Acc 70%, Loss 0.639787]\n","Iter 990. [Val Acc 72%, Loss 0.619058]\n","Iter 1000. [Val Acc 71%, Loss 0.630849]\n","-------------------------\n","optimal mu= 0.1 optimal batch_size= 50000\n"]}]},{"cell_type":"markdown","metadata":{"id":"J8ij82PbM_gf"},"source":["We found the optimal weights and biases by scanning over a range of different learning rates and batch sizes for 1000 iterations each and then picking the weights and biases in which the loss over the validation set was minimal.\n","The best weights and biases were observed when $\\mu=0.1$ and batch_size = 50000 (the size of the training data set). The best results are achieved when the batch size is maximal. The reason is that the number of iterations is the number of update steps. Because this number is constant (1000), we have the same number of updates independently of the batch size. Thus, when the batch size is large, as we learned in the lectures, we get a better approximation of the true gradient, and hence the results are better.\n","It is important to mention that we should not conclude that the best choice is always picking the largest batch size. The reason is that if we measure the validation loss per epoch (for a fixed number of epochs), the number of update steps with a large batch size will be smaller than the number of updates when the batch size is smaller which may lead to worse performance. Furthermore, a smaller batch size can assist the algorithm in avoiding it from stuck in a local minimum or a saddle point due to the noisier gradient."]},{"cell_type":"code","metadata":{"id":"z_HNFu9_U5sq","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1636560229909,"user_tz":-120,"elapsed":91831,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"080625e5-d416-453a-9f2a-dd58ec1cff69"},"source":["\n","\n","# Write your code here\n","mu = 0.1\n","batch_sizes = [1,50000]\n","Iterations = 1000\n","plt.figure()\n","MC_SIZE = 3\n","losses_vec = np.zeros((MC_SIZE, len(batch_sizes), int(Iterations/10)))\n","for mc_indx in range(MC_SIZE):\n","    w0 = np.random.randn(90)\n","    b0 = np.random.randn(1)[0]\n","    print(\"monte_carlo:\", mc_indx)\n","    for curr_idx, minibatch_size in enumerate(batch_sizes):\n","      print(\"mu=\",mu, 'batch_size=', minibatch_size, ':')\n","      w,b,losses=run_gradient_descent(w0,b0,mu, max_iters=Iterations)\n","      losses_vec[mc_indx][curr_idx] = losses\n","      print(\"-------------------------\")\n","\n","\n","mean = np.mean(losses_vec,axis=0)\n","std = np.std(losses_vec,axis=0)\n","for curr_idx, minibatch_size in enumerate(batch_sizes):\n","  plt.semilogy(range(0,Iterations,10),mean[curr_idx], label=f'{minibatch_size}')\n","  y_neg_error = mean[curr_idx]-std[curr_idx]\n","  y_pos_error = mean[curr_idx]+std[curr_idx]\n","  plt.fill_between(range(0,Iterations,10), y_neg_error, y_pos_error, alpha=0.3)\n","plt.legend(loc='best')\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Loss\")\n","plt.grid()\n","plt.title(\"Batch size convergance properties\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["monte_carlo: 0\n","mu= 0.1 batch_size= 1 :\n","Iter 10. [Val Acc 50%, Loss 2.988975]\n","Iter 20. [Val Acc 50%, Loss 2.667662]\n","Iter 30. [Val Acc 51%, Loss 2.407711]\n","Iter 40. [Val Acc 52%, Loss 2.213487]\n","Iter 50. [Val Acc 53%, Loss 2.034886]\n","Iter 60. [Val Acc 54%, Loss 1.918314]\n","Iter 70. [Val Acc 55%, Loss 1.783832]\n","Iter 80. [Val Acc 55%, Loss 1.679106]\n","Iter 90. [Val Acc 56%, Loss 1.575458]\n","Iter 100. [Val Acc 57%, Loss 1.505344]\n","Iter 110. [Val Acc 57%, Loss 1.429909]\n","Iter 120. [Val Acc 58%, Loss 1.359682]\n","Iter 130. [Val Acc 59%, Loss 1.299835]\n","Iter 140. [Val Acc 59%, Loss 1.230887]\n","Iter 150. [Val Acc 60%, Loss 1.174670]\n","Iter 160. [Val Acc 61%, Loss 1.117459]\n","Iter 170. [Val Acc 61%, Loss 1.073838]\n","Iter 180. [Val Acc 62%, Loss 1.035060]\n","Iter 190. [Val Acc 62%, Loss 1.000929]\n","Iter 200. [Val Acc 63%, Loss 0.972118]\n","Iter 210. [Val Acc 63%, Loss 0.943865]\n","Iter 220. [Val Acc 63%, Loss 0.914478]\n","Iter 230. [Val Acc 64%, Loss 0.887211]\n","Iter 240. [Val Acc 64%, Loss 0.860926]\n","Iter 250. [Val Acc 64%, Loss 0.843443]\n","Iter 260. [Val Acc 65%, Loss 0.830494]\n","Iter 270. [Val Acc 65%, Loss 0.802618]\n","Iter 280. [Val Acc 66%, Loss 0.792238]\n","Iter 290. [Val Acc 66%, Loss 0.772187]\n","Iter 300. [Val Acc 66%, Loss 0.760490]\n","Iter 310. [Val Acc 67%, Loss 0.752141]\n","Iter 320. [Val Acc 67%, Loss 0.737006]\n","Iter 330. [Val Acc 67%, Loss 0.726057]\n","Iter 340. [Val Acc 68%, Loss 0.713343]\n","Iter 350. [Val Acc 68%, Loss 0.703581]\n","Iter 360. [Val Acc 68%, Loss 0.694631]\n","Iter 370. [Val Acc 68%, Loss 0.686823]\n","Iter 380. [Val Acc 69%, Loss 0.677702]\n","Iter 390. [Val Acc 69%, Loss 0.670663]\n","Iter 400. [Val Acc 69%, Loss 0.662769]\n","Iter 410. [Val Acc 69%, Loss 0.657274]\n","Iter 420. [Val Acc 69%, Loss 0.652371]\n","Iter 430. [Val Acc 69%, Loss 0.646517]\n","Iter 440. [Val Acc 69%, Loss 0.641465]\n","Iter 450. [Val Acc 69%, Loss 0.637293]\n","Iter 460. [Val Acc 70%, Loss 0.630870]\n","Iter 470. [Val Acc 70%, Loss 0.627512]\n","Iter 480. [Val Acc 70%, Loss 0.626387]\n","Iter 490. [Val Acc 70%, Loss 0.622051]\n","Iter 500. [Val Acc 70%, Loss 0.626211]\n","Iter 510. [Val Acc 70%, Loss 0.615420]\n","Iter 520. [Val Acc 71%, Loss 0.611164]\n","Iter 530. [Val Acc 71%, Loss 0.608687]\n","Iter 540. [Val Acc 71%, Loss 0.610778]\n","Iter 550. [Val Acc 71%, Loss 0.603462]\n","Iter 560. [Val Acc 71%, Loss 0.600741]\n","Iter 570. [Val Acc 71%, Loss 0.597940]\n","Iter 580. [Val Acc 71%, Loss 0.595799]\n","Iter 590. [Val Acc 71%, Loss 0.594981]\n","Iter 600. [Val Acc 71%, Loss 0.594292]\n","Iter 610. [Val Acc 71%, Loss 0.591960]\n","Iter 620. [Val Acc 72%, Loss 0.589173]\n","Iter 630. [Val Acc 72%, Loss 0.588827]\n","Iter 640. [Val Acc 72%, Loss 0.586945]\n","Iter 650. [Val Acc 72%, Loss 0.585873]\n","Iter 660. [Val Acc 72%, Loss 0.584911]\n","Iter 670. [Val Acc 72%, Loss 0.581751]\n","Iter 680. [Val Acc 72%, Loss 0.583727]\n","Iter 690. [Val Acc 71%, Loss 0.584256]\n","Iter 700. [Val Acc 72%, Loss 0.581169]\n","Iter 710. [Val Acc 72%, Loss 0.579066]\n","Iter 720. [Val Acc 72%, Loss 0.577616]\n","Iter 730. [Val Acc 72%, Loss 0.578711]\n","Iter 740. [Val Acc 72%, Loss 0.576714]\n","Iter 750. [Val Acc 72%, Loss 0.576335]\n","Iter 760. [Val Acc 72%, Loss 0.575805]\n","Iter 770. [Val Acc 73%, Loss 0.573263]\n","Iter 780. [Val Acc 73%, Loss 0.572973]\n","Iter 790. [Val Acc 72%, Loss 0.573444]\n","Iter 800. [Val Acc 72%, Loss 0.573600]\n","Iter 810. [Val Acc 72%, Loss 0.574825]\n","Iter 820. [Val Acc 72%, Loss 0.573456]\n","Iter 830. [Val Acc 73%, Loss 0.571975]\n","Iter 840. [Val Acc 72%, Loss 0.571336]\n","Iter 850. [Val Acc 72%, Loss 0.573646]\n","Iter 860. [Val Acc 73%, Loss 0.570579]\n","Iter 870. [Val Acc 73%, Loss 0.570091]\n","Iter 880. [Val Acc 72%, Loss 0.572335]\n","Iter 890. [Val Acc 73%, Loss 0.571174]\n","Iter 900. [Val Acc 72%, Loss 0.572811]\n","Iter 910. [Val Acc 73%, Loss 0.572232]\n","Iter 920. [Val Acc 73%, Loss 0.570569]\n","Iter 930. [Val Acc 72%, Loss 0.569783]\n","Iter 940. [Val Acc 73%, Loss 0.569087]\n","Iter 950. [Val Acc 73%, Loss 0.568024]\n","Iter 960. [Val Acc 73%, Loss 0.567501]\n","Iter 970. [Val Acc 73%, Loss 0.567914]\n","Iter 980. [Val Acc 73%, Loss 0.566466]\n","Iter 990. [Val Acc 73%, Loss 0.567172]\n","Iter 1000. [Val Acc 73%, Loss 0.566885]\n","-------------------------\n","mu= 0.1 batch_size= 50000 :\n","Iter 10. [Val Acc 69%, Loss 0.609053]\n","Iter 20. [Val Acc 71%, Loss 0.594825]\n","Iter 30. [Val Acc 71%, Loss 0.586232]\n","Iter 40. [Val Acc 72%, Loss 0.580912]\n","Iter 50. [Val Acc 72%, Loss 0.579176]\n","Iter 60. [Val Acc 72%, Loss 0.575480]\n","Iter 70. [Val Acc 73%, Loss 0.568668]\n","Iter 80. [Val Acc 73%, Loss 0.568389]\n","Iter 90. [Val Acc 73%, Loss 0.567299]\n","Iter 100. [Val Acc 73%, Loss 0.565149]\n","Iter 110. [Val Acc 73%, Loss 0.565355]\n","Iter 120. [Val Acc 73%, Loss 0.563947]\n","Iter 130. [Val Acc 73%, Loss 0.566062]\n","Iter 140. [Val Acc 73%, Loss 0.562906]\n","Iter 150. [Val Acc 73%, Loss 0.566718]\n","Iter 160. [Val Acc 73%, Loss 0.566786]\n","Iter 170. [Val Acc 73%, Loss 0.564401]\n","Iter 180. [Val Acc 73%, Loss 0.568687]\n","Iter 190. [Val Acc 73%, Loss 0.565350]\n","Iter 200. [Val Acc 73%, Loss 0.564624]\n","Iter 210. [Val Acc 73%, Loss 0.564854]\n","Iter 220. [Val Acc 73%, Loss 0.565167]\n","Iter 230. [Val Acc 73%, Loss 0.563991]\n","Iter 240. [Val Acc 73%, Loss 0.564317]\n","Iter 250. [Val Acc 73%, Loss 0.567851]\n","Iter 260. [Val Acc 73%, Loss 0.562957]\n","Iter 270. [Val Acc 73%, Loss 0.564235]\n","Iter 280. [Val Acc 73%, Loss 0.563862]\n","Iter 290. [Val Acc 73%, Loss 0.561805]\n","Iter 300. [Val Acc 73%, Loss 0.562217]\n","Iter 310. [Val Acc 73%, Loss 0.566289]\n","Iter 320. [Val Acc 73%, Loss 0.563408]\n","Iter 330. [Val Acc 73%, Loss 0.563460]\n","Iter 340. [Val Acc 73%, Loss 0.565811]\n","Iter 350. [Val Acc 73%, Loss 0.564462]\n","Iter 360. [Val Acc 73%, Loss 0.562446]\n","Iter 370. [Val Acc 73%, Loss 0.561673]\n","Iter 380. [Val Acc 73%, Loss 0.562338]\n","Iter 390. [Val Acc 73%, Loss 0.562108]\n","Iter 400. [Val Acc 73%, Loss 0.563027]\n","Iter 410. [Val Acc 73%, Loss 0.561438]\n","Iter 420. [Val Acc 73%, Loss 0.562686]\n","Iter 430. [Val Acc 73%, Loss 0.561587]\n","Iter 440. [Val Acc 74%, Loss 0.559880]\n","Iter 450. [Val Acc 74%, Loss 0.560219]\n","Iter 460. [Val Acc 73%, Loss 0.562076]\n","Iter 470. [Val Acc 73%, Loss 0.560777]\n","Iter 480. [Val Acc 73%, Loss 0.564152]\n","Iter 490. [Val Acc 73%, Loss 0.560935]\n","Iter 500. [Val Acc 73%, Loss 0.561742]\n","Iter 510. [Val Acc 73%, Loss 0.561520]\n","Iter 520. [Val Acc 73%, Loss 0.559534]\n","Iter 530. [Val Acc 73%, Loss 0.560051]\n","Iter 540. [Val Acc 73%, Loss 0.560920]\n","Iter 550. [Val Acc 73%, Loss 0.562220]\n","Iter 560. [Val Acc 73%, Loss 0.559898]\n","Iter 570. [Val Acc 73%, Loss 0.560552]\n","Iter 580. [Val Acc 73%, Loss 0.563651]\n","Iter 590. [Val Acc 73%, Loss 0.562176]\n","Iter 600. [Val Acc 73%, Loss 0.560833]\n","Iter 610. [Val Acc 73%, Loss 0.562583]\n","Iter 620. [Val Acc 73%, Loss 0.563069]\n","Iter 630. [Val Acc 73%, Loss 0.560673]\n","Iter 640. [Val Acc 73%, Loss 0.561327]\n","Iter 650. [Val Acc 74%, Loss 0.560986]\n","Iter 660. [Val Acc 73%, Loss 0.560266]\n","Iter 670. [Val Acc 73%, Loss 0.562147]\n","Iter 680. [Val Acc 73%, Loss 0.562432]\n","Iter 690. [Val Acc 73%, Loss 0.560802]\n","Iter 700. [Val Acc 73%, Loss 0.559577]\n","Iter 710. [Val Acc 73%, Loss 0.559615]\n","Iter 720. [Val Acc 74%, Loss 0.559283]\n","Iter 730. [Val Acc 73%, Loss 0.559184]\n","Iter 740. [Val Acc 73%, Loss 0.561234]\n","Iter 750. [Val Acc 73%, Loss 0.561935]\n","Iter 760. [Val Acc 73%, Loss 0.560156]\n","Iter 770. [Val Acc 73%, Loss 0.561854]\n","Iter 780. [Val Acc 73%, Loss 0.560557]\n","Iter 790. [Val Acc 73%, Loss 0.560726]\n","Iter 800. [Val Acc 73%, Loss 0.561080]\n","Iter 810. [Val Acc 73%, Loss 0.560786]\n","Iter 820. [Val Acc 73%, Loss 0.567863]\n","Iter 830. [Val Acc 73%, Loss 0.561067]\n","Iter 840. [Val Acc 73%, Loss 0.560656]\n","Iter 850. [Val Acc 73%, Loss 0.562331]\n","Iter 860. [Val Acc 73%, Loss 0.562238]\n","Iter 870. [Val Acc 73%, Loss 0.562508]\n","Iter 880. [Val Acc 73%, Loss 0.559847]\n","Iter 890. [Val Acc 73%, Loss 0.559329]\n","Iter 900. [Val Acc 73%, Loss 0.559951]\n","Iter 910. [Val Acc 73%, Loss 0.559736]\n","Iter 920. [Val Acc 73%, Loss 0.559922]\n","Iter 930. [Val Acc 73%, Loss 0.560707]\n","Iter 940. [Val Acc 73%, Loss 0.565256]\n","Iter 950. [Val Acc 73%, Loss 0.562449]\n","Iter 960. [Val Acc 73%, Loss 0.560893]\n","Iter 970. [Val Acc 73%, Loss 0.562585]\n","Iter 980. [Val Acc 73%, Loss 0.560763]\n","Iter 990. [Val Acc 73%, Loss 0.560838]\n","Iter 1000. [Val Acc 73%, Loss 0.561143]\n","-------------------------\n","monte_carlo: 1\n","mu= 0.1 batch_size= 1 :\n","Iter 10. [Val Acc 50%, Loss 3.606403]\n","Iter 20. [Val Acc 50%, Loss 3.232711]\n","Iter 30. [Val Acc 50%, Loss 2.954263]\n","Iter 40. [Val Acc 51%, Loss 2.713782]\n","Iter 50. [Val Acc 51%, Loss 2.505076]\n","Iter 60. [Val Acc 52%, Loss 2.304634]\n","Iter 70. [Val Acc 53%, Loss 2.157649]\n","Iter 80. [Val Acc 53%, Loss 2.025197]\n","Iter 90. [Val Acc 54%, Loss 1.911352]\n","Iter 100. [Val Acc 54%, Loss 1.805560]\n","Iter 110. [Val Acc 55%, Loss 1.721055]\n","Iter 120. [Val Acc 55%, Loss 1.644382]\n","Iter 130. [Val Acc 56%, Loss 1.572820]\n","Iter 140. [Val Acc 56%, Loss 1.499186]\n","Iter 150. [Val Acc 57%, Loss 1.433355]\n","Iter 160. [Val Acc 57%, Loss 1.375669]\n","Iter 170. [Val Acc 58%, Loss 1.329589]\n","Iter 180. [Val Acc 58%, Loss 1.271582]\n","Iter 190. [Val Acc 58%, Loss 1.218902]\n","Iter 200. [Val Acc 59%, Loss 1.163627]\n","Iter 210. [Val Acc 60%, Loss 1.125665]\n","Iter 220. [Val Acc 60%, Loss 1.092377]\n","Iter 230. [Val Acc 61%, Loss 1.057642]\n","Iter 240. [Val Acc 61%, Loss 1.023549]\n","Iter 250. [Val Acc 61%, Loss 0.991729]\n","Iter 260. [Val Acc 62%, Loss 0.965199]\n","Iter 270. [Val Acc 62%, Loss 0.934834]\n","Iter 280. [Val Acc 63%, Loss 0.912913]\n","Iter 290. [Val Acc 63%, Loss 0.887457]\n","Iter 300. [Val Acc 63%, Loss 0.869442]\n","Iter 310. [Val Acc 64%, Loss 0.847111]\n","Iter 320. [Val Acc 64%, Loss 0.823289]\n","Iter 330. [Val Acc 64%, Loss 0.810874]\n","Iter 340. [Val Acc 64%, Loss 0.798560]\n","Iter 350. [Val Acc 65%, Loss 0.780805]\n","Iter 360. [Val Acc 65%, Loss 0.766289]\n","Iter 370. [Val Acc 66%, Loss 0.754752]\n","Iter 380. [Val Acc 66%, Loss 0.743504]\n","Iter 390. [Val Acc 66%, Loss 0.730939]\n","Iter 400. [Val Acc 66%, Loss 0.723672]\n","Iter 410. [Val Acc 66%, Loss 0.717218]\n","Iter 420. [Val Acc 67%, Loss 0.715492]\n","Iter 430. [Val Acc 67%, Loss 0.695718]\n","Iter 440. [Val Acc 67%, Loss 0.689669]\n","Iter 450. [Val Acc 67%, Loss 0.684444]\n","Iter 460. [Val Acc 68%, Loss 0.675668]\n","Iter 470. [Val Acc 68%, Loss 0.673336]\n","Iter 480. [Val Acc 68%, Loss 0.666558]\n","Iter 490. [Val Acc 68%, Loss 0.660774]\n","Iter 500. [Val Acc 68%, Loss 0.654241]\n","Iter 510. [Val Acc 69%, Loss 0.649946]\n","Iter 520. [Val Acc 69%, Loss 0.646229]\n","Iter 530. [Val Acc 69%, Loss 0.642368]\n","Iter 540. [Val Acc 69%, Loss 0.640598]\n","Iter 550. [Val Acc 69%, Loss 0.635907]\n","Iter 560. [Val Acc 69%, Loss 0.633418]\n","Iter 570. [Val Acc 70%, Loss 0.628302]\n","Iter 580. [Val Acc 70%, Loss 0.627600]\n","Iter 590. [Val Acc 70%, Loss 0.626729]\n","Iter 600. [Val Acc 70%, Loss 0.621411]\n","Iter 610. [Val Acc 70%, Loss 0.621846]\n","Iter 620. [Val Acc 70%, Loss 0.617425]\n","Iter 630. [Val Acc 70%, Loss 0.613802]\n","Iter 640. [Val Acc 70%, Loss 0.614414]\n","Iter 650. [Val Acc 70%, Loss 0.613460]\n","Iter 660. [Val Acc 70%, Loss 0.608775]\n","Iter 670. [Val Acc 70%, Loss 0.606546]\n","Iter 680. [Val Acc 71%, Loss 0.603055]\n","Iter 690. [Val Acc 71%, Loss 0.601859]\n","Iter 700. [Val Acc 71%, Loss 0.601248]\n","Iter 710. [Val Acc 71%, Loss 0.599102]\n","Iter 720. [Val Acc 71%, Loss 0.597411]\n","Iter 730. [Val Acc 71%, Loss 0.594596]\n","Iter 740. [Val Acc 71%, Loss 0.592610]\n","Iter 750. [Val Acc 71%, Loss 0.592885]\n","Iter 760. [Val Acc 71%, Loss 0.592737]\n","Iter 770. [Val Acc 71%, Loss 0.594528]\n","Iter 780. [Val Acc 72%, Loss 0.590904]\n","Iter 790. [Val Acc 72%, Loss 0.592990]\n","Iter 800. [Val Acc 72%, Loss 0.587860]\n","Iter 810. [Val Acc 72%, Loss 0.586981]\n","Iter 820. [Val Acc 72%, Loss 0.585866]\n","Iter 830. [Val Acc 72%, Loss 0.588084]\n","Iter 840. [Val Acc 72%, Loss 0.585397]\n","Iter 850. [Val Acc 72%, Loss 0.583619]\n","Iter 860. [Val Acc 72%, Loss 0.582261]\n","Iter 870. [Val Acc 72%, Loss 0.580161]\n","Iter 880. [Val Acc 72%, Loss 0.580038]\n","Iter 890. [Val Acc 72%, Loss 0.578393]\n","Iter 900. [Val Acc 72%, Loss 0.577898]\n","Iter 910. [Val Acc 72%, Loss 0.579943]\n","Iter 920. [Val Acc 72%, Loss 0.577038]\n","Iter 930. [Val Acc 73%, Loss 0.575450]\n","Iter 940. [Val Acc 72%, Loss 0.579716]\n","Iter 950. [Val Acc 72%, Loss 0.574575]\n","Iter 960. [Val Acc 72%, Loss 0.575289]\n","Iter 970. [Val Acc 72%, Loss 0.576867]\n","Iter 980. [Val Acc 72%, Loss 0.574316]\n","Iter 990. [Val Acc 73%, Loss 0.573036]\n","Iter 1000. [Val Acc 72%, Loss 0.572772]\n","-------------------------\n","mu= 0.1 batch_size= 50000 :\n","Iter 10. [Val Acc 60%, Loss 0.736115]\n","Iter 20. [Val Acc 65%, Loss 0.686533]\n","Iter 30. [Val Acc 68%, Loss 0.652574]\n","Iter 40. [Val Acc 69%, Loss 0.627356]\n","Iter 50. [Val Acc 70%, Loss 0.610556]\n","Iter 60. [Val Acc 71%, Loss 0.603844]\n","Iter 70. [Val Acc 72%, Loss 0.592599]\n","Iter 80. [Val Acc 72%, Loss 0.588085]\n","Iter 90. [Val Acc 72%, Loss 0.584503]\n","Iter 100. [Val Acc 72%, Loss 0.582740]\n","Iter 110. [Val Acc 72%, Loss 0.577937]\n","Iter 120. [Val Acc 72%, Loss 0.576994]\n","Iter 130. [Val Acc 73%, Loss 0.573602]\n","Iter 140. [Val Acc 73%, Loss 0.571907]\n","Iter 150. [Val Acc 73%, Loss 0.571011]\n","Iter 160. [Val Acc 73%, Loss 0.569678]\n","Iter 170. [Val Acc 73%, Loss 0.570306]\n","Iter 180. [Val Acc 73%, Loss 0.570271]\n","Iter 190. [Val Acc 73%, Loss 0.568046]\n","Iter 200. [Val Acc 73%, Loss 0.570174]\n","Iter 210. [Val Acc 73%, Loss 0.568895]\n","Iter 220. [Val Acc 73%, Loss 0.569121]\n","Iter 230. [Val Acc 73%, Loss 0.568084]\n","Iter 240. [Val Acc 73%, Loss 0.567097]\n","Iter 250. [Val Acc 73%, Loss 0.567504]\n","Iter 260. [Val Acc 73%, Loss 0.567608]\n","Iter 270. [Val Acc 73%, Loss 0.567378]\n","Iter 280. [Val Acc 73%, Loss 0.568777]\n","Iter 290. [Val Acc 73%, Loss 0.566003]\n","Iter 300. [Val Acc 73%, Loss 0.567384]\n","Iter 310. [Val Acc 73%, Loss 0.568386]\n","Iter 320. [Val Acc 73%, Loss 0.564794]\n","Iter 330. [Val Acc 73%, Loss 0.566777]\n","Iter 340. [Val Acc 73%, Loss 0.564675]\n","Iter 350. [Val Acc 73%, Loss 0.564413]\n","Iter 360. [Val Acc 73%, Loss 0.568015]\n","Iter 370. [Val Acc 73%, Loss 0.565861]\n","Iter 380. [Val Acc 73%, Loss 0.564881]\n","Iter 390. [Val Acc 73%, Loss 0.564935]\n","Iter 400. [Val Acc 73%, Loss 0.564168]\n","Iter 410. [Val Acc 73%, Loss 0.563537]\n","Iter 420. [Val Acc 73%, Loss 0.564986]\n","Iter 430. [Val Acc 73%, Loss 0.564259]\n","Iter 440. [Val Acc 73%, Loss 0.564824]\n","Iter 450. [Val Acc 73%, Loss 0.565714]\n","Iter 460. [Val Acc 73%, Loss 0.565908]\n","Iter 470. [Val Acc 73%, Loss 0.563462]\n","Iter 480. [Val Acc 73%, Loss 0.563564]\n","Iter 490. [Val Acc 73%, Loss 0.563662]\n","Iter 500. [Val Acc 73%, Loss 0.564205]\n","Iter 510. [Val Acc 73%, Loss 0.563514]\n","Iter 520. [Val Acc 73%, Loss 0.564427]\n","Iter 530. [Val Acc 73%, Loss 0.565638]\n","Iter 540. [Val Acc 73%, Loss 0.563486]\n","Iter 550. [Val Acc 73%, Loss 0.563062]\n","Iter 560. [Val Acc 73%, Loss 0.564175]\n","Iter 570. [Val Acc 73%, Loss 0.563936]\n","Iter 580. [Val Acc 73%, Loss 0.564311]\n","Iter 590. [Val Acc 73%, Loss 0.562764]\n","Iter 600. [Val Acc 73%, Loss 0.562597]\n","Iter 610. [Val Acc 73%, Loss 0.565040]\n","Iter 620. [Val Acc 73%, Loss 0.565235]\n","Iter 630. [Val Acc 73%, Loss 0.562720]\n","Iter 640. [Val Acc 73%, Loss 0.563254]\n","Iter 650. [Val Acc 73%, Loss 0.563242]\n","Iter 660. [Val Acc 73%, Loss 0.563620]\n","Iter 670. [Val Acc 73%, Loss 0.564230]\n","Iter 680. [Val Acc 73%, Loss 0.564952]\n","Iter 690. [Val Acc 73%, Loss 0.566127]\n","Iter 700. [Val Acc 73%, Loss 0.562753]\n","Iter 710. [Val Acc 73%, Loss 0.563075]\n","Iter 720. [Val Acc 73%, Loss 0.563789]\n","Iter 730. [Val Acc 73%, Loss 0.562714]\n","Iter 740. [Val Acc 73%, Loss 0.561236]\n","Iter 750. [Val Acc 73%, Loss 0.561560]\n","Iter 760. [Val Acc 73%, Loss 0.561698]\n","Iter 770. [Val Acc 73%, Loss 0.562777]\n","Iter 780. [Val Acc 73%, Loss 0.561599]\n","Iter 790. [Val Acc 73%, Loss 0.563078]\n","Iter 800. [Val Acc 73%, Loss 0.564514]\n","Iter 810. [Val Acc 73%, Loss 0.567656]\n","Iter 820. [Val Acc 73%, Loss 0.563739]\n","Iter 830. [Val Acc 73%, Loss 0.563306]\n","Iter 840. [Val Acc 73%, Loss 0.564770]\n","Iter 850. [Val Acc 73%, Loss 0.564090]\n","Iter 860. [Val Acc 73%, Loss 0.560671]\n","Iter 870. [Val Acc 73%, Loss 0.559944]\n","Iter 880. [Val Acc 73%, Loss 0.561137]\n","Iter 890. [Val Acc 73%, Loss 0.561869]\n","Iter 900. [Val Acc 74%, Loss 0.560795]\n","Iter 910. [Val Acc 73%, Loss 0.562739]\n","Iter 920. [Val Acc 73%, Loss 0.560706]\n","Iter 930. [Val Acc 73%, Loss 0.559462]\n","Iter 940. [Val Acc 73%, Loss 0.561349]\n","Iter 950. [Val Acc 73%, Loss 0.562603]\n","Iter 960. [Val Acc 73%, Loss 0.563293]\n","Iter 970. [Val Acc 73%, Loss 0.562432]\n","Iter 980. [Val Acc 73%, Loss 0.562910]\n","Iter 990. [Val Acc 73%, Loss 0.562498]\n","Iter 1000. [Val Acc 73%, Loss 0.561585]\n","-------------------------\n","monte_carlo: 2\n","mu= 0.1 batch_size= 1 :\n","Iter 10. [Val Acc 49%, Loss 4.088643]\n","Iter 20. [Val Acc 49%, Loss 3.643840]\n","Iter 30. [Val Acc 49%, Loss 3.286488]\n","Iter 40. [Val Acc 49%, Loss 2.997371]\n","Iter 50. [Val Acc 50%, Loss 2.791540]\n","Iter 60. [Val Acc 50%, Loss 2.586161]\n","Iter 70. [Val Acc 51%, Loss 2.387459]\n","Iter 80. [Val Acc 51%, Loss 2.258004]\n","Iter 90. [Val Acc 52%, Loss 2.107343]\n","Iter 100. [Val Acc 53%, Loss 1.984522]\n","Iter 110. [Val Acc 53%, Loss 1.879489]\n","Iter 120. [Val Acc 54%, Loss 1.769485]\n","Iter 130. [Val Acc 54%, Loss 1.682595]\n","Iter 140. [Val Acc 55%, Loss 1.587613]\n","Iter 150. [Val Acc 56%, Loss 1.518055]\n","Iter 160. [Val Acc 57%, Loss 1.462195]\n","Iter 170. [Val Acc 57%, Loss 1.389192]\n","Iter 180. [Val Acc 58%, Loss 1.337626]\n","Iter 190. [Val Acc 59%, Loss 1.273107]\n","Iter 200. [Val Acc 59%, Loss 1.222966]\n","Iter 210. [Val Acc 60%, Loss 1.176070]\n","Iter 220. [Val Acc 61%, Loss 1.142139]\n","Iter 230. [Val Acc 61%, Loss 1.100943]\n","Iter 240. [Val Acc 61%, Loss 1.062682]\n","Iter 250. [Val Acc 62%, Loss 1.029151]\n","Iter 260. [Val Acc 62%, Loss 1.002743]\n","Iter 270. [Val Acc 63%, Loss 0.980588]\n","Iter 280. [Val Acc 63%, Loss 0.950483]\n","Iter 290. [Val Acc 63%, Loss 0.925953]\n","Iter 300. [Val Acc 64%, Loss 0.905360]\n","Iter 310. [Val Acc 64%, Loss 0.885543]\n","Iter 320. [Val Acc 65%, Loss 0.861586]\n","Iter 330. [Val Acc 65%, Loss 0.842996]\n","Iter 340. [Val Acc 65%, Loss 0.830159]\n","Iter 350. [Val Acc 65%, Loss 0.805328]\n","Iter 360. [Val Acc 66%, Loss 0.794037]\n","Iter 370. [Val Acc 66%, Loss 0.780222]\n","Iter 380. [Val Acc 66%, Loss 0.768399]\n","Iter 390. [Val Acc 66%, Loss 0.752948]\n","Iter 400. [Val Acc 67%, Loss 0.740897]\n","Iter 410. [Val Acc 67%, Loss 0.728946]\n","Iter 420. [Val Acc 67%, Loss 0.723727]\n","Iter 430. [Val Acc 67%, Loss 0.710282]\n","Iter 440. [Val Acc 68%, Loss 0.702487]\n","Iter 450. [Val Acc 68%, Loss 0.694278]\n","Iter 460. [Val Acc 68%, Loss 0.688999]\n","Iter 470. [Val Acc 69%, Loss 0.680275]\n","Iter 480. [Val Acc 69%, Loss 0.675113]\n","Iter 490. [Val Acc 69%, Loss 0.674970]\n","Iter 500. [Val Acc 69%, Loss 0.668249]\n","Iter 510. [Val Acc 69%, Loss 0.658802]\n","Iter 520. [Val Acc 69%, Loss 0.652548]\n","Iter 530. [Val Acc 69%, Loss 0.651298]\n","Iter 540. [Val Acc 70%, Loss 0.645338]\n","Iter 550. [Val Acc 70%, Loss 0.640313]\n","Iter 560. [Val Acc 70%, Loss 0.636863]\n","Iter 570. [Val Acc 70%, Loss 0.630801]\n","Iter 580. [Val Acc 70%, Loss 0.626878]\n","Iter 590. [Val Acc 70%, Loss 0.622515]\n","Iter 600. [Val Acc 71%, Loss 0.618195]\n","Iter 610. [Val Acc 70%, Loss 0.618347]\n","Iter 620. [Val Acc 71%, Loss 0.612297]\n","Iter 630. [Val Acc 71%, Loss 0.610601]\n","Iter 640. [Val Acc 71%, Loss 0.606956]\n","Iter 650. [Val Acc 71%, Loss 0.604283]\n","Iter 660. [Val Acc 71%, Loss 0.602555]\n","Iter 670. [Val Acc 71%, Loss 0.602099]\n","Iter 680. [Val Acc 71%, Loss 0.598500]\n","Iter 690. [Val Acc 71%, Loss 0.599564]\n","Iter 700. [Val Acc 72%, Loss 0.596450]\n","Iter 710. [Val Acc 72%, Loss 0.595301]\n","Iter 720. [Val Acc 71%, Loss 0.592705]\n","Iter 730. [Val Acc 72%, Loss 0.591639]\n","Iter 740. [Val Acc 72%, Loss 0.588997]\n","Iter 750. [Val Acc 72%, Loss 0.588292]\n","Iter 760. [Val Acc 72%, Loss 0.586960]\n","Iter 770. [Val Acc 72%, Loss 0.586938]\n","Iter 780. [Val Acc 72%, Loss 0.585840]\n","Iter 790. [Val Acc 72%, Loss 0.583550]\n","Iter 800. [Val Acc 72%, Loss 0.581137]\n","Iter 810. [Val Acc 72%, Loss 0.581396]\n","Iter 820. [Val Acc 72%, Loss 0.580694]\n","Iter 830. [Val Acc 72%, Loss 0.579579]\n","Iter 840. [Val Acc 72%, Loss 0.578937]\n","Iter 850. [Val Acc 72%, Loss 0.578088]\n","Iter 860. [Val Acc 72%, Loss 0.576692]\n","Iter 870. [Val Acc 72%, Loss 0.575402]\n","Iter 880. [Val Acc 72%, Loss 0.575965]\n","Iter 890. [Val Acc 73%, Loss 0.573956]\n","Iter 900. [Val Acc 72%, Loss 0.573690]\n","Iter 910. [Val Acc 72%, Loss 0.574301]\n","Iter 920. [Val Acc 73%, Loss 0.573690]\n","Iter 930. [Val Acc 72%, Loss 0.573711]\n","Iter 940. [Val Acc 73%, Loss 0.572138]\n","Iter 950. [Val Acc 72%, Loss 0.573484]\n","Iter 960. [Val Acc 73%, Loss 0.571933]\n","Iter 970. [Val Acc 73%, Loss 0.572595]\n","Iter 980. [Val Acc 72%, Loss 0.576842]\n","Iter 990. [Val Acc 73%, Loss 0.571624]\n","Iter 1000. [Val Acc 73%, Loss 0.572718]\n","-------------------------\n","mu= 0.1 batch_size= 50000 :\n","Iter 10. [Val Acc 72%, Loss 0.588926]\n","Iter 20. [Val Acc 72%, Loss 0.582369]\n","Iter 30. [Val Acc 73%, Loss 0.577977]\n","Iter 40. [Val Acc 73%, Loss 0.575667]\n","Iter 50. [Val Acc 73%, Loss 0.574534]\n","Iter 60. [Val Acc 73%, Loss 0.571842]\n","Iter 70. [Val Acc 73%, Loss 0.571034]\n","Iter 80. [Val Acc 73%, Loss 0.568841]\n","Iter 90. [Val Acc 73%, Loss 0.568537]\n","Iter 100. [Val Acc 73%, Loss 0.568093]\n","Iter 110. [Val Acc 73%, Loss 0.566364]\n","Iter 120. [Val Acc 73%, Loss 0.568557]\n","Iter 130. [Val Acc 73%, Loss 0.568008]\n","Iter 140. [Val Acc 73%, Loss 0.568118]\n","Iter 150. [Val Acc 73%, Loss 0.567871]\n","Iter 160. [Val Acc 73%, Loss 0.567331]\n","Iter 170. [Val Acc 73%, Loss 0.567699]\n","Iter 180. [Val Acc 73%, Loss 0.565132]\n","Iter 190. [Val Acc 73%, Loss 0.567102]\n","Iter 200. [Val Acc 73%, Loss 0.567063]\n","Iter 210. [Val Acc 73%, Loss 0.566032]\n","Iter 220. [Val Acc 73%, Loss 0.564880]\n","Iter 230. [Val Acc 73%, Loss 0.566003]\n","Iter 240. [Val Acc 73%, Loss 0.563302]\n","Iter 250. [Val Acc 73%, Loss 0.564889]\n","Iter 260. [Val Acc 73%, Loss 0.563869]\n","Iter 270. [Val Acc 73%, Loss 0.564403]\n","Iter 280. [Val Acc 73%, Loss 0.565197]\n","Iter 290. [Val Acc 73%, Loss 0.565225]\n","Iter 300. [Val Acc 73%, Loss 0.563910]\n","Iter 310. [Val Acc 73%, Loss 0.562909]\n","Iter 320. [Val Acc 73%, Loss 0.563186]\n","Iter 330. [Val Acc 73%, Loss 0.563382]\n","Iter 340. [Val Acc 73%, Loss 0.563543]\n","Iter 350. [Val Acc 73%, Loss 0.564477]\n","Iter 360. [Val Acc 73%, Loss 0.563013]\n","Iter 370. [Val Acc 73%, Loss 0.562193]\n","Iter 380. [Val Acc 73%, Loss 0.561940]\n","Iter 390. [Val Acc 73%, Loss 0.561862]\n","Iter 400. [Val Acc 73%, Loss 0.562086]\n","Iter 410. [Val Acc 73%, Loss 0.563611]\n","Iter 420. [Val Acc 73%, Loss 0.562986]\n","Iter 430. [Val Acc 73%, Loss 0.563011]\n","Iter 440. [Val Acc 73%, Loss 0.562432]\n","Iter 450. [Val Acc 73%, Loss 0.563435]\n","Iter 460. [Val Acc 73%, Loss 0.563261]\n","Iter 470. [Val Acc 73%, Loss 0.564406]\n","Iter 480. [Val Acc 73%, Loss 0.562204]\n","Iter 490. [Val Acc 73%, Loss 0.562455]\n","Iter 500. [Val Acc 73%, Loss 0.562971]\n","Iter 510. [Val Acc 73%, Loss 0.563270]\n","Iter 520. [Val Acc 73%, Loss 0.562334]\n","Iter 530. [Val Acc 73%, Loss 0.561062]\n","Iter 540. [Val Acc 73%, Loss 0.562285]\n","Iter 550. [Val Acc 73%, Loss 0.561582]\n","Iter 560. [Val Acc 73%, Loss 0.559796]\n","Iter 570. [Val Acc 73%, Loss 0.561570]\n","Iter 580. [Val Acc 74%, Loss 0.562317]\n","Iter 590. [Val Acc 73%, Loss 0.561832]\n","Iter 600. [Val Acc 73%, Loss 0.560521]\n","Iter 610. [Val Acc 73%, Loss 0.560394]\n","Iter 620. [Val Acc 73%, Loss 0.561316]\n","Iter 630. [Val Acc 74%, Loss 0.560536]\n","Iter 640. [Val Acc 73%, Loss 0.560485]\n","Iter 650. [Val Acc 73%, Loss 0.560967]\n","Iter 660. [Val Acc 74%, Loss 0.562116]\n","Iter 670. [Val Acc 73%, Loss 0.559665]\n","Iter 680. [Val Acc 73%, Loss 0.559768]\n","Iter 690. [Val Acc 73%, Loss 0.560675]\n","Iter 700. [Val Acc 73%, Loss 0.561690]\n","Iter 710. [Val Acc 73%, Loss 0.564173]\n","Iter 720. [Val Acc 73%, Loss 0.560785]\n","Iter 730. [Val Acc 73%, Loss 0.563405]\n","Iter 740. [Val Acc 73%, Loss 0.565020]\n","Iter 750. [Val Acc 73%, Loss 0.561862]\n","Iter 760. [Val Acc 74%, Loss 0.561143]\n","Iter 770. [Val Acc 73%, Loss 0.562776]\n","Iter 780. [Val Acc 73%, Loss 0.563701]\n","Iter 790. [Val Acc 73%, Loss 0.560083]\n","Iter 800. [Val Acc 73%, Loss 0.560464]\n","Iter 810. [Val Acc 73%, Loss 0.560005]\n","Iter 820. [Val Acc 73%, Loss 0.561094]\n","Iter 830. [Val Acc 73%, Loss 0.560373]\n","Iter 840. [Val Acc 73%, Loss 0.561206]\n","Iter 850. [Val Acc 73%, Loss 0.565063]\n","Iter 860. [Val Acc 73%, Loss 0.562654]\n","Iter 870. [Val Acc 73%, Loss 0.561572]\n","Iter 880. [Val Acc 74%, Loss 0.563063]\n","Iter 890. [Val Acc 73%, Loss 0.564624]\n","Iter 900. [Val Acc 73%, Loss 0.562960]\n","Iter 910. [Val Acc 73%, Loss 0.561574]\n","Iter 920. [Val Acc 74%, Loss 0.561824]\n","Iter 930. [Val Acc 73%, Loss 0.561352]\n","Iter 940. [Val Acc 73%, Loss 0.563464]\n","Iter 950. [Val Acc 73%, Loss 0.562971]\n","Iter 960. [Val Acc 73%, Loss 0.561951]\n","Iter 970. [Val Acc 74%, Loss 0.562226]\n","Iter 980. [Val Acc 73%, Loss 0.560642]\n","Iter 990. [Val Acc 74%, Loss 0.559579]\n","Iter 1000. [Val Acc 73%, Loss 0.562161]\n","-------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Batch size convergance properties')"]},"metadata":{},"execution_count":87},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"pkZt7_932zX2"},"source":["\n","\n","For $\\mu=0.1$ and batch size 1 we can see from this figure that it takes more iterations for the validation loss to converge (compared with batch size 50000) because the estimation of the gradient is based on one sample, which makes this estimation very noisy. Through the training procedure we see that a minibatch with smaller size suffer from higher variance when applying Monte Carlo experiment with random parameters initialization.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1KrQqSj2zGrI"},"source":["### Part (h) -- 15%\n","\n","Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n","and test accuracy. Are there any differences between those three values? If so, why?"]},{"cell_type":"code","metadata":{"id":"fuKw2mLozGrI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636560230864,"user_tz":-120,"elapsed":963,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"214e0d6b-bf20-49b1-a8a6-d2e979e75f93"},"source":["train_ys=pred(opt_w,opt_b,train_norm_xs)\n","test_ys=pred(opt_w,opt_b,test_norm_xs)\n","val_ys=pred(opt_w,opt_b,val_norm_xs)\n","\n","\n","train_acc = get_accuracy(train_ys,train_ts)\n","val_acc = get_accuracy(val_ys,val_ts)\n","test_acc =  get_accuracy(test_ys,test_ts)\n","\n","print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_acc =  0.7315905877234328  val_acc =  0.73532  test_acc =  0.7247143133836916\n"]}]},{"cell_type":"markdown","metadata":{"id":"RXZa1u6920M3"},"source":["Both train and validation accuracies are better than the test accuracy, we explain this phenomenoa by the fact that we didn't see the test data during the model building process, so we expect to encounter minor degradation in the model performance over the new unseen data.\n","\n","Moreover, we can see that the gap between the train and test data is not significant and therefore we conclude that the model can extract meaningful information from the training dataset.\n","Meaning that the model is capable to perform similarly over unseen data, e.g., the model is well generalized. \n","\n","Regarding the validation accuracy, we have tuned our model hyperparameters(Learning rate and Batch size) such that it achieves the best performance over the validation data, therefore, we shall expect to achieve better results in the validation data than the test data, and this is indeed the result that we've achieved. \n"]},{"cell_type":"markdown","metadata":{"id":"h4eP2Yh1zGrI"},"source":["### Part (i) -- 15%\n","\n","Writing a classifier like this is instructive, and helps you understand what happens when\n","we train a model. However, in practice, we rarely write model building and training code\n","from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n","\n","Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n","[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n","\n","Compute the training, validation and test accuracy of this model."]},{"cell_type":"code","metadata":{"id":"24LCfAa1zGrJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636560238749,"user_tz":-120,"elapsed":7890,"user":{"displayName":"שחף ימין","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16772830221257596567"}},"outputId":"fc4c96a9-82cf-49b5-858a-17ea19be9420"},"source":["import sklearn.linear_model\n","\n","model = sklearn.linear_model.LogisticRegression()\n","model.fit(train_norm_xs, train_ts.squeeze())\n","train_ys = model.predict(train_norm_xs)\n","test_ys = model.predict(test_norm_xs)\n","val_ys = model.predict(val_norm_xs)\n","train_acc = get_accuracy(train_ys,train_ts)\n","val_acc = get_accuracy(val_ys,val_ts)\n","test_acc =  get_accuracy(test_ys,test_ts)\n","\n","print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_acc =  0.7323979067715698  val_acc =  0.73642  test_acc =  0.7265736974627155\n"]}]},{"cell_type":"markdown","metadata":{"id":"HRqucdV923tG"},"source":["**This parts helps by checking if the code worked.**\n","**Check if you get similar results, if not repair your code**\n"]}]}